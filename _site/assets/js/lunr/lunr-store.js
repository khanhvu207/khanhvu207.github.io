var store = [{
        "title": "Noisy-channel coding theorem",
        "excerpt":"Here is my second attempt to understand the proof for the channel coding theorem. My initial exposure to this concept was through Professor Amos Lapidoth’s lecture in Information Theory I. His presentation, though concise, was packed with ingenious methods that were challenging to fully comprehend at time. To better understand...","categories": [],
        "tags": [],
        "url": "/channel_coding_proof/",
        "teaser": null
      },{
        "title": "Rate-distortion theory",
        "excerpt":"In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem. The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression. I will try to keep the...","categories": [],
        "tags": [],
        "url": "/rate_distortion/",
        "teaser": null
      },{
        "title": "An upper bound for the Kullback-Leibler divergence",
        "excerpt":"Zürich is so hot now in the night that I couldn’t fall asleep. While lying on the bed, I arrived at an interesting upper bound for the KL divergence (relative entropy) between two Bernoulli distributions. In this case, the KL divergence is expressed as: \\[D_\\text{KL}(p \\parallel q) := p\\log\\left(\\frac{p}{q}\\right) +...","categories": [],
        "tags": [],
        "url": "/kl_upper_bound/",
        "teaser": null
      },{
        "title": "Jane Street Puzzle - August 2024",
        "excerpt":"I decided to slack off my exam studying with Jane Street’s puzzle. I first read the problem in early August, forgot about it, and then submitted my answer on August 16—and got it right on the first try! (username: Khanh Vu). This month’s puzzle involves straightforward state analysis with a...","categories": [],
        "tags": [],
        "url": "/jane_street_aug24/",
        "teaser": null
      },{
        "title": "Modern hashing made simple",
        "excerpt":"This is a short personal note on “Modern Hashing Made Simple” by Bender et al. (2024). I’m going to present this paper for the Advanced Graph Algorithms and Optimization Seminar (Fall 2024) at ETH Zürich. My personal goal is to understand the paper well enough to present it in a...","categories": [],
        "tags": [],
        "url": "/modern_hashing/",
        "teaser": null
      },{
        "title": "Some personal notes on probability theory",
        "excerpt":"A self-contained summary of some key results in probability theory. I’ll mostly summarize the results from Durrett’s Probability: Theory and Examples, Bandit Algorithms (Lattimore and Szepesvári) and some other sources. Note to self: For now I will dump everything here and later I will try to organize it better. 1....","categories": [],
        "tags": [],
        "url": "/probability_theory/",
        "teaser": null
      },{
        "title": "Monty Hall problem with Bayesian inference",
        "excerpt":"One simple way to think about the Monty Hall problem is to use Baye’s rule. For the sake of completeness, here is the problem statement: Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You...","categories": [],
        "tags": [],
        "url": "/monty_hall/",
        "teaser": null
      },{
        "title": "Estimating density ratios without knowing the densities",
        "excerpt":"Suppose we have two probability distributions $p$ and $q$ over the same space $\\mathcal{X}$, and we want to estimate the density ratio $\\frac{q(x)}{p(x)}$ for some $x \\in \\mathcal{X}$. The caveat is that we do not know the densities $p$ and $q$, but we have a way to sample from them....","categories": [],
        "tags": [],
        "url": "/density_ratio_estimation/",
        "teaser": null
      },{
        "title": "Model averaging with heterogeneous predictors",
        "excerpt":"Competitors on Kaggle always employ model averaging to improve their scores. The resulting improvement, often achieved via a weighted average of predictors, is usually guaranteed and sometimes significant. Here, I notice that a common piece of advice is to average the predictions over a diverse pool of models, with a...","categories": [],
        "tags": [],
        "url": "/model_averaging/",
        "teaser": null
      }]
