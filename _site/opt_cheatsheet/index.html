<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Mathematical optimization cheatsheet - EntropyMaxxing</title>
<meta name="description" content="In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms. In the first part, I will recite the definitions of convexity, smoothness, and strong convexity, which are fundamental properties in the realm of convex optimization on differentiable functions. In the second part, we will relax to the case of non-differentiable functions and discuss subgradients and proximal operators.">


  <meta name="author" content="Khánh Vũ">
  
  <meta property="article:author" content="Khánh Vũ">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="EntropyMaxxing">
<meta property="og:title" content="Mathematical optimization cheatsheet">
<meta property="og:url" content="http://localhost:4000/opt_cheatsheet/">


  <meta property="og:description" content="In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms. In the first part, I will recite the definitions of convexity, smoothness, and strong convexity, which are fundamental properties in the realm of convex optimization on differentiable functions. In the second part, we will relax to the case of non-differentiable functions and discuss subgradients and proximal operators.">







  <meta property="article:published_time" content="2024-04-26T00:00:00+02:00">





  

  


<link rel="canonical" href="http://localhost:4000/opt_cheatsheet/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Khánh Vũ",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="EntropyMaxxing Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- for mathjax support -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>



<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--posts">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          EntropyMaxxing
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/avatar.png" alt="Khánh Vũ" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Khánh Vũ</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>one should never try to prove anything that is not almost obvious</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">Mathematical optimization cheatsheet</h1>
    
    <p>In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms.
In the first part, I will recite the definitions of convexity, smoothness, and strong convexity, which are fundamental properties in the realm of convex optimization on differentiable functions.
In the second part, we will relax to the case of non-differentiable functions and discuss subgradients and proximal operators.</p>

<h1 id="1-convex-optimization">1. Convex optimization</h1>

<p>Let us define a function $f: \text{dom}(f)\rightarrow \mathbb{R}$ with $\text{dom}(f) \subseteq \mathbb{R}^d$.
We assume that $f$ is differentiable, meaning that the gradient $\nabla f(x) \in \mathbb{R}^d$ exists $\forall x \in \text{dom}(f)$.</p>

<center>
    <figure>
        <img src="../assets/images/smooth_strong_convex.png" alt="smoothness and strong convexity illustration" style="display: inline-block;" />
        <figcaption> Figure 1. Smoothness and strong convexity play complementary roles in bounding the function values. </figcaption>
    </figure>
</center>

<h2 id="11-convexity">1.1 Convexity</h2>

<p><strong>(a) Definition.</strong>
A function $f$ is convex if its domain is a convex set and if for all $x, y \in \text{dom}(f)$ and $\theta \in [0, 1]$, we have</p>

\[f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)\]

<p>This definition is also known as the Jensen’s inequality.
Oftentimes, it is more convenient to work with one of the following equivalent definitions:</p>

<p><strong>(b) (First-order characterization)</strong>
\(f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle\)</p>

<p>Geometrically, this definition implies that the function lies above its tangent line at any point (see Figure 1).</p>

<p><strong>(c) (Second-order characterization)</strong>
Suppose that $f$ is twice differentiable.
A function $f$ is convex if its domain is a convex set and if for all $x \in \text{dom}(f)$, we have</p>

\[\nabla^2 f(x) \succeq 0\]

<p>where $\nabla^2 f(x)$ is the Hessian matrix of $f$ at $x$.</p>

<p><strong>(d) (Monotonicity of the gradient)</strong>
A function $f$ is convex if its domain is a convex set and if for all $x, y \in \text{dom}(f)$, we have</p>

\[\langle \nabla f(y) - \nabla f(x), y-x \rangle \geq 0\]

<h2 id="12-smooth-functions">1.2 Smooth functions</h2>

<p><strong>(a) Definition.</strong>
Let $X\subseteq \text{dom}(f)$ be a convex set and let $L &gt; 0$. A function $f$ is $L$-smooth on $X$ if for all $x, y \in X$, we have</p>

\[f(y) \leq f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2}\|y-x\|^2\]

<p>One can think of this definition as Lipschitz continuity of the gradient map.
When $f$ is twice differentiable, smoothness implies that the largest eigenvalue of the Hessian is bounded by $L$, e.g. \(\|\nabla^2 f(x)\| \leq L\).
If $f$ is $L$-smooth, then $f$ is also $L’$-smooth for all $L’ \geq L$.
Alternatively, we can use the following equivalent definition:</p>

<p><strong>(b) (Lipschitz continuity of the gradient)</strong>
\(\| f(x) - \nabla f(y)\| \leq L\| x-y\|\) for all \(x, y \in X\).</p>

<p><strong>(c)</strong>
\(g(x) = \frac{L}{2} x^\top x - f(x)\) is convex.</p>

<p><strong>(d)</strong>
\(\langle \nabla f(x) - \nabla f(y), x-y \rangle \leq L\|x-y\|^2\) for all \(x, y \in X\).</p>

<p>These definitions are not actually equivalent, but they are related.
We have the following relations:</p>

\[[b] \Rightarrow [a] \Leftrightarrow [c] \Leftrightarrow [d]\]

<p>If $f$ is additionally convex, then \([a] \Rightarrow [b]\).</p>

<h2 id="13-strong-convexity">1.3 Strong convexity</h2>

<p><strong>(a) Definition.</strong>
Let $X\subseteq \text{dom}(f)$ be a convex set and let $\mu &gt; 0$. A function $f$ is $\mu$-strongly convex on $X$ if for all $x, y \in X$, we have</p>

\[f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle + \frac{\mu}{2}\|y-x\|^2\]

<p>The notion of strong convexity complements the notion of smoothness.
Strong convexity implies that the function grows at least quadratically.
Moreover, $f$ is strongly convex implies that $f$ is strictly convex and has a unique minimizer.
And similarly to smoothness, if $f$ is $\mu$-strongly convex, then $f$ is also $\mu’$-strongly convex for all $\mu’ \leq \mu$.
Alternatively, we can use the following equivalent definition:</p>

<p><strong>(b)</strong>
\(\| f(x) - \nabla f(y)\| \geq \mu\| x-y\|\) for all \(x, y \in X\).</p>

<p><strong>(c)</strong>
\(g(x) = f(x) - \frac{\mu}{2} x^\top x\) is convex.</p>

<p><strong>(d)</strong>
\(\langle \nabla f(x) - \nabla f(y), x-y \rangle \geq \mu\|x-y\|^2\) for all \(x, y \in X\).</p>

<p>The relations are as follows:</p>

\[[b] \Leftarrow [a] \Leftrightarrow [c] \Leftrightarrow [d]\]

<p>Figure 1 above illustrates the complementary roles of smoothness and strong convexity in bounding the function values.</p>

<h1 id="2-non-smooth-optimization">2. Non-smooth optimization</h1>

<p>In this section, we define the function \(f\) similar to before, but we make no assumption of differentiability.</p>

<h2 id="22-non-smooth-functions">2.2 Non-smooth functions</h2>

<p><strong>(a) Definition.</strong> 
If \(f\) is not differentiable, or if the gradients of \(f\) exists but are not Lipschitz continuous, then we say that \(f\) is non-smooth.
This definition is exactly the negation of the smoothness property.
Examples of non-smooth functions include the absolute value function \(f(x) = |x|\), the hinge loss function \(f(x) = \max(0, 1-x)\), or many activation functions in neural networks such as the ReLU function \(f(x) = \max(0, x)\).</p>

<p><strong>(b) Subgradients.</strong>
A vector \(g \in \mathbb{R}^d\) is a subgradient of \(f\) at \(x\) if for all \(y \in \text{dom}(f)\), we have</p>

\[f(y) \geq f(x) + \langle g, y-x \rangle\]

<p>Also, we define a subdifferential \(\partial f(x)\) denoting the set of all subgradients of \(f\) at \(x\).</p>

<p><strong>(c) Properties of subgradients.</strong></p>

<ul>
  <li>If \(f\) is differentiable at \(x\in \text{dom}(f)\), then \(\partial f(x) \subseteq \{\nabla f(x)\}\), meaning that the subdifferential either contains the gradient or is empty.</li>
  <li>If \(f\) is convex, then \(\partial f(x)\) is non-empty for all \(x\in \text{int(dom)}(f)\).</li>
  <li>If \(\text{dom}(f)\) is convex and \(\partial f(x) \neq \emptyset\) for all \(x\in \text{dom}(f)\), then \(f\) is convex.</li>
  <li><strong>(Subgradient optimality condition)</strong> If \(\mathbf{0} \in \partial f(x)\), then \(x\) is a global minimum of \(f\).</li>
</ul>


<!-- <ul class="taxonomy__index">
  
  
    <li>
      <a href="#2024">
        <strong>2024</strong> <span class="taxonomy__count">1</span>
      </a>
    </li>
  
    <li>
      <a href="#2023">
        <strong>2023</strong> <span class="taxonomy__count">2</span>
      </a>
    </li>
  
</ul> -->

<!-- 


  <section id="2024" class="taxonomy__section">
    <h2 class="archive__subtitle">2024</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/opt_cheatsheet/" rel="permalink">Mathematical optimization cheatsheet
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms.
In the first part, I will recite the definiti...</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2023" class="taxonomy__section">
    <h2 class="archive__subtitle">2023</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/rate_distortion/" rel="permalink">Rate-distortion theory
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of t...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/channel_coding_proof/" rel="permalink">Noisy-channel coding theorem
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Here is my second attempt to understand the proof for the channel coding theorem.
My initial exposure to this concept was through Professor Amos Lapidoth’s l...</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>
 -->

  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/khanhvu207" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/khanhvu207" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Khánh Vũ. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
