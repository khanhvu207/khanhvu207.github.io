<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-04-14T18:36:24+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">EntropyMaxxing</title><author><name>Khánh Vũ</name></author><entry><title type="html">Some personal notes on probability theory</title><link href="http://localhost:4000/probability_theory/" rel="alternate" type="text/html" title="Some personal notes on probability theory" /><published>2025-03-16T00:00:00+01:00</published><updated>2025-03-16T00:00:00+01:00</updated><id>http://localhost:4000/probability_theory</id><content type="html" xml:base="http://localhost:4000/probability_theory/"><![CDATA[<p>A self-contained summary of some key results in probability theory.
I’ll mostly summarize the results from <a href="https://www.cambridge.org/ch/universitypress/subjects/statistics-probability/probability-theory-and-stochastic-processes/probability-theory-and-examples-5th-edition?format=HB">Durrett’s Probability: Theory and Examples</a>, <a href="https://tor-lattimore.com/downloads/book/book.pdf">Bandit Algorithms (Lattimore and Szepesvári)</a> and some other sources.</p>

<h2 id="0-measure-theoretic-probability">0. Measure-theoretic probability</h2>

<p><strong>($\sigma$-algebra and probability measure)</strong>
Let $\Omega$ be a set of outcomes.
A set of events $\mathcal{F} \subseteq 2^\Omega$ is a $\sigma$-algebra if $\Omega\in\mathcal{F}$, $A^c \in \mathcal{F}$ for all $A \in \mathcal{F}$ and $\bigcup_i A_i \in \mathcal{F}$ for all $A_i \in \mathcal{F}$.
A function $\mathbb{P}: \mathcal{F} \to \mathbb{R}$ is a probability measure if $\mathbb{P}(\Omega) = 1$ and $\mathbb{P}(A) \ge 0$ for all $A \in \mathcal{F}$.
Moreover, $\mathbb{P}(A^c) = 1 - \mathbb{P}(A)$ for all $A \in \mathcal{F}$ and $\mathbb{P}(\bigcup_i A_i) = \sum_i \mathbb{P}(A_i)$ for all $A_i \in \mathcal{F}$ such that $A_i \cap A_j = \emptyset$ for all $i \neq j$.
A set $\mathcal{G}$ is a sub-$\sigma$-algebra of $\mathcal{F}$ if $\mathcal{G} \subseteq \mathcal{F}$ and $\mathcal{G}$ is a $\sigma$-algebra.
The restriction of $\mathbb{P}$ to $\mathcal{G}$ is denoted by $\mathbb{P}|_{\mathcal{G}}$.</p>

<p>For an event $A\in\mathcal{F}$, we denote the probability of $A$ by $\mathbb{P}(A)$.</p>

<p><strong>(Measurable space)</strong>
A measurable space is a pair $(\Omega, \mathcal{F})$ where $\Omega$ is a set and $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$.</p>

<p><strong>($\mathcal{F}/\mathcal{G}$-measurable map)</strong>
Let $(\Omega, \mathcal{F})$ be a measurable space and let $\mathcal{X}$ be any set and $\mathcal{G}\subseteq 2^{\mathcal{X}}$.
A map $X:\Omega \to \mathcal{X}$ is $\mathcal{F}/\mathcal{G}$-measurable if $X^{-1}(A) \in \mathcal{F}$ for all $A \in \mathcal{G}$.
Here, $\mathcal{G}$ <em>need not</em> to be a $\sigma$-algebra.
If $X$ is $\mathcal{F}/\mathcal{G}$-measurable, then it is also $\mathcal{F}/\sigma(\mathcal{G})$-measurable, where $\sigma(\mathcal{G})$ is the smallest $\sigma$-algebra containing $\mathcal{G}$.</p>

<p>Given a map $X:\Omega\to\mathcal{X}$ between measurable spaces $(\Omega, \mathcal{F})$ and $(\mathcal{X}, \mathcal{G})$, we define \(\sigma(X) = \{X^{-1}(A): A\in\mathcal{G}\}\) to be the $\sigma$-algebra generated by $X$.
Here, the term “generated” means that $\sigma(X)$ is the <em>smallest</em> $\sigma$-algebra containing $X^{-1}(A)$ for all $A \in \mathcal{G}$.
The map $X$ is $\mathcal{F}/\mathcal{G}$-measurable if and only if $\sigma(X) \subseteq \mathcal{F}$.
In fact, $\sigma(X)$ is a sub-$\sigma$-algebra of $\mathcal{F}$ and is also the smallest sub-$\sigma$-algebra for which $X$ is measurable.
Furthurmore, if $\mathcal{G} = \sigma(\mathcal{A})$ itself is generated by a set $\mathcal{A}\subseteq 2^{\mathcal{X}}$, it is enough to check that \(X^{-1}(\mathcal{A}) = \{X^{-1}(A) : A\in\mathcal{A}\}\) is a subset of $\mathcal{F}$.</p>

<p><strong>(Borel $\sigma$-algebra)</strong>
If $\mathcal{G}$ is a set of open intervals in $\mathbb{R}$, then the Borel $\sigma$-algebra $\mathcal{B}(\mathbb{R})$ is the smallest $\sigma$-algebra containing $\mathcal{G}$.</p>

<hr />
<p><strong>(Random variable)</strong>
A random variable on a measurable space $(\Omega, \mathcal{F})$ is a $\mathcal{F}/\mathcal{B}(\mathbb{R})$-measurable map $X:\Omega \to \mathbb{R}$.</p>

<p style="background-color: lightgreen; padding: 10px;">
$\color{darkgreen}{\text{(Lemma 0.1: Doob–Dynkin lemma (also known as factorization lemma))}}$
Assume that we are given measurable spaces $(\Omega, \mathcal{F})$, $(\mathcal{X}, \mathcal{G})$ and $(\mathcal{Y}, \mathcal{H})$, and $X:\Omega\to\mathcal{X}$ and $Y:\Omega\to\mathcal{Y}$ are the random elements (generalization of random variables to higher dimensions), if $(\mathcal{Y}, \mathcal{H})$ is a Borel space, then $Y$ is $\sigma(X)$-measureable if and only if there exists a $\mathcal{G}/\mathcal{H}$-measurable map $f:\mathcal{X}\to\mathcal{Y}$ such that $Y = f\circ X$.
Here, $Y$ is $\sigma(X)$-measurable means that $\sigma(Y) \subseteq \sigma(X)$, i.e. knowing $X$ gives us information about $Y$.
</p>

<hr />
<p><strong>(Filtration)</strong>
Given a measurable space $(\Omega, \mathcal{F})$, a filtration is a sequence of \((\mathcal{F}_t)_{t=0}^n\) of sub-$\sigma$-algebras of $\mathcal{F}$ such that \(\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \ldots \subseteq \mathcal{F}_n\).
Also we define \(\mathcal{F}_\infty = \sigma\left(\bigcup_{t=0}^\infty \mathcal{F}_t\right)\) to be the smallest $\sigma$-algebra containing the union of all \(\mathcal{F}_t\).</p>

<p>A sequence of random variables \((X_t)_{t=1}^n\) if <em>adapted</em> to the filtration \(\mathbb{F} = (\mathcal{F}_t)_{t=0}^n\) if \(X_t\) is \(\mathcal{F}_{t}\)-measurable for all $t\in[n]$.
For brevity, we can say \((X_t)_t\) is $\mathbb{F}$-adapted.
Finally, \((X_t)_t\) is $\mathbb{F}$-predictable if $X_t$ is \(\mathcal{F}_{t-1}\)-measurable for all $t\in[n]$.</p>

<hr />
<p><strong>(Conditional probability)</strong>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space.
The conditional probability $\mathbb{P}(A|B)$ of $A$ given $B$ is defined as \(\mathbb{P}(A|B) = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\).
Intuitively, this tells us how large the portion of $A$ is in $B$.</p>

<p><strong>(Independence)</strong>
Two events $A, B\in\mathcal{F}$ are independent if $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$.
Alternatively, we can say that $A$ and $B$ are independent if $\mathbb{P}(A|B) = \mathbb{P}(A)$.
Two random variables $X$ and $Y$ are independent if $\sigma(X)$ and $\sigma(Y)$ are independent $\sigma$-algebras, i.e. $\mathbb{P}(X\in A, Y\in B) = \mathbb{P}(X\in A)\mathbb{P}(Y\in B)$ for all $A, B\in\mathcal{B}(\mathbb{R})$.</p>

<hr />
<p><strong>(Expectation)</strong>
Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, the expectation of a random variable $X:\Omega\to\mathbb{R}$ is defined as its Lebesgue integral with respect to $\mathbb{P}$:</p>

\[\mathbb{E}[X] = \int_\Omega X(\omega) d\mathbb{P}(\omega)\]

<p>To rigourously define what exactly this is, we shall need two facts about integration.
First, the integral of an indicator function is the probability of the event:
If \(X(\omega) = \mathbb{I}\{\omega\in A\}\) for some $A\in\mathbb{F}$, then $\int_\Omega X(\omega) d\mathbb{P}(\omega) = \mathbb{P}(A)$.
Second, the integral is a linear operator.</p>

<p>Now, if \(X(\omega) = \sum_{i=1}^n a_i \mathbb{I}\{\omega\in A_i\}\) for some $A_i\in\mathcal{F}$, then we can write the integral as:</p>

\[\int_\Omega X(\omega) d\mathbb{P}(\omega) = \sum_{i=1}^n a_i \int_\Omega \mathbb{I}\{\omega\in A_i\} d\mathbb{P}(\omega) = \sum_{i=1}^n a_i \mathbb{P}(A_i)\]

<p>Such an $X$ is called a simple function.
Then, the expectation is an approximation from below:</p>

\[\int_\Omega X(\omega) d\mathbb{P}(\omega) = \sup\left\{\int_\Omega h(\omega) d\mathbb{P}(\omega) : \text{$h$ is simple and $0\le h \le X$ pointwise} \right\}\]

<hr />
<p><strong>(Stochastic process)</strong>
A stochastic process on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is a collection of random variables $(X_t)_{t\in T}$ indexed by a set $T$.</p>

<p><strong>($\mathbb{P}$-almost surely)</strong>
When two random variables $X$ and $Y$ are equal almost surely, we write $X = Y$ $\mathbb{P}$-almost surely, denoted by $X = Y$ $\mathbb{P}$-a.s., if $\mathbb{P}(X = Y) = 1$.
To put it another way, they disagree on a set of measure zero.</p>

<p><strong>(Martingale)</strong>
Let $X_1, X_2, \ldots$ be a sequence of random variables on $(\Omega, \mathcal{F}, \mathbb{P})$ and \(\mathbb{F} = (\mathcal{F}_t)_{t=1}^n\) a filtration of $\mathcal{F}$ (we allow $n = \infty$).
A $\mathbb{F}$-adapted sequence of random variables \((X_t)_{t\in\mathbb{N}_+}\) is a $\mathbb{F}$-adapted martingale if:
(a) \(\mathbb{E}[X_t | \mathcal{F}_{t-1}] = X_{t-1}\) almost surely for all \(t\in\{2, 3, \ldots\}\) and
(b) \(X_t\) is integrable.
If we replace the equality with less-than or greater-than, we call \((X_t)_t\) a supermartingale or submartingale respectively.</p>

<p>A fair betting game is one real-life example, where \(S_t\) is the total money you have after $t$ rounds of a fair game which can be proven to be a martingale.</p>

<p><strong>(Stopping time)</strong>
Let \(\mathbb{F} = (\mathcal{F}_t)_{t\in\mathbb{N}}\) be a filtration.
A random variable \(\tau:\Omega\to\mathbb{N}\cup\{\infty\}\) is a stopping time with respect to \(\mathbb{F}\) if for all \(t\in\mathbb{N}\), the event \(\{\tau \le t\} \in \mathcal{F}_t\) (or we can say \(\mathbb{I}\{\tau \le t\}\) is \(\mathcal{F}_t\)-measurable).
The $\sigma$-algebra at the stopping time $\tau$ is:</p>

\[\mathcal{F}_\tau = \{A \in \mathcal{F}_\infty : A \cap \{\tau \le t\} \in \mathcal{F}_t \text{ for all $t$} \}\]

<p style="background-color: lightblue; padding: 10px;">
$\color{darkblue}{(\text{Theorem 0.1: Doob's optional stopping})}$
Let $\mathbb{F} = (\mathcal{F}_t)_{t\in\mathbb{N}}$ be a filtration and $(X_t)_{t\in\mathbb{N}}$ be an $\mathbb{F}$-adapted martingale and $\tau$ an $\mathbb{F}$-stopping time such that <i>at least one</i> of the following conditions hold:

<br />
1. $\exists n\in\mathbb{N}$ such that $\mathbb{P}(\tau &gt; n) = 0$, e.g. $\tau$ is almost surely bounded.

<br />
2. $\mathbb{E}[\tau] &lt; \infty$ and $\exists c\in\mathbb{R}$ such that for all $t\in\mathbb{N}$, $\mathbb{E}\left[|X_{t+1} - X_t| ~\middle| \mathcal{F}_t\right] \le c$ almost surely.

<br />
3. $\exists c$ such that $|X_{t\land \tau}| \le c$ almost surely for all $t\in\mathbb{N}$.

<br />
Then, $X_\tau$ is almost surely well defined and $\mathbb{E}[X_\tau] = \mathbb{E}[X_0]$.
</p>

<p>One practical consequence of this theorem is that if you play a fair betting game and try to outsmart the casino by stopping at a certain time, say when \(S_t \ge \$100\), you will expect to spend eternity in the casino, e.g. if $S_t$ is the total money you have after $t$ rounds of a fair game, then either $\mathbb{E}[S_\tau] = 0$ or $\mathbb{E}[\tau] = \infty$.</p>

<p style="background-color: lightblue; padding: 10px;">
$\color{darkblue}{(\text{Theorem 0.2: Maximal inequality (also known as Ville's inequality)})}$
Let $(X_t)_{t=0}^\infty$ be a supermartingale with $X_t\ge 0$ almost surely for all $t$.
Then, for any $\epsilon&gt;0$,

$$
    \mathbb{P}\left(\sup_{t\in\mathbb{N}} X_t \ge \epsilon\right) \le \frac{\mathbb{E}[X_0]}{\epsilon}.
$$
<!-- As a remark, this is a generalization of the Markov's inequality. -->
</p>

<!-- _Proof._
Let define an event $$A_n = \{\sup_{t\le n} X_t \ge \epsilon\}$$.
We have that $A_1 \subseteq A_2 \subseteq \ldots$ and that:

$$
    \lim_{n\to \infty} A_n = \{\sup_{t\in\mathbb{N}} X_t \ge \epsilon\}
$$

Furthermore, let us define $$\tau = (n+1) \land \min\{t\le n: X_t \ge \epsilon\}$$.
Clearly, $\tau$ is a stopping time and $\mathbb{E}[X_0] \ge \mathbb{E}[X_\tau]$.
By theorem 0.1, we have that:

$$
\begin{align*}
    \mathbb{E}[X_0] \ge \mathbb{E}[X_\tau] \ge \mathbb{E}[X_\tau\mathbb{I}\{\tau \le n\}] \ge \mathbb{E}[\epsilon\mathbb{I}\{\tau \le n\}] = \epsilon \mathbb{P}(\tau \le n) = \epsilon \mathbb{P}(A_n)
\end{align*}
$$

Here, the second inequality stemmed from the definition of stopping time, i.e. $$\mathbb{E}[X_\tau\mathbb{I}\{\tau > n\}] = 0$$:

$$
    \mathbb{E}[X_\tau] = \mathbb{E}[X_\tau\mathbb{I}\{\tau \le n\}] + \underbrace{\mathbb{E}[X_\tau\mathbb{I}\{\tau > n\}]}_{=0} \ge \mathbb{E}[X_\tau\mathbb{I}\{\tau \le n\}] 
$$ -->

<h2 id="1-law-of-large-numbers">1. Law of large numbers</h2>

<p><strong>(Convergence in probability)</strong>
We say that $X_n$ converges to $X$ in probability if for every $\epsilon &gt; 0$, $\lim_{n \to \infty} \Pr(|X_n - X| &gt; \epsilon) = 0$, denoted by $X_n \overset{p}{\longrightarrow} X$.</p>

<p><strong>(Convergence in $L^r$)</strong>
We say that $X_n$ converges to $X$ in $L^r$ if $\lim_{n \to \infty} \mathbb{E}(|X_n - X|^r) = 0$, denoted by $X_n \overset{L^r}{\longrightarrow} X$.</p>

<p><strong>(Uncorrelated variables)</strong>
Two random variables $X$ and $Y$ are uncorrelated if $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$.</p>

<p style="background-color: lightgreen; padding: 10px;">
$\color{darkgreen}{\text{(Lemma 1.1)}}$
Let $X_1, X_2, \ldots$ be a sequence of uncorrelated random variables with $\mathbb{E}(X_i) &lt; \infty$, then we have that $\mathbb{V}(X_1 + \ldots + X_n) = \mathbb{V}(X_1) + \ldots + \mathbb{V}(X_n)$.
</p>

<hr />
<p style="background-color: lightblue; padding: 10px;">
$\color{darkblue}{(\text{Theorem 1.1: $L^2$ weak law})}$
Let $X_1, X_2, \ldots$ be a sequence of uncorrelated random variables with $\mathbb{E}(X_i) = \mu$ and $\mathbb{V}(X_i) \le C &lt; \infty$.
If $S_n := X_1 + \ldots + X_n$, then $S_n/n \overset{L^2}{\longrightarrow} \mu$.
</p>

<p><em>Proof.</em>
We have that $\mathbb{E}[|S_n/n - \mu|^2] = \mathbb{V}(S_n/n) \le \frac{C/n}{n^2} \to 0$ as $n \to \infty$.
Here, we used Lemma 1.1 to compute the variance of $S_n/n$.</p>

<hr />
<p><strong>(Chebyshev’s inequality)</strong>
For any random variable $X$ and $r &gt; 0$, $\Pr(|X| \ge \epsilon) \le \epsilon^{-r}\mathbb{E}(|X|^r)$.</p>

<p>One small remark is that Chebyshev’s inequality is generally not a sharp inequality (for example try bounding standard normal variables), however, it cannot be improved in <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality#Sharpness_of_bounds">certain cases</a>.</p>

<p style="background-color: lightgreen; padding: 10px;">
$\color{darkgreen}{\text{(Lemma 1.2: Convergence in $L^r$ implies convergence in probability)}}$
For $r &gt; 0$, if $Z_n \overset{L^r}{\longrightarrow} 0$, then $Z_n \overset{p}{\longrightarrow} 0$.
</p>

<p><em>Proof.</em> Since $\mathbb{E}(|Z_n|^r) \to 0$, the proof follows directly from Chebyshev’s inequality applying to the random variable $Z_n$.</p>

<p style="background-color: lightblue; padding: 10px;">
$\color{darkblue}{(\text{Theorem 1.2: Weak law of large numbers})}$
Let $X_1, X_2, \ldots$ be a sequence of uncorrelated random variables with $\mathbb{E}(X_i) = \mu$ and $\mathbb{V}(X_i) \le C &lt; \infty$.
If $S_n := X_1 + \ldots + X_n$, then $S_n/n \overset{p}{\longrightarrow} \mu$.
</p>

<p><em>Proof.</em> Follows from Lemma 1.2 with $r = 2$ and the $L^2$ weak law.</p>

<p>If we relax the assumption of bounded variance, we can still obtain a weak law of large numbers with i.i.d assumption and a weaker variance assumption.</p>

<p style="background-color: lightblue; padding: 10px;">
$\color{darkblue}{(\text{Theorem 1.3: Weak law of large numbers with relaxed variance assumption})}$
Let $X_1, X_2, \ldots$ be a sequence of i.i.d random variables with $x\cdot\Pr(|X_i| &gt; x) \to 0$ as $x \to \infty$.
Let $\mu_n := \mathbb{E}[X_1\cdot\mathbb{I}\{|X_1| \le n\}]$.
Then, $S_n/n - \mu_n \overset{p}{\longrightarrow} 0$.
</p>

<p style="background-color: lightblue; padding: 10px;">
$\color{darkblue}{(\text{Theorem 1.4: Weak law of large numbers for i.i.d random variables})}$
Let $X_1, X_2, \ldots$ be a sequence of i.i.d random variables with $\mathbb{E}[|X_1|] &lt; \infty$.
Let $\mu := \mathbb{E}[X_1]$.
Then, $S_n/n \overset{p}{\longrightarrow} \mu$.
</p>

<p><em>Proof sketch.</em> Use theorem 1.3 and the dominated convergence theorem.</p>

<h2 id="2-borel-cantelli-lemmas-coming-soon">2. Borel-Cantelli lemmas (coming soon)</h2>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[A self-contained summary of some key results in probability theory. I’ll mostly summarize the results from Durrett’s Probability: Theory and Examples, Bandit Algorithms (Lattimore and Szepesvári) and some other sources.]]></summary></entry><entry><title type="html">What is Fisher information?</title><link href="http://localhost:4000/fisher_information/" rel="alternate" type="text/html" title="What is Fisher information?" /><published>2025-03-10T00:00:00+01:00</published><updated>2025-03-10T00:00:00+01:00</updated><id>http://localhost:4000/fisher_information</id><content type="html" xml:base="http://localhost:4000/fisher_information/"><![CDATA[<p>It bugs me that I often have hard time to recall what <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information</a> is.
So let’s write it down here.</p>

<p>At the high-level view, Fisher information shows us how the observations constrain the possible values of the parameter of interest; or to put it another way, how informative the data is about the parameter.
There are several ways to see Fisher information, here are some of them:</p>

<ul>
  <li>Fisher information is the variance of the <a href="https://en.wikipedia.org/wiki/Informant_(statistics)">score</a>.</li>
  <li>Fisher information measures the curvature of the <a href="https://en.wikipedia.org/wiki/Likelihood_function">log-likelihood function</a>.</li>
</ul>

<p>I will try to explain the first point in this post, the second point might follow naturally from the math.
We will consider the setting where the parameter of interest is a vector \(\theta \in \Theta \subseteq \mathbb{R}^d\) and \(X = \{x_t\}_{t=1}^n \in \mathcal{X}\) are the observations that are drawn i.i.d from some unknown distribution \(p_{\theta}\).</p>

<h2 id="score-function">Score function</h2>

<p>We define the score function \(s: \Theta \to \mathbb{R}^d\) as the gradient of the log-likelihood function, i.e.,</p>

\[s(\theta; X) := \nabla_{\theta} \log p(X | \theta) = \sum_{t=1}^n \nabla_{\theta} \log p(x_t | \theta).\]

<p>Although score function seems like a function of the parameter, it is actually a \(\color{darkorange}{\text{random variable}}\) that depends on the data \(X\).
Intuitively, the score function tells us the direction in which we should move in the parameter space \(\Theta\) to increase the likelihood of the observed data.
We typically solve for the maximum-likelihood estimate by setting the score function to zero, i.e., \(\theta_\text{MLE}\) is the solution to \(s(\theta; X) = \mathbf{0}\).</p>

<h2 id="fisher-information">Fisher information</h2>

<p>Before we discuss what Fisher information is, we shall start with the following result which states that the expectation of the score function is zero, i.e.,</p>

\[\begin{align*}
    \mathbb{E}_{X\sim p_{\theta}}[s(\theta; X)] &amp;= \mathbb{E}_{X\sim p_{\theta}}\left[\nabla_{\theta} \log p(X | \theta)\right] \\
    &amp;= \mathbb{E}_{X\sim p_{\theta}}\left[\frac{1}{p(X | \theta)}\nabla_{\theta} p(X | \theta)\right] \\
    &amp;= \int_{\mathcal{X}} p(x | \theta)  \frac{1}{p(x | \theta)}\nabla_{\theta} p(x | \theta) d x \\
    &amp;= \nabla_{\theta} \int_{\mathcal{X}} p(x | \theta) d x \quad\text{(by the Leibniz rule)} \\
    &amp;= \nabla_{\theta} 1 = \mathbf{0}.
\end{align*}\]

<p>Intuitively, this makes sense because the true parameter should be the one that maximizes the likelihood function, and the gradient of the log-likelihood function at the maximum vanishes.</p>

<p>Now, Fisher information is a mapping \(\mathcal{I}(\theta) : \Theta \to \mathbb{R}^{d \times d}\) that measures the variance of the score function at some parameter \(\theta\), i.e.,</p>

\[\begin{align*}
    \mathcal{I}(\theta) &amp;:= \mathbb{V}_{X\sim p_{\theta}}[s(\theta; X)] \\
    &amp;= \mathbb{E}_{X\sim p_{\theta}}\left[(s(\theta; X) - \mathbb{E}_{X\sim p_{\theta}}[s(\theta; X)])(s(\theta; X) - \mathbb{E}_{X\sim p_{\theta}}[s(\theta; X)])^\top\right] \\
    &amp;= \mathbb{E}_{X\sim p_{\theta}}\left[s(\theta; X)s(\theta; X)^\top\right] - \underbrace{\mathbb{E}_{X\sim p_{\theta}}[s(\theta; X)]\mathbb{E}_{X\sim p_{\theta}}[s(\theta; X)]^\top}_{=0~\text{(as shown above)}} \\
    &amp;= \mathbb{E}_{X\sim p_{\theta}}\left[s(\theta; X)s(\theta; X)^\top\right] \\
    &amp;= \mathbb{E}_{X\sim p_{\theta}}\left[\nabla_{\theta} \log p(X | \theta) \nabla_{\theta} \log p(X | \theta)^\top\right]
\end{align*}\]

<p>Unlike score function, Fisher information matrix is a \(\color{darkgreen}{\text{deterministic quantity}}\) that depends only on the parameter.</p>

<p><a href="https://en.wikipedia.org/wiki/Fisher_information#Matrix_form">Alternatively</a>, one can reformulates the Fisher information matrix as the negative expectation of the Hessian of the log-likelihood function, i.e.,</p>

\[\mathcal{I}(\theta) = -\mathbb{E}_{X\sim p_{\theta}}\left[\nabla_{\theta}^2 \log p(X | \theta)\right]\]

<p>Under this formulation, it measure the curvature of the log-likelihood function at the parameter \(\theta\).</p>

<p>It is worth pointing out that \(\theta\) plays <em>two different roles</em> in this context: it is the parameter of the distribution \(p_{\theta}\) that gives rise to the data \(X\) and the argument of the score function \(s(\theta; X)\). 
I think this is the source of confusion for me.
Previously, I thought that the argument of the score function can be any parameter \(\tilde{\theta}\) other than the parameter \(\theta\) that generated the data, but this is not the case.
<!-- Why should these two be the same? -->
<!-- In a way, it tells us how accurate the maximum-likelihood estimate $$\theta_\text{MLE}$$ is;
if the log-likehood function is sharp (curvature is high), we are fairly confident about the estimate, and vice versa, if the log-likelihood function is flat (curvature is small), we are not so sure about the estimate. -->
Generally, Fisher information is a quantitative statement about the amount of information that the data provides about the parameter of interest.
If the observed data is concentrated sharply around a small region, meaning that the underlying log-likelihood function \(p(\cdot \mid \theta)\) which the data is drawn from is sharp (instead of being flat), then only a small number of parameters around \(\theta\) will be consistent with the data.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[It bugs me that I often have hard time to recall what Fisher information is. So let’s write it down here.]]></summary></entry><entry><title type="html">Confidence sets for Gaussian linear models</title><link href="http://localhost:4000/confidence_sets_exact/" rel="alternate" type="text/html" title="Confidence sets for Gaussian linear models" /><published>2025-03-03T00:00:00+01:00</published><updated>2025-03-03T00:00:00+01:00</updated><id>http://localhost:4000/confidence_sets_exact</id><content type="html" xml:base="http://localhost:4000/confidence_sets_exact/"><![CDATA[<p>In statistical inference, we often want to construct confidence sets for the parameter of interest; for example, you may have heard of the <a href="https://en.wikipedia.org/wiki/Confidence_interval">95% confidence interval</a>.
Confidence sets are a way to quantify the uncertainty in our estimates.
At its core, the construction of confidence sets is typically achieved through inverting some concentration inequalities.
<!-- We shall explore this in a setting where the estimator is [asymptotically normal](https://en.wikipedia.org/wiki/Asymptotic_distribution#Central_limit_theorem) under some regularity conditions. --></p>

<p><strong>The setting.</strong>
We observe \(n\) samples \(\{(Y_t, x_t)\}_{t=1}^n\) that follows the linear regression model</p>

\[Y_t = \langle x_t, \theta_\star \rangle + \eta_t,\]

<p>where the design vector \(x_t \in \mathbb{R}^d\) is fixed and known, the noise \(\eta_t\) is drawn i.i.d from a Gaussian \(\mathcal{N}(0, 1)\) and the parameter of interest is \(\theta_\star \in \mathbb{R}^d\).
For simplicity, we assume that the Gram matrix \(V:= X^\top X = \sum_{t=1}^n x_t x_t^\top\) is non-singular (here, \(X = [x_1^\top, \ldots, x_n^\top]^\top\)).
For brevity, we can write $Y = X\theta_\star + \eta$ with $Y\in \mathbb{R}^n$ and $\eta \in \mathbb{R}^n$.</p>

<p>Our goal is to construct a sequence of confidence sets \(C_1, C_2, \ldots\) such that \(\Pr(\exists n \ge 1: \theta_\star \notin C_n) \le \delta\) for some confidence level \(\delta \in (0, 1)\).
We will start with the maximum-likelihood estimator and then construct the confidence set for a certain $n$. 
Under the above assumptions, the maximum-likelihood estimator \(\hat{\theta}\) is given by</p>

\[\hat{\theta} = V^{-1} X^\top Y,\]

<p>which turns out to be a Gaussian because $Y$ is a Gaussian vector as given. 
In particular, we can write</p>

<!-- which is also known to be [asymptotically normal](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Consistency) (well, in fact $$\hat{\theta}$$ is normal already due to the Gaussian noise!). -->

<!-- More precisely, the distribution of $$\hat{\theta}$$ converges to a Gaussian distribution when $$n$$ is large, i.e.,

$$
    \hat{\theta} \sim \mathcal{N}(\theta_\star, n^{-1}\mathcal{I}(\theta_\star)^{-1}).
$$

Above, $$\mathcal{I}(\theta_\star)$$ is the Fisher information matrix at $$\theta_\star$$.
It is also known that $$\hat{\theta}$$ is [asymptotically efficient](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Efficiency), i.e., it achieves the [Cramér-Rao lower bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound)

$$
    \mathbb{V}(\hat{\theta}) = n^{-1} \mathcal{I}(\theta_\star)^{-1}
$$

This is a bless because it allows us to work out the formula for the inversed Fisher information matrix:

$$
\begin{align*}
    \mathbb{V}(\hat{\theta}) &= \mathbb{V}(V^{-1} X^\top Y) \\
    &= \mathbb{V}(V^{-1} X^\top (X \theta_\star + \eta)) \\
    &= V^{-1} X^\top \mathbb{V}(\eta) X V^{-1} \\
    &=  V^{-1} X^\top X V^{-1} \\
    &=  V^{-1}.
\end{align*}
$$

From this we then have that: -->

\[\begin{align*}
    \hat{\theta} &amp;\sim \mathcal{N}(\theta_\star,  V^{-1}) \\
     V^{1/2}(\hat{\theta} - \theta_\star) &amp;\sim \mathcal{N}(0, I_d)
\end{align*}\]

<p>Let \(Z :=  V^{1/2}(\hat{\theta} - \theta_\star)\), then we have \(\|Z\|_2^2 =  \|\hat{\theta}-\theta_\star\|_{V}^2\) follows a \(\mathcal{X}_d^2\)-distribution with \(d\) degrees of freedom.
From <a href="https://stats.stackexchange.com/a/4821/301376">the tail bounds of the \(\chi^2\)-distribution</a>, we have</p>

\[\begin{align*}
    \Pr\left(\|Z\|_2^2 \geq d + 2\sqrt{d\log(1/\delta)} + 2\log(1/\delta)\right) &amp;\leq \delta
\end{align*}\]

<p>So if we define the confidence set \(C_n\) as</p>

\[C_n = \left\{\theta \in \mathbb{R}^d: \|\hat{\theta}-\theta\|_{V}^2 \leq  d + 2\sqrt{d\log(1/\delta)} + 2\log(1/\delta)\right\},\]

<p>then it is a \((1-\delta)\)-confidence set for \(\theta_\star\).</p>

<p>Using union bound, we can construct a sequence of confidence sets \(C_1, C_2, \ldots\) such that \(\Pr(\exists n \ge 1: \theta_\star \notin C_n) \leq \delta\) by choosing a larger confidence set for each \(n\):</p>

\[C_n = \left\{\theta \in \mathbb{R}^d: \|\hat{\theta}-\theta\|_{V}^2 \leq  d + 2\sqrt{d\log(n(n+1)/\delta)} + 2\log(n(n+1)/\delta)\right\}.\]

<p>Geometrically, the confidence set \(C_n\) is an ellipsoid centered at \(\hat{\theta}\) with the principal axes aligned with the eigenvectors of the Gram matrix \(V\).
The length of the axes is determined by the eigenvalues of the Gram matrix \(V\) and the confidence level \(\delta\).
To see this, for any confidence set that is of the form \(C_n = \{\theta :\|\hat{\theta} - \theta\|_V^2 \le \beta\}\), we can rewrite it as</p>

\[C_n = \hat{\theta} + \beta^{1/2} V^{-1/2} \mathcal{B}_d,\]

<p>where \(\mathcal{B}_d\) is the unit \(\ell_2\)-ball in \(\mathbb{R}^d\).
Furthermore, the volume of this ellipsoid is exactly the determinant of \(\beta^{1/2} V^{-1/2}\).
The calculation of this determinant is a bit dense but it invokes the <em>Elliptical potential lemma</em> (see Lemma 19.4 in <a href="https://tor-lattimore.com/downloads/book/book.pdf">Bandit Algorithms</a>) which turns out to scale as \(\mathcal{O}\left(\frac{\log n}{n}\right)^{d/2}\).
This basically agrees with the classical concentration results like Hoeffding’s inequality, Bernstein’s inequality, etc, where the extra \(\log n\) factor arises due to us requiring the time-uniform guarantee.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[In statistical inference, we often want to construct confidence sets for the parameter of interest; for example, you may have heard of the 95% confidence interval. Confidence sets are a way to quantify the uncertainty in our estimates. At its core, the construction of confidence sets is typically achieved through inverting some concentration inequalities.]]></summary></entry><entry><title type="html">The upper confidence bound algorithm</title><link href="http://localhost:4000/ucb/" rel="alternate" type="text/html" title="The upper confidence bound algorithm" /><published>2025-02-03T00:00:00+01:00</published><updated>2025-02-03T00:00:00+01:00</updated><id>http://localhost:4000/ucb</id><content type="html" xml:base="http://localhost:4000/ucb/"><![CDATA[<!-- ## A primer on bandits -->
<p>Bandits are a class of reinforcement learning (RL) problems where a learner has to choose between different actions, each of which has an unknown reward.
The key difference between bandits and RL problems is that in bandits, there is an absence of states or transition between states.
Through interaction with the environment, the learner learns the reward distribution of each action and tries to maximize its cumulative reward.</p>

<p>In this blog post, we will discuss the upper confidence bound (UCB) algorithm, which is a popular algorithm for solving bandit problems.</p>

<h2 id="a-primer-on-stochastic-bandits-lattimore-and-szepesvári-2020">A primer on stochastic bandits (Lattimore and Szepesvári, 2020)</h2>

<p>A stochastic bandit is a collection of distributions \(\nu = \{P_a : a\in \mathcal{A} \}\) in which:</p>

<ul>
  <li>The learner interacts with the environment for $n$ rounds. We call $n$ the time horizon.</li>
  <li>At each round $t$, the learner selects an action \(A_t \in \mathcal{A}\) and receives a reward \(X_t \sim P_{A_t}\).
In this blog post, we assume that \(X_t\) is a \(1\)-sub-Gaussian random variable with mean \(\mu_a := \mathbb{E}[X | A = a]\) and variance of at most \(1\).</li>
  <li>The sequence of outcomes is denoted by \(A_1, X_1, A_2, X_2, \ldots, A_n, X_n\).</li>
</ul>

<p>In additional, it has to satisfy two statistical assumptions:</p>

<ol>
  <li>The conditional distribution $X_t$ given $A_1, X_1, \ldots, A_{t-1}, X_{t-1}, A_t$ is \(P_{A_t}\), capturing the fact that the environment samples rewards from the distribution corresponding to the action selected by the learner.</li>
  <li>The conditional law of action $A_t$ given $A_1, X_1, \ldots, A_{t-1}, X_{t-1}$ is \(\pi_t(\cdot \mid A_1, X_1, \ldots, A_{t-1}, X_{t-1})\), where $\pi_t$ is a probability distribution over the action space $\mathcal{A}$.
The learner is thus characterized by the sequence of probability distributions $\pi_1, \pi_2, \ldots, \pi_n$.
Importantly, the assumption here is that the learner cannot use the future to make decisions.</li>
</ol>

<p>The usual metric to evaluate the performance of the learner is the <em>expected regret</em>, which is defined as the difference between the cumulative reward of the optimal policy and the cumulative reward of the learner:</p>

\[\begin{align}
R_n := n \mu^* - \mathbb{E}\left[\sum_{t=1}^n X_t\right]
\end{align}\]

<p>where \(\mu^* := \max_{a\in \mathcal{A}} \mu_a\) is the mean of the optimal action.</p>

<p>From the above formulation, the optimal policy is indeed the one that selects the action with the highest expected reward at each round.
The goal of the learner is to <em>minimize the expected regret</em> by devising a good policy.
It is worth emphasizing that this is not an optimization problem; the learner does not know the reward distributions \(\nu\) and has to learn them through interaction with the environment.</p>

<h2 id="the-ucb-algorithm">The UCB algorithm</h2>

<p>The crux of this algorithm relies on the <em>optimism principle</em>, which states that the learner should be optimistic about the reward of each action.
Why is this a good idea?
We shall see that this philosophy leads to a good balance between exploration and exploitation.</p>

<p>In particular, the UCB algorithm maintains an upper confidence bound for each action, denoted by \(\text{UCB}_a(t-1)\).
At each round, the learner selects the action with the highest UCB and receives the corresponding reward.
Concretely, we have the following steps for every round \(t=1,2,\ldots,n\):</p>

<ol>
  <li>Select action \(A_t = \underset{a\in \mathcal{A}}{\text{argmax}}~\text{UCB}_a(t-1)\).</li>
  <li>Observe reward \(X_t \sim P_{A_t}\).</li>
</ol>

<p>What should we choose as the UCB?
By the optimism principle, \(\text{UCB}_a(t-1)\) should be an overestimate of \(\mu_a := \mathbb{E}[X | A = a]\) and should converge to it as the number of rounds increases.
Specifically, we define the UCB as follows:</p>

\[\begin{align}
\text{UCB}_a(t-1, \delta) = \hat{\mu}_a(t-1) + \color{darkred}{\underbrace{\sqrt{\frac{2\log(1/\delta)}{T_a(t-1)}}}_{\text{Exploration bonus}}} \tag{1} \label{eq:ucb}
\end{align}\]

<p>where \(T_a(t-1)\) is the number of times action \(a\) has been selected up to round \(t-1\) and \(\delta\) is a confidence parameter.
The term \(\hat{\mu}_a(t-1)\) is the empirical mean of the rewards of action \(a\) up to round \(t-1\):</p>

\[\begin{align}
\hat{\mu}_a(t-1) := \frac{1}{T_a(t-1)}\sum_{s=1}^{t-1} \mathbb{I}(A_s = a)X_s,
\end{align}\]

<p>One small remark is that this choice of UCB is not unique; there are other ways to define the UCB.
In essence, this form of UCB in Eq. \(\ref{eq:ucb}\) is almost an inverse of the <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding inequality</a> with one caveat that the count \(T_a(t-1)\) in the denominator is a random variable as well.
We can think of the term \(\sqrt{\frac{2\log(1/\delta)}{T_a(t-1)}}\) as an <em>exploration bonus</em> that encourages the learner to explore actions that have not been selected frequently (notice the inverse relationship between \(T_a(t-1)\) and the exploration bonus).
And at the same time, choosing \(\delta\) is a delicate problem; a small value of \(\delta\) leads to a more aggressive exploration, while a large value of \(\delta\) leads to premature abandonment of the optimal action.</p>

<h2 id="theoretical-guarantees-of-ucb">Theoretical guarantees of UCB</h2>

<p>The UCB algorithm has been shown to have a sublinear regret bound:</p>

<p style="background-color: lightblue; padding: 10px;">
    <strong>Theorem (Lattimore and Szepesvári, 2020).</strong> If $\delta=1/n^2$, then the regret of the UCB algorithm is bounded by
    \[
        R_n = \mathcal{O}(\sqrt{nk\log n})
    \]
</p>

<p>As an intermediate step, we first prove a weaker result:</p>

<p><strong>Lemma.</strong> If $\delta=1/n^2$, then the regret of the UCB algorithm is bounded by</p>

\[\begin{align}
R_n \leq 3\sum_{i=1}^k \Delta_i + \sum_{i: \Delta_i \geq 0} \frac{16\log(n)}{\Delta_i}
\end{align}\]

<p>where \(\Delta_i := \mu^* - \mu_i\) is the gap between the optimal action and action \(i\).</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Bandits are a class of reinforcement learning (RL) problems where a learner has to choose between different actions, each of which has an unknown reward. The key difference between bandits and RL problems is that in bandits, there is an absence of states or transition between states. Through interaction with the environment, the learner learns the reward distribution of each action and tries to maximize its cumulative reward.]]></summary></entry><entry><title type="html">Why I think mathematics is beautiful</title><link href="http://localhost:4000/why_i_do_math/" rel="alternate" type="text/html" title="Why I think mathematics is beautiful" /><published>2024-12-13T00:00:00+01:00</published><updated>2024-12-13T00:00:00+01:00</updated><id>http://localhost:4000/why_i_do_math</id><content type="html" xml:base="http://localhost:4000/why_i_do_math/"><![CDATA[<p>A friend of mine asked me why I think mathematics is beautiful.
I believe the exact answer does not matter, but it is important to have an answer.</p>

<p>Years ago, I visited the Rijksmuseum in Amsterdam—it is one of the biggest museums in the Northern European region.
In my memories, there was something <em>beautiful</em> about the Dutch oil paintings from three centuries ago.
The composition is simple and direct, usually composed of a natural landscape under a light blue sky and cotton-like clouds.</p>

<center>
    <figure>
        <img src="../assets/images/geest_bridge.png" alt="View near the Geest Bridge by Johan Hendrik Weissenbruch" style="width: 80%; display: inline-block;" />
        <figcaption><small>"View near the Geest Bridge" by Johan Hendrik Weissenbruch (1824–1903), oil on panel, 1868.</small></figcaption>
    </figure>
</center>

<p>There is nothing particularly profound about these kinds of paintings, and they were not meant to showcase how skillful the painters who painted them were.
But the paintings spoke to me, they was telling me that the appreciation for being present in the moment is a universal human experience.
Our lives always move forward and are filled with uncertainties, but the beauty of this world, this reality is always there, waiting for us to notice it.
It is easy to find nothing in everything, but it takes effort to find everything in nothing.</p>

<p>Like arts, mathematics reveals a dimension of us.
In my early years of education, mathematics is usually portrayed as a strictly logical and rigorous discipline.
It was taught as if mathematics is an absolute truth, and the only way to understand it is to follow the rules.
However, the more I study mathematics, the more I realize that mathematics is just a vehicle for our mind to explore the unknown, and it is allowed to be imperfect and <a href="https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems">incomplete</a>.
Throughout the history of mathematics, we evolved our understanding as a collective, <a href="https://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox">old flaws</a> were <a href="https://en.wikipedia.org/wiki/Measurable_space">patched</a>, and <a href="https://en.wikipedia.org/wiki/Continuum_hypothesis">new ones</a> were discovered.
It is reflection that, in the end, we are all human beings, we are all struggling to make sense of this reality.
This realization is what makes mathematics beautiful to me.
<!-- It also changes the way I approach mathematics, I always try to push the boundaries of my intuition and my mental representation of mathematical objects, and leave the rigor to the end.  --></p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[A friend of mine asked me why I think mathematics is beautiful. I believe the exact answer does not matter, but it is important to have an answer.]]></summary></entry><entry><title type="html">Modern hashing made simple</title><link href="http://localhost:4000/modern_hashing/" rel="alternate" type="text/html" title="Modern hashing made simple" /><published>2024-12-08T00:00:00+01:00</published><updated>2024-12-08T00:00:00+01:00</updated><id>http://localhost:4000/modern_hashing</id><content type="html" xml:base="http://localhost:4000/modern_hashing/"><![CDATA[<!-- Introduction -->
<p>This is a short personal note on <a href="https://epubs.siam.org/doi/10.1137/1.9781611977936.33">“Modern Hashing Made Simple” by Bender et al. (2024)</a>.
I’m going to present this paper for the <a href="https://kyng.inf.ethz.ch/courses/AGAO24seminar/">Advanced Graph Algorithms and Optimization Seminar (Fall 2024)</a> at ETH Zürich.
My personal goal is to understand the paper well enough to present it in a clear and engaging way.</p>

<p><strong>The set-up.</strong>
We want to store \(n\) items in a hash table that supports three operations: querying, insertion, and deletion.
We assume that the <a href="https://en.wikipedia.org/wiki/Word_(computer_architecture)">word size</a> \(w=\Theta(\log n)\).
Our goal is to design such a hash table that is <em>time-efficient</em> and <em>space-efficient</em>.
By time-efficient, I mean that the execution time of each operation should be sublinear in \(n\), ideally \(\mathcal{O}(1)\) (on average or with high probability).
And by space-efficient, I mean that the total number of bits used by the hash table should be of order \((1+o(1))n w\), because otherwise we can achieve constant time execution by storing all items in an long array with zero hash collisions.</p>

<p>Here’s a small anecdote to start with:
Most hashing algorithms with space \((1+o(1))n w\), such as <a href="https://en.cppreference.com/w/cpp/container/unordered_map">std::unordered_map in C++</a> or <a href="https://en.wikipedia.org/wiki/Linear_probing">linear probing</a>, support querying, insertion, and deletion operations in <strong>\(\mathcal{O}(1)\) time on average</strong>.
However, they can degrade to <strong>\(\mathcal{O}(n)\) time in the worst case</strong>.
Many modern work has led to hashing algorithms that guarantee <strong>\(\mathcal{O}(1)\) time with high probability</strong> for all operations.
This is a much stronger guarantee than the average-case constant time of traditional hashing algorithms, since the probability of a bad event is exponentially small in the input size (usually being bounded by \(1/\text{poly}(n)\)).</p>

<p>However, these modern hashing algorithms are often more complex and thus harder to explain than traditional hashing algorithms.
The paper by Bender et al. presents a simple and practical hashing algorithm that achieves the said strong guarantee.</p>

<h2 id="the-roadmap">The roadmap</h2>

<p>Throughout this work, we assume to have access to a fully random hash function \(f\), meaning that the output of \(f\) is <em>uniformly distributed</em> over the range of possible hash values.
The paper is structured as follows:</p>

<ol>
  <li>
    <p><strong>Slow partition hash table.</strong>
We start with a simple hash table that is partitioned into blocks of size \(\mathcal{O}(\log^3(n))\).
The time complexity for insertion is constant w.h.p., but the time complexity for querying and deletion is linear in block size.
The space complexity is \(nw + \mathcal{O}(n)\) bits.</p>
  </li>
  <li>
    <p><strong>Indexed partition hash table.</strong>
Building on the slow partition hash table, we introduce an index that allows us to query and delete in worst-case constant time (an improvement!) but insertion time is constant on average (a degradation).
We will make use of a data structure called <a href="https://en.wikipedia.org/wiki/B-tree">B-tree</a> to maintain the index.
The space complexity is \(nw + \mathcal{O}(n\log \log n)\) bits, which has an extra overhead of \(\log \log n\) bits compared to the slow partition hash table.</p>
  </li>
  <li>
    <p><strong>Partition hash table.</strong>
We will further introduce an extra layer of processing to achieve constant time w.h.p. for insertion and deletion operations.
The core idea is to <a href="https://en.wikipedia.org/wiki/Amortized_analysis">amortize</a> the cost of insertion, which is occasionally expensive, over multiple insertions and deletions.
We will use a data structure called <a href="https://en.wikipedia.org/wiki/Trie">k-tries</a> to maintain a growing buffer that store the upcoming insertions and deletions, and a shrinking buffer that executes these operations in parallel.
The space complexity is \(nw + \mathcal{O}(n\log \log n)\) bits.</p>
  </li>
</ol>

<h2 id="slow-partition-hash-table">Slow partition hash table</h2>

<p>As a starting point, we will use an array of size \(n\) to represent our hash table and partition it into \(n/\log^3(n)\) blocks of size \(\log^3(n)\).</p>

<p>To insert a new item \(x\), we first hash it to a random block \(f(x)\) and insert it to the first empty slot \(k\) from the left of the block.
We can maintain this left-aligned invariant by maintaining a \(\mathcal{O}(\log \log n)\)-bit counter for each block that keeps track of the number of items in the block.
So each time we insert an item, we insert it to the slot \(k+1\) and increment the counter by one.
However, if the block is full, we rebuild the entire hash table by choosing a new random hash function and rehashing all items.
For deletion, we linearly scan the block to find the item and remove it, and move the right-most item to the empty slot and decrement the counter by one.
For querying, we linearly scan the block to find the item.</p>

<p>The time complexity for deletion and querying is linear in the block size, which is \(\log^3(n)\).
The time complexity for insertion is constant unless we need to rebuild the hash table.
If we introduce a small slack factor \(\color{darkorange}{c\log^2(n)}\) to the block size, the probability of having a block with more than \(\log^3(n) + \color{darkorange}{c\log^2(n)}\) items is very small, e.g. at most \(1/\text{poly}(n)\).
This fact is a consequence of the <a href="https://en.wikipedia.org/wiki/Chernoff_bound">Chernoff’s bound</a>.
So on average, the runtime of insertion is constant with high probability.
Overall, the space complexity is \(nw + \mathcal{O}(n)\) bits.</p>

<h2 id="indexed-partition-hash-table">Indexed partition hash table</h2>

<p>In the previous step, we simply neglected deletions/queries in order to achieve constant time insertion w.h.p.
To make deletions/queries efficient, we will introduce an index for each block that allows us to find the location of an item in constant time.</p>

<p>One naive way to index a block is to use a hash function \(g:B\rightarrow [\log^3(n)]\) that maps each item in block \(B\) to a unique index in the block.
The probability of having a collision for any pair \(x, y\in B\) is:</p>

\[\begin{align}
    \Pr(g(x) = g(y)) = \frac{1}{\log^3(n)}
\end{align}\]

<p>By union bound, the probability of any collisions occuring is at most \(\text{card}(B)^2 \frac{1}{\log^3(n)} = \log^3(n)\), which is a vacuous bound.</p>

<p>Instead, we assign all items \(x\) within a block to distinct \(\Theta(\log\log n)\)-bit fingerprints \(g(x)\).
Here, we specifically choose \(g: B \rightarrow [\log^9(n)]\), which will allow us to show that the probability of having a fingerprint collision is at most \(1/\log^3(n)\) (the proof is similar to the previous one).
If the fingerprints are not distinct, we redo the assignment of fingerprints using a new random hash function.</p>

<p>Next, we will maintain a data structure for each block called <em>query-mapper</em> that maps each fingerprint \(g(x)\) to the corresponding position \(i\in \mathcal{O}(\log^3 n)\) where the item \(x\) is stored in that block.
In itself, the query-mapper is nothing but a hash table that maps fingerprints to positions (isn’t this the problem we are trying to solve?).
However, notice that the fingerprints and positions are \(\Theta(\log\log n)\) bits long, which means that the key-value pairs are small enough to be stored in a special data structure called B-tree.
The exact details of how B-tree works are not important, but the key idea is that it allows us to query/insert/delete in worst-case constant time with a small memory overhead of \(m\log\log n\) bits, where \(m=\text{polylog}(n)\) is the number of key-value pairs in the B-tree.</p>

<p>The query-mapper will interact with our hash table in the following way:</p>
<ul>
  <li>
    <p>To insert an item \(x\), we first check if the fingerprint \(g(x)\) is already in the query-mapper.
If it is, there is a fingerprint collision and we need to rebuild the index from scratch (for this block only).
Otherwise, we insert the item to the first empty position \(k\) and add the key-value pair \((g(x), k)\) to the query-mapper.</p>
  </li>
  <li>
    <p>To query an item \(x\), we simply look up the fingerprint \(g(x)\) in the query-mapper to find the position \(k\) where the item is stored.</p>
  </li>
  <li>
    <p>To delete an item \(x\), we look up the fingerprint \(g(x)\) in the query-mapper to find the position \(k\) where the item is stored, remove the item from the hash table and potentially move the right-most item \(x'\) to the empty slot to keep the hash table left-aligned, and delete the key-value pair \((g(x), k)\) and update the position for \(x'\) in the query-mapper.</p>
  </li>
</ul>

<p>Overall, the time complexity for querying and deletion is constant in the worst case.
The time complexity for insertion is constant on average however.
We need to rebuild the index only when there is a fingerprint collision (which happens with probability of at most \(1/\log^3(n)\)).
The time complexity for rebuilding the index is \(\mathcal{O}(\log^3(n))\), which is the same as the block size.
There is also a chance that we need to rebuild the index several times in a row, but the expected time spent on rebuilding is still constant:</p>

\[\sum_{i=1}^{\infty} \mathcal{O}(\log^3 n) \cdot \mathcal{O}(1/\log^3 n)^i = \mathcal{O}(1)\]

<p>The space complexity is \(nw + \mathcal{O}(n\log\log n)\) bits, which has an extra overhead of \(\log\log n\) bits compared to the slow partition hash table.</p>

<h2 id="partition-hash-table">Partition hash table</h2>

<p>The last step is to make insertion and deletion operations constant time w.h.p while maintaining the constant time worst-case guarantee for querying.
We will use a trick called <em>deamortization</em> to achieve this.
The core idea is to deamortize the cost of insertion, so if they formerly required \(\mathcal{O}(1)\) time on average, they now require \(\mathcal{O}(1)\) time w.h.p.
One interesting remark is that this trick can be extended to apply to any hash tables.</p>

<p>We maintain two type of buffers: a growing buffer and a shrinking buffer.
We will use a data structure called k-tries to implement these buffers, with \(k=\sqrt{n}\), and they store up to \(n^{1/4}\) items of \(w\) bits each.
Again, the exact details of how k-tries work are not important, but the key idea is that they allow us to insert/delete/query in worst-case constant time with a small memory overhead of \(\mathcal{O}(n^{0.75}\log n)\) bits.</p>

<p>The growing buffer receives the upcoming insertions/deletions while the shrinking buffer has their items removed as they are executed.
For each insertion/deletion being added to the growing buffer, we spend a constant \(c&gt;1\) amount of time executing the operations in the shrinking buffer in the actual hash table.
We will guarantee that whenever the growing buffer reaches size \(n^{1/4}\), the shrinking buffer is empty, and they switch roles.</p>

<p>We will prove the following claim: <em>Any batch \(Q\) of \(n^{1/4}\) insertions/deletions can be executed in total \(\mathcal{O}(n^{1/4})\) time w.h.p</em>.</p>

<p>Why is this so crucial?
The shrinking trie is slowly being cleared out as new operations occur.
If every batch of \(n^{1/4}\) operations can be done in \(O(n^{1/4})\) total time w.h.p, that means each operation’s amortized cost is constant (since \(O(n^{1/4}) / n^{1/4} = O(1)\)).
As a result, by proving this claim, we ensure the shrinking trie will be emptied at a rate that keeps the entire hash table’s performance near constant time per update. 
In other words, if we can show that even a fairly large batch of operations doesn’t cause a time blow-up, then doing a little work per operation (just constant time on average) suffices to keep things running smoothly.</p>

<p><em>Proof sketch of the claim:</em>
Let \(A_1, A_2, \ldots, A_{n/\log^3(n)}\) be the number of insertion/deletion operations from \(Q\) that get mapped to the \(i\)-th block.
We’ve shown that, with high probability, \(A_i \le \mathcal{O}(\log^3(n))\) for all \(i\).
Let \(X_1, X_2, \ldots, X_{n/\log^3(n)}\) be the amount of time needed to execute the operations from \(A_i\) on the \(i\)-th block.
In the worst case, we can show that \(X_i \le \mathcal{O}(\log^7(n))\) with high probability (this is simply because there are at most \(\log^3(n)\) operations to execute in \(A_i\), the cost of rebuilding is \(\mathcal{O}(\log^3(n))\), and w.h.p there will be \(\mathcal{O}(\log n)\) rebuild attempts).
Furthermore, we have \(\mathbb{E}[X_i] \le \mathcal{O}(|A_i|)\) by linearity of expectation and the fact that each operation in \(A_i\) takes constant time on average (shown in the previous section).
By Chernoff’s bound, we can show that \(X:=\sum_{i=1}^{n/\log^3(n)} X_i \le \mathcal{O}(n^{1/4})\) with high probability.</p>

<p>The space complexity is \(nw + \mathcal{O}(n\log\log n)\) bits, which is the same as the indexed partition hash table.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[This is a short personal note on “Modern Hashing Made Simple” by Bender et al. (2024). I’m going to present this paper for the Advanced Graph Algorithms and Optimization Seminar (Fall 2024) at ETH Zürich. My personal goal is to understand the paper well enough to present it in a clear and engaging way.]]></summary></entry><entry><title type="html">Jane Street Puzzle - August 2024</title><link href="http://localhost:4000/jane_street_aug24/" rel="alternate" type="text/html" title="Jane Street Puzzle - August 2024" /><published>2024-09-03T00:00:00+02:00</published><updated>2024-09-03T00:00:00+02:00</updated><id>http://localhost:4000/jane_street_aug24</id><content type="html" xml:base="http://localhost:4000/jane_street_aug24/"><![CDATA[<!-- Personal remarks -->
<p>I decided to slack off my exam studying with Jane Street’s puzzle.
I first read the problem in early August, forgot about it, and then submitted my answer on August 16—and got it right on the first try! (username: Khanh Vu).
This month’s puzzle involves straightforward state analysis with a nice probability garnish.</p>

<h2 id="the-problem">The problem</h2>

<p>Aaron and Beren play a game on an infinite complete binary tree where each edge is independently labeled as \(\mathrm{A}\) with probability \(p\) and \(\mathrm{B}\) otherwise.
The game starts with Aaron at the root, and the players take turns moving a shared token down the tree.
On each turn, the active player chooses one of the two child nodes to move the token to.
If the token traverses an edge labeled B, Beren wins; otherwise, Aaron wins.</p>

<p>The goal is to find the <a href="https://en.wikipedia.org/wiki/Infimum_and_supremum">infimum</a> of all probabilities \(p\) for which Aaron has a nonzero probability of winning. 
<!-- This infimum represents the smallest probability $$p$$ at which Aaron’s chance of winning is not zero, considering that Beren can choose a path that includes at least one edge labeled B. --></p>

<p>You can read the full problem statement here at: <a href="https://www.janestreet.com/puzzles/tree-edge-triage-index/">www.janestreet.com/puzzles/tree-edge-triage-index/</a>.</p>

<h2 id="my-approach">My approach</h2>

<p>What does it mean for Aaron to win?
The universe that Aaron and Beren reside in is controlled by a demon, I assume.
This demon loves upside down binary trees, so it generates an infinite number of binary trees out of boredom.
Among these trees, if there exists a tree that allows Aaron to keep playing indefinitely, then we say that he wins with non-zero probability.</p>

<p>The demon grows a tree in the following fashion:
A tree starts with a root, and at each time step, every leaf node grows two child nodes.
The edges connecting each parent to its children are labeled \(\mathrm{A}\) with probability \(p\), and \(\mathrm{B}\) otherwise.
As \(p\) gets closer to \(1\), it is more likely for Aaron to win.
So it makes sense that the problem asks for some sort of a lower bound for such \(p\).</p>

<center>
    <figure>
        <img src="../assets/images/jane_street/js00.png" alt="cases" style="display: inline-block;" />
        <figcaption> <em>Figure 1. When it's Aaron turn, there are four scenarios.</em> </figcaption>
    </figure>
</center>

<p>We can zoom in and examine the local picture.
Assuming that it’s Aaron turn and the token is at a node \(u\), the only state that makes him lose immediately is when the two edges are labeled \((\mathrm{B}, \mathrm{B})\) (state \(4\) in Figure \(1\)).
Otherwise, he moves the token down to the child node \(v_1\) or \(v_2\), depending on the prospect of the two options (or it is possible at all to move the token to \(v_i\) if the edge is labeled \(\mathrm{B}\)).</p>

<center>
    <figure>
        <img src="../assets/images/jane_street/js01.png" alt="cases" style="display: inline-block;" />
        <figcaption> <em>Figure 2. When it's Beren turn, only the first state allows the game to continue.</em> </figcaption>
    </figure>
</center>

<p>When the token is in Beren’s control, Aaron’s only chance of winning is if both edges are labeled \((\mathrm{A}, \mathrm{A})\).
In any other case, Beren wins by moving the token along the edge labeled \(\mathrm{B}\).</p>

<p>Formally, let \(w_u\) be the event that Aaron wins if he starts his turn at node \(u\).
Now, node \(u\) has children \(v_1\) and \(v_2\).
Let \(w_{v_1}\) be the event that Aaron wins if it’s Beren’s turn and the token is at node \(v_1\), and similarly, let \(w_{v_2}\) represent the event that Aaron wins if it’s Beren’s turn and the token is at node \(v_2\).
Then, we have the following recursive relation:</p>

\[\begin{align}
    \mathbb{P}(w_u) &amp;= p^2 \cdot \mathbb{P}(w_{v_1} \lor w_{v_2}) + p(1-p)\cdot\mathbb{P}(w_{v_1}) + (1-p)p\cdot\mathbb{P}(w_{v_2}) &amp;\text{(1)} \\
    \mathbb{P}(w_v) &amp;= p^2 \cdot \mathbb{P}(w_{u_1} \land w_{u_2}) = p^2 \cdot \mathbb{P}(w_u)^2 &amp;\text{(2)} 
\end{align}\]

<p>Eq. (2) used the fact that \(w_{u_1}\) and \(w_{u_2}\) are independent events.</p>

<p>To break things down for clarity, Eq. (1) and (2) are composed of terms capturing the states in Fig. 1 and 2 respectively.
For instance, the term \(p^2 \cdot \mathbb{P}(w_{v_1} \lor w_{v_2})\) in Eq. (1) accounts for the state where both edges connecting \(u\) to children \(v_1, v_2\) are labeled \((\mathrm{A}, \mathrm{A})\).
In this case, Aaron wins (e.g. \(w_u\) holds) if either \(w_{v_1}\) or \(w_{v_2}\) holds.</p>

<p>Now, from the sum rule of probability, we know that:</p>

\[\newcommand{\ind}{\perp\!\!\!\!\perp} 
\begin{align}
    \mathbb{P}(w_{v_1} \lor w_{v_2}) &amp;= \mathbb{P}(w_{v_1}) + \mathbb{P}(w_{v_2}) - \mathbb{P}(w_{v_1} \land w_{v_2}) \\
    &amp;= 2\mathbb{P}(w_v) - \mathbb{P}(w_v)^2 &amp;(\text{since $w_{v_1} \ind w_{v_2}$})
\end{align}\]

<p>For brevity, let \(x:= \mathbb{P}(w_u)\).
Combining the fact above and plugging (2) into (1), we arrive at the following identity:</p>

\[\begin{align}
    x &amp;= 2x^2p^3 - x^4p^6
\end{align}\]

<p>Let the left-hand side be defined as \(f(x) := x\) and the right-hand side as \(g(x) := 2x^2p^3 - x^4p^6\). 
Graphically, the problem is now to find a value \(p_0 \in (0, 1)\) such that for all \(p &gt; p_0\), the function \(f(x)\) intersects with \(g(x)\) at some \(x:=\mathbb{P}(w_u) &gt; 0\).
<a href="https://www.wolframalpha.com/input?i=x+%3D+2x%5E2p%5E3+-+x%5E4p%5E3+%3E+0">Solving for \(p_0\)</a>, we get \(\boxed{\frac{\sqrt{3}}{2^{5/6}}}\approx 0.972\).</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[I decided to slack off my exam studying with Jane Street’s puzzle. I first read the problem in early August, forgot about it, and then submitted my answer on August 16—and got it right on the first try! (username: Khanh Vu). This month’s puzzle involves straightforward state analysis with a nice probability garnish.]]></summary></entry><entry><title type="html">Counting with expectation</title><link href="http://localhost:4000/expectation_counting/" rel="alternate" type="text/html" title="Counting with expectation" /><published>2024-08-08T00:00:00+02:00</published><updated>2024-08-08T00:00:00+02:00</updated><id>http://localhost:4000/expectation_counting</id><content type="html" xml:base="http://localhost:4000/expectation_counting/"><![CDATA[<p><strong>Problem 10 (Individual test, ARML 2013)</strong>.
For a positive integer \(n\), let \(C(n)\) equal the number of pairs of consecutive \(1\)’s in the binary representation of \(n\).
For example, \(C(183)=C(10110111_2)=3\).
Compute \(C(1) + C(2) + \ldots + C(256)\).</p>

<p><em>Solution</em>.
You can go ahead with doing casework and will eventually discover some recurrent formulas like in the <a href="https://www.arml.com/ARML/arml_2019/public_contest_files/2009_2014_book/ARML_2009_2014.pdf">intended solution</a>.</p>

<p>However, this problem has a shorter solution that leverages <a href="https://en.wikipedia.org/wiki/Expected_value#Properties:~:text=0.-,Linearity%20of%20expectation,-%3A%5B34">linearity of expectation</a>.
In short, linearity of expectation means:</p>

\[\mathbb{E}[\mathrm{X}_1 + \mathrm{X}_2 + \ldots + \mathrm{X}_n] = \mathbb{E}[\mathrm{X}_1] + \mathbb{E}[\mathrm{X}_2] + \ldots + \mathbb{E}[\mathrm{X}_n]\]

<p>This holds regardless of whether these random variables are dependent or not.
Now, we can rewrite the target quantity in term of an expectation:</p>

\[\sum_{i=1}^{256} C(i) = 256 \cdot \mathbb{E}[C(\mathrm{X})],\]

<p>for \(\mathrm{X}\) being a discrete RV drawn uniformly from \(\{1, 2, \ldots, 256\}\).
Let \(\text{bin}(\mathrm{X})\) denotes the binary representation of \(\mathrm{X}\) and \(\text{bin}(\mathrm{X})_i\) denotes the \(i\)-th bit from the left.
Let \(Z_i = [\text{bin}(\mathrm{X})_i = 1 \land \text{bin}(\mathrm{X})_{i+1} = 1]\) be an indicator function that takes value 1 if both \(i\)-th and \((i+1)\)-th bits are \(1\)’s, and otherwise takes value 0.
Given this, we can write</p>

\[\begin{align}
\mathbb{E}[C(\mathrm{X})] &amp;= \mathbb{E}\left[\sum_{i=1}^7 Z_i\right] \\
&amp;= \sum_{i=1}^7 \mathbb{E}[Z_i] &amp;\text{(Linearity of expectation)}\\ 
&amp;= \sum_{i=1}^7 \mathbb{P}\left(\text{bin}(\mathrm{X})_i = 1 \land \text{bin}(\mathrm{X})_{i+1} = 1\right)\\
&amp;= 7\cdot \frac{1}{2^2} = \frac{7}{4}
\end{align}\]

<p>The answer would then be \(256 \cdot \frac{7}{4} =\boxed{448}\).
Cute trick isn’t it?</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Problem 10 (Individual test, ARML 2013). For a positive integer \(n\), let \(C(n)\) equal the number of pairs of consecutive \(1\)’s in the binary representation of \(n\). For example, \(C(183)=C(10110111_2)=3\). Compute \(C(1) + C(2) + \ldots + C(256)\).]]></summary></entry><entry><title type="html">An upper bound for the Kullback-Leibler divergence</title><link href="http://localhost:4000/kl_upper_bound/" rel="alternate" type="text/html" title="An upper bound for the Kullback-Leibler divergence" /><published>2024-07-23T00:00:00+02:00</published><updated>2024-07-23T00:00:00+02:00</updated><id>http://localhost:4000/kl_upper_bound</id><content type="html" xml:base="http://localhost:4000/kl_upper_bound/"><![CDATA[<p>Zürich is so hot now in the night that I couldn’t fall asleep.
While lying on the bed, I arrived at an interesting upper bound for the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence (relative entropy)</a> between two Bernoulli distributions.
In this case, the KL divergence is expressed as:</p>

\[D_\text{KL}(p \parallel q) := p\log\left(\frac{p}{q}\right) + (1-p)\log\left(\frac{1-p}{1-q}\right)\]

<p>Here, \(p, q\) denote the parameters of the two distributions.
I assume \(p, q \in (0, 1)\) and \(\log(\cdot)\) is the natural logarithm function.
For other cases like when \(p&gt;0\) and \(q=0\), the KL divergence diverges to infinity.
I can show that, in this setting, the following holds:</p>

\[D_\text{KL}(p \parallel q) \le \frac{(p - q)^2}{(1-q)q} =: \frac{2D_\text{TV}(p, q)^2}{\mathbb{V}[X]}\]

<p>Above, \(D_\text{TV}(\cdot, \cdot)\) is the <a href="https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures">variational distance</a> between two distributions and \(X\) is a random variable distributed according to Bernoulli(\(q\)).
My result is in fact a looser bound as compared to the <a href="https://en.wikipedia.org/wiki/Pinsker%27s_inequality">reversed Pinsker’s inequality</a>.</p>

<p>For the proof, I define a function \(f(q):=D_\text{KL}(p \parallel q)\) with \(p\) kept as a constant.</p>

<p><strong>Corollary 1.</strong>
The function \(f\) is convex on \((0, 1)\).</p>

<p><em>Proof</em>.
\(f\) is convex if and only if \(\forall \lambda \in [0, 1]\) and \(\forall q_1, q_2 \in \text{dom}(f)\):</p>

\[f(\lambda q_1 + (1-\lambda) q_2) \le \lambda f(q_1) + (1-\lambda) f(q_2)\]

<p>Indeed this is true since:</p>

<p>\(\begin{align}
    f(\lambda q_1 + (1-\lambda) q_2) &amp;= p \log\left(\frac{p}{\lambda q_1 + (1-\lambda) q_2}\right) + (1-p) \log\left(\frac{1-p}{1 - \lambda q_1 - (1-\lambda) q_2}\right) \\
    &amp;= -p \log\left(\frac{\lambda q_1 + (1-\lambda) q_2}{p}\right) - (1-p) \log\left(\frac{\lambda (1-q_1) + (1-\lambda) (1-q_2)}{1-p}\right) \\ 
    &amp;\le \lambda \left(p\log\frac{p}{q_1} + (1-p)\log\frac{1-p}{1-q_1} \right) + (1-\lambda) \left(p\log\frac{p}{q_2} + (1-p)\log\frac{1-p}{1-q_2} \right) \\
    &amp;= \lambda f(q_1) + (1-\lambda) f(q_2)
\end{align}\)
The inequality above is due to the convexity of \(-\log(\cdot)\) and Jensen’s inequality.</p>

<p><strong>Corallary 2.</strong>
\(\forall q_1, q_2 \in \text{dom}(f)\), it holds that:</p>

\[f(q_1) \ge f(q_2) + f'(q_2)(q_1 - q_2)\]

<p>where \(f'\) is the first-order derivative of \(f\) at \(q_2\).</p>

<p><em>Proof</em>.
Given that \(f\) is convex on its domain, we have:</p>

\[\begin{align}
    f(\lambda q_1 + (1-\lambda) q_2) &amp;\le \lambda f(q_1) + (1-\lambda) f(q_2) \\
    f(q_2 + \lambda(q_1 - q_2)) - f(q_2) + \lambda f(q_2) &amp;\le \lambda f(q_1) \\
    \frac{f(q_2 + \lambda(q_1 - q_2)) - f(q_2)}{\lambda (q_1 - q_2)} (q_1 - q_2) + f(q_2) &amp;\le f(q_1) &amp;\text{(Dividing $\lambda$ both sides)} \\
    \lim_{\lambda \rightarrow 0} \frac{f(q_2 + \lambda(q_1 - q_2)) - f(q_2)}{\lambda (q_1 - q_2)} (q_1 - q_2) + f(q_2) &amp;\le f(q_1) \\
    f'(q_2) (q_1 - q_2) + f(q_2) &amp;\le f(q_1)
\end{align}\]

<p>We can explicitly derive the derivative of \(f\) at a given \(q\):</p>

\[f'(q) = \frac{p - q}{(q - 1)q}\]

<p>Putting everything together, we have the desired upper bound:</p>

\[\begin{align}
    f(p) &amp;\ge f(q) + f'(q)(p - q) &amp;\text{(Corollary 2)}\\
    f'(q)(p - q) &amp;\ge f(q) &amp;\text{($f(p)=0$ by definition)}\\
    \frac{(q - p)^2}{(1 - q)q} &amp;\ge f(q):= D_\text{KL}(p \parallel q)
\end{align}\]

<p>This bound is particularly useful when it comes to deriving some concentration bounds for the KL divergence with respect to \(q\).
I will discuss this matter in another blog post.
It’s 1:44AM now and I need to go to bed for real this time.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Zürich is so hot now in the night that I couldn’t fall asleep. While lying on the bed, I arrived at an interesting upper bound for the KL divergence (relative entropy) between two Bernoulli distributions. In this case, the KL divergence is expressed as:]]></summary></entry><entry><title type="html">Mathematical optimization cheatsheet</title><link href="http://localhost:4000/opt_cheatsheet/" rel="alternate" type="text/html" title="Mathematical optimization cheatsheet" /><published>2024-04-26T00:00:00+02:00</published><updated>2024-04-26T00:00:00+02:00</updated><id>http://localhost:4000/opt_cheatsheet</id><content type="html" xml:base="http://localhost:4000/opt_cheatsheet/"><![CDATA[<p>In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms.</p>

<p>In the first part, I will recite the definitions of convexity, smoothness, and strong convexity, which are fundamental properties in the realm of convex optimization on differentiable functions.
In the second part, we will relax to the case of non-differentiable functions and discuss subgradients, proximal operators and other concepts that are essential in the analysis of non-smooth optimization algorithms.</p>

<h1 id="1-convex-optimization">1. Convex optimization</h1>

<p>Let us define a function $f: \text{dom}(f)\rightarrow \mathbb{R}$ with $\text{dom}(f) \subseteq \mathbb{R}^d$.
We assume that $f$ is differentiable, meaning that the gradient $\nabla f(x) \in \mathbb{R}^d$ exists $\forall x \in \text{dom}(f)$.</p>

<center>
    <figure>
        <img src="../assets/images/smooth_strong_convex.png" alt="smoothness and strong convexity illustration" style="display: inline-block;" />
        <figcaption> Figure 1. Smoothness and strong convexity play complementary roles in bounding the function values. </figcaption>
    </figure>
</center>

<h2 id="11-convexity">1.1 Convexity</h2>

<p><strong>(a) Definition.</strong>
A function $f$ is convex if its domain is a convex set and if for all $x, y \in \text{dom}(f)$ and $\theta \in [0, 1]$, we have</p>

\[f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)\]

<p>This definition is also known as the Jensen’s inequality.
Oftentimes, it is more convenient to work with one of the following equivalent definitions:</p>

<p><strong>(b) (First-order characterization)</strong>
\(f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle\)</p>

<p>Geometrically, this definition implies that the function lies above its tangent line at any point (see Figure 1).
Or, equivalently, the vector \((\nabla f(x), -1) \in \mathbb{R}^{d+1}\) defines a supporting hyperplane to the epigraph of \(f\) at \((x, f(x))\).</p>

<p><strong>(c) (Second-order characterization)</strong>
Suppose that $f$ is twice differentiable.
A function $f$ is convex if its domain is a convex set and if for all $x \in \text{dom}(f)$, we have</p>

\[\nabla^2 f(x) \succeq 0\]

<p>where $\nabla^2 f(x)$ is the Hessian matrix of $f$ at $x$.</p>

<p><strong>(d) (Monotonicity of the gradient)</strong>
A function $f$ is convex if its domain is a convex set and if for all $x, y \in \text{dom}(f)$, we have</p>

\[\langle \nabla f(y) - \nabla f(x), y-x \rangle \geq 0\]

<h2 id="12-smooth-functions">1.2 Smooth functions</h2>

<p><strong>(a) Definition.</strong>
Let $X\subseteq \text{dom}(f)$ be a convex set and let $L &gt; 0$. A function $f$ is $L$-smooth on $X$ if for all $x, y \in X$, we have</p>

\[f(y) \leq f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2}\|y-x\|^2\]

<p>One can think of this definition as Lipschitz continuity of the gradient map.
When $f$ is twice differentiable, smoothness implies that the largest eigenvalue of the Hessian is bounded by $L$, e.g. \(\|\nabla^2 f(x)\| \leq L\).
If $f$ is $L$-smooth, then $f$ is also $L’$-smooth for all $L’ \geq L$.
Alternatively, we can use the following equivalent definition:</p>

<p><strong>(b) (Lipschitz continuity of the gradient)</strong>
\(\| \nabla f(x) - \nabla f(y)\| \leq L\| x-y\|\) for all \(x, y \in X\).</p>

<p><strong>(c)</strong>
\(g(x) = \frac{L}{2} x^\top x - f(x)\) is convex.</p>

<p><strong>(d)</strong>
\(\langle \nabla f(x) - \nabla f(y), x-y \rangle \leq L\|x-y\|^2\) for all \(x, y \in X\).</p>

<p>These definitions are not actually equivalent, but they are related.
We have the following relations:</p>

\[[b] \Rightarrow [a] \Leftrightarrow [c] \Leftrightarrow [d]\]

<p>If $f$ is additionally convex, then \([a] \Rightarrow [b]\).</p>

<h2 id="13-strong-convexity">1.3 Strong convexity</h2>

<p><strong>(a) Definition.</strong>
Let $X\subseteq \text{dom}(f)$ be a convex set and let $\mu &gt; 0$. A function $f$ is $\mu$-strongly convex on $X$ if for all $x, y \in X$, we have</p>

\[f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle + \frac{\mu}{2}\|y-x\|^2\]

<p>The notion of strong convexity complements the notion of smoothness.
Strong convexity implies that the function grows at least quadratically.
Moreover, $f$ is strongly convex implies that $f$ is strictly convex and has a unique minimizer.
And similarly to smoothness, if $f$ is $\mu$-strongly convex, then $f$ is also $\mu’$-strongly convex for all $\mu’ \leq \mu$.
Alternatively, we can use the following equivalent definition:</p>

<p><strong>(b)</strong>
\(\| \nabla f(x) - \nabla f(y)\| \geq \mu\| x-y\|\) for all \(x, y \in X\).</p>

<p><strong>(c)</strong>
\(g(x) = f(x) - \frac{\mu}{2} x^\top x\) is convex.</p>

<p><strong>(d)</strong>
\(\langle \nabla f(x) - \nabla f(y), x-y \rangle \geq \mu\|x-y\|^2\) for all \(x, y \in X\).</p>

<p>The relations are as follows:</p>

\[[b] \Leftarrow [a] \Leftrightarrow [c] \Leftrightarrow [d]\]

<p>Figure 1 above illustrates the complementary roles of smoothness and strong convexity in bounding the function values.</p>

<h1 id="2-non-smooth-optimization">2. Non-smooth optimization</h1>

<p>In this section, we define the function \(f\) similar to before, but we make no assumption of differentiability.</p>

<h2 id="21-non-smooth-functions">2.1 Non-smooth functions</h2>

<p><strong>(a) Definition.</strong> 
If \(f\) is not differentiable, or if the gradients of \(f\) exists but are not Lipschitz continuous, then we say that \(f\) is non-smooth.
This definition is exactly the negation of the smoothness property.
Examples of non-smooth functions include the absolute value function \(f(x) = |x|\), the hinge loss function \(f(x) = \max(0, 1-x)\), or many activation functions in neural networks such as the ReLU function \(f(x) = \max(0, x)\).</p>

<p><strong>(b) Subgradients.</strong>
A vector \(g \in \mathbb{R}^d\) is a subgradient of \(f\) at \(x\) if for all \(y \in \text{dom}(f)\), we have</p>

\[f(y) \geq f(x) + \langle g, y-x \rangle\]

<p>We define a subdifferential \(\partial f(x)\) denoting the set of all subgradients of \(f\) at \(x\).
Geometrically, \(g\) is a subgradient of \(f\) at \(x\) if the hyperplane defined by \((g, -1)\in\mathbb{R}^{d+1}\) supports the epigraph of \(f\) at \((x, f(x))\).</p>

<p><strong>(c) Properties of subgradients.</strong></p>

<ul>
  <li>If \(f\) is differentiable at \(x\in \text{dom}(f)\), then \(\partial f(x) \subseteq \{\nabla f(x)\}\), meaning that the subdifferential either contains the gradient or is empty.</li>
  <li>If \(f\) is convex, then \(\partial f(x)\) is non-empty for all \(x\in \text{int(dom)}(f)\).</li>
  <li>If \(\text{dom}(f)\) is convex and \(\partial f(x) \neq \emptyset\) for all \(x\in \text{dom}(f)\), then \(f\) is convex.</li>
  <li><strong>(Subgradient optimality condition)</strong> If \(\mathbf{0} \in \partial f(x)\), then \(x\) is a global minimum of \(f\).</li>
</ul>

<h2 id="22-dual-norms">2.2 Dual norms</h2>

<p><strong>(a) Definition.</strong>
Given a general norm \(\|\cdot\|\), the dual norm \(\|\cdot\|_*\) is defined as</p>

\[\|z\|_* = \max_{\|x\|\leq 1} \langle z, x \rangle\]

<p><strong>(b) Examples.</strong>
In general, \(\|\cdot\|_p\) is the dual norm of \(\|\cdot\|_q\) if \(\frac{1}{p} + \frac{1}{q} = 1\).</p>

<ul>
  <li>The dual norm of the \(\ell_1\) norm is the \(\ell_\infty\) norm.</li>
  <li>The dual norm of the \(\ell_2\) norm is the \(\ell_2\) norm.</li>
</ul>

<p><strong>(c) Interpretation of dual norms.</strong>
(WIP)</p>

<h2 id="23-proximal-operators">2.3 Proximal operators</h2>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms.]]></summary></entry></feed>