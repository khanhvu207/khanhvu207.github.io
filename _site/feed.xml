<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-27T01:02:35+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">EntropyMaxing</title><author><name>Khánh Vũ</name></author><entry><title type="html">Rate-distortion theory</title><link href="http://localhost:4000/rate_distortion/" rel="alternate" type="text/html" title="Rate-distortion theory" /><published>2023-12-19T00:00:00+01:00</published><updated>2023-12-19T00:00:00+01:00</updated><id>http://localhost:4000/rate_distortion</id><content type="html" xml:base="http://localhost:4000/rate_distortion/"><![CDATA[<p>In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem.
The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression.
I will try to keep the material short and concise.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li><a href="#1-definitions">Definitions</a></li>
  <li><a href="#2-the-rate-distortion-theorem">Rate-distortion theorem</a>
    <ol>
      <li><a href="#21-converse-part">Converse part</a></li>
      <li><a href="#22-direct-part">Direct part</a></li>
    </ol>
  </li>
</ol>

<h2 id="1-definitions">1. Definitions</h2>

<p>We have a sequence \(X^n=(X_1, \dots, X_n)\) with \(X_i\) being sampled IID according to a source distribution \(P_X\). 
We also assume \(X_i\) takes on values from a finite alphabet \(\mathcal{X}\).
Let us define an encoder-decoder pair with encoding function \(f_n: \mathcal{X}^n \rightarrow \{1,\dots, 2^{nR}\}\) and decoding function \(\phi_n: \{1,\dots, 2^{nR}\} \rightarrow \hat{\mathcal{X}}^n\), where \(R\) is the rate (denoting bits per source symbol) and \(\hat{\mathcal{X}}\) is the reconstruction alphabet.
In plain words, the encoder \(f_n\) maps \(X^n\) to a index of a codebook \(\mathcal{C}\) consisting of \(2^{nR}\) codewords, and the decoder \(\phi_n\) returns the corresponding codeword \(\hat{X}^n\) which is one of the rows in \(\mathcal{C}\).</p>

<p>One example of the scheme above is the representation of real numbers in computers.
We would need infinite precision, thus infinite bits, to represent a number in \(\mathbb{R}\), so we instead truncate it to \(32\)-bit floating point, resulting in a codebook consisting \(2^{32}\) possible numbers.
Such a scheme is also called quantization.
We essentially quantize numbers in \(\mathbb{R}\) using a finite set of rational numbers \(\mathbb{Q}\).</p>

<center>
    <figure>
        <img src="" alt="rate-distortion diagram" style="width:500px; display: inline-block;" />
        <figcaption>...</figcaption>
    </figure>
</center>

<p>To measure the quality of the chosen scheme, which is a pair \((f_n, \phi_n)\), we can measure the distortion between the input and the reconstruction as \(d(X^n,\hat{X}^n)=\frac{1}{n}\sum_{i=1}^n d(X_i, \hat{X}_i)\). Here, \(d:\mathcal{X}\times \hat{\mathcal{X}} \rightarrow \mathbb{R}^+\) is the distortion measure.
Some popular choices of distortion measure includes:</p>

<ul>
  <li><em>Hamming distortion</em> that is given by \(d(x,\hat{x}) = [x \neq \hat{x}]\). Here, \([\cdot]\) denotes the indicator function.</li>
  <li><em>Squared-error distortion</em> that is given by \(d(x,\hat{x})=(x-\hat{x})^2\).</li>
</ul>

<p>For simplicity, I assume that Hamming distortion is used in this post.
Moving on, the following introduces several definitions highlighting the interplay between rate \(R\) and distortion \(D\).</p>

<p><u>Definition 1:</u> 
The <strong>rate distortion pair \((R,D)\)</strong> is <em>achievable</em> if there exists a sequence of \((f_n,\phi_n)\) with \(\lim_{n\rightarrow \infty} \mathbb{E} d(X^n, \phi_n(f_n(X^n))) \leq D\).</p>

<p><u>Definition 2:</u> 
The <strong>rate distortion region</strong> is the closure of all achievable \((R,D)\).</p>

<center>
    <figure>
        <img src="" alt="rate-distortion function" style="width:500px; display: inline-block;" />
        <figcaption>...</figcaption>
    </figure>
</center>

<p><u>Definition 3:</u> 
The <strong>rate distortion function \(R(D)\)</strong> is the <em>infimum</em> over all rates \(R\) such that \((R,D)\) is achievable for a given distortion \(D\).</p>

<p><u>Definition 4:</u> 
The <strong>information rate distortion function</strong> is defined as</p>

\[R^{(I)}(D) = \underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})\]

<p>Where \(I(\cdot;\cdot)\) is the mutual information function and the minimum is taken over all possible \(P_{\hat{X}|X}\), assuming that the source distribution \(P_X\) is fixed. 
Alternatively, we can also define \(R^{(I)}(D)\) as the minimization over the joint \(P_{\hat{X}X}\) as long as the marginal matches the given \(P_X\).</p>

<h2 id="2-the-rate-distortion-theorem">2. The rate-distortion theorem</h2>

<p>In his foundational work in information theory, Claude Shannon stated a key result in rate-distortion theory:</p>

<p><u>Theorem 1:</u></p>

\[R(D)=R^{(I)}(D)=\underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})\]

<p>This gives the original definition of rate distortion function \(R(D)\) an operational value as an optimization problem. Spelling things out, the theorem has two parts:</p>

<ol>
  <li><strong>Direct part:</strong> If \(R&gt;R^{(I)}(D)\) then \((R,D)\) is achievable.</li>
  <li><strong>Converse part:</strong> If \((R,D)\) is achievable then \(R\geq R^{(I)}(D)\).</li>
</ol>

<h3 id="21-converse-part">2.1. Converse part</h3>

<p>The proof for the converse is rather straighforward and depends only on some properties of \(R^{(I)}(D)\), namely \(R^{(I)}(D)\) is monotonically non-increasing, convex, and continuous. For interested readers, you can refer to the proofs of these properties in <em>Elements of Information Theory by Thomas &amp; Cover</em>.</p>

<p><u>Claim 1:</u>
\(nR \geq I(X^n; \hat{X}^n)\)</p>

<p><u>Proof of claim 1:</u></p>

\[\begin{align}
I(X^n; \hat{X}^n) &amp;\leq I(X^n; f_n(X^n)) &amp;&amp;\text{(Data processing ineq.)}\\
&amp;= H(f_n(X^n)) &amp;&amp;(H(f_n(X^n)|X^n)=0)\\
&amp;\leq nR &amp;&amp;(\text{Codebook is of size } 2^{nR})
\end{align}\]

<p><u>Proof of the converse part:</u></p>

\[\begin{align}
nR &amp;\geq I(X^n; \hat{X}^n) &amp;&amp;\text{(Claim 1)}\\
&amp;= H(X^n) - H(X^n|\hat{X}^n)\\
&amp;= \sum_i H(X_i) - \sum_i H(X_i|X^{i-1}, \hat{X}^n) &amp;&amp;\text{(Chain rule)}\\
&amp;\geq \sum_i H(X_i) - \sum_i H(X_i|X^{i-1}) &amp;&amp;\text{(Conditioning reduces entropy)}\\
&amp;= \sum_i I(X_i; \hat{X}_i)\\
&amp;\geq \sum_i R^{(I)} (\mathbb{E} d(X_i, \hat{X}_i)) &amp;&amp;(\text{By def. of } R^{(I)}(D))\\
&amp;= n \sum_i \frac{1}{n} R^{(I)} (\mathbb{E} d(X_i, \hat{X}_i))\\
&amp;\geq n R^{(I)} \left(\frac{1}{n} \sum_i \mathbb{E} d(X_i, \hat{X}_i) \right) &amp;&amp;(\text{Convexity of } R^{(I)}(D) \text{ and Jensen's ineq.})\\
&amp;= n R^{(I)} (\mathbb{E} d(X^n, \hat{X}^n)) &amp;&amp;\text{(By def. of distortion)}\\ 
&amp;\geq n R^{(I)} (D) &amp;&amp;(\text{Monotonicity of } R^{(I)}(D) \text{ and the claim that } \mathbb{E} d(X^n, \hat{X}^n) \leq D)\\
\end{align}\]

<p>The above completes the proof of the converse part.
Intuitively, the converse says that if you give me a wonderful scheme that achieves distortion \(\leq D\), I will show that your rate \(R\) must be at least \(R^{(I)}(D)\).</p>

<h3 id="22-direct-part">2.2. Direct part</h3>

<p>In contrast to the converse part, the proof for the direct part is tricky.
Nevertheless, it is actually quite similar to the direct part of noise-channel coding theorem.
The crux of this proof would follow the random codebook construction with some small differences.</p>

<p>I will start by laying out some ingredients for the main proof.</p>

<p><u>Lemma 1:</u>
Given \(N\) IID Bernoulli \(p\) variables \(X_1,\dots,X_n\), we have that</p>

\[P(\text{at least 1 success}) = P((X_1=1) \lor \cdots \lor (X_n=1)) \rightarrow 1 \text{ if } np\rightarrow \infty\]

<p><u>Proof of lemma 1:</u></p>

\[\begin{align}
P(\text{at least 1 success}) &amp;= 1-P(\text{failure in all trials}) \\
&amp;\geq 1-(1-p)^n \\
&amp;\geq 1-\exp(-np) \rightarrow 1 \text{ as } np\rightarrow \infty
\end{align}\]

<p><u>Lemma 2:</u>
Given a function \(g:\mathcal{X}\rightarrow \mathbb{R}^+\) and a sequence \(\underline{x}=(x_1,\dots,x_n)\) with \(x_i\in\mathcal{X}\), the following holds:</p>

\[\underline{x} \in \mathcal{T}_{\varepsilon}^{(n)} (P_X)\Rightarrow \frac{1}{n} \sum_i g(x_i) \leq (1+\varepsilon) \mathbb{E}g(X)\]

<p>Here, \(\mathcal{T}_{\varepsilon}^{(n)}(P_X)\) is a strongly typical set with tolerence \(\varepsilon\).</p>

<p><u>Proof of lemma 2:</u> The proof follows naturally from the definition of a strongly typical set.</p>

<p><u>Lemma 3:</u>
Given two sequences of random variables \(\{\tilde{X}_i\}, \{\tilde{Y}_i\}\) that are drawn IID from \(P_X\) and \(P_Y\) respectively, where \(P_X\) and \(P_Y\) are the marginals of some \(P_{XY}\), then</p>

\[Pr((\tilde{X}, \tilde{Y})\in \mathcal{A}_{\varepsilon}^{(n)}(P_{XY})) \leq 2^{-n(I(X;Y)-3\varepsilon)}\]

<p>Where \(\mathcal{A}_{\varepsilon}^{(n)}(P_{XY})\) is a weakly typical set with tolerence $\varepsilon$.</p>

<p><u>Lemma 4:</u>
Given \(0&lt;\varepsilon'&lt;\varepsilon\), if \(\underline{x}\in \mathcal{T}_{\varepsilon'}^{(n)}(P_X)\) and \(\{Y_i\} ~\) IID from \(P_Y\) then:</p>

\[Pr((\underline{x}, \underline{Y})\in \mathcal{T}_{\varepsilon}^{(n)}(P_{XY})) \geq 2^{-n(I(X;Y)+4\delta_{XY})}\]

<p>The lemma 3 has been proven in the previous blog post, and the proof of lemma 4 is omitted since it is quite complicated.</p>

<p><strong>Proof of the direct part:</strong>
Given the source distribution \(P_X\),
we fix the conditional probability \(P_{\hat{X}|X}\) such that it satisfies \(\mathbb{E}_{P_{X\hat{X}}} d(X,\hat{X}) \leq D\). 
We will show that if \(R &gt; I_{P_{X\hat{X}}}(X;\hat{X}) + \tilde{\epsilon}\) then \((R,D)\) is achievable.
If we are able to prove this, the proof in the case \(R&gt;R^{(I)}(D)\) would follow naturally.</p>

<p>Since we already have \(P_X\) and \(P_{\hat{X}\vert X}\), we also get the output symbol probability \(P_\hat{X}\) using the law of total probability:</p>

\[P_{\hat{X}}(\hat{x}) = \sum_{x} P(\hat{x}|x) P(x)\]

<p>Similar to the direct part of noisy-channel coding, we consider a random codebook \(\mathcal{C}\in \mathcal{\hat{X}}^{2^{nR}\times n}\) where the row elements follow \(P_{\hat{X}}\).
In this proof, the decoder follows the strongly typical decoding scheme;
given a source sequence \(\underline{x}\in \mathcal{X}^n\), we search for the existence of a row \(j\) in \(\mathcal{C}\) such that \((\underline{x}, \hat{x}(j))\in \mathcal{T}_{\epsilon}^{(n)}(P_X \circ P_{\hat{X}|X})\).
Also, to be more precise with the constant terms invovled, we assume that \(0&lt;\epsilon'&lt;\epsilon&lt;\tilde{\epsilon}\).
We define the event of “success” for the decoder \(\phi_n\) if there exists such \(j\) satisfying the above statement, and if so, the reconstruction for \(\underline{x}\) is then \(\hat{x}(j)\) (the row \(j\) in \(\mathcal{C}\)).
In this case, the distortion is upper bounded by:</p>

\[\begin{align}
d(\underline{x}, \hat{x}(j)) &amp;\leq \mathbb{E}[d(x,\hat{x})](1+\epsilon) &amp;\text{(Lemma 2)}\\
&amp;=D(1+\epsilon) &amp;\text{(By assumption)}
\end{align}\]

<p>In the case where there is no such \(j\) exist, the distortion is assumed to be at most \(d_\max \doteq \underset{x,\hat{x}}{\max} d(x,\hat{x})\).
For this, the expected distortion, assuming that \(\underline{x}, \mathcal{C}\) are random, is upper bounded by:</p>

\[\mathbb{E}[d(X,\hat{X})] \leq \text{Pr(success)}D(1+\epsilon) + \text{Pr(failure)} d_\max\]

<p>For a fixed codebook \(\mathcal{C}\),</p>

\[\mathbb{E}[d(X,\hat{X})\vert \mathcal{C}] \leq \text{Pr}(\text{success}\vert \mathcal{C})D(1+\epsilon) + \text{Pr}(\text{failure}\vert \mathcal{C}) d_\max\]

<p>The plan now is quite similar to the noisy-channel coding’s proof: if I can upper bound the expected distortion \(\mathbb{E}[d(X,\hat{X})]\) (averaged over \(\mathcal{C}\)) by some amount \(\gamma\), then there must exist a codebook \(\mathcal{C}^*\) that actually achieves the distortion of at most \(\gamma\).
To achieve this plan, we shall turn our attention to \(\text{Pr}(\text{success})\) which we can eventually show to happen with almost guarantee.
There are two ways we can factor \(\text{Pr}(\text{success})\) with the law of total probability:</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;= \sum_{\mathcal{C}} \text{Pr}(\mathcal{C}) \text{Pr}(\text{success} | \mathcal{C}) &amp;\text{(1)}\\
\text{Pr}(\text{success}) &amp;= \sum_{\underline{x}} \text{Pr}(\underline{x}) \text{Pr}(\text{success} | \underline{x}) &amp;\text{(2)}
\end{align}\]

<p>Notice that the right-hand side of \((2)\) depends on the choice of \(\mathcal{C}\).
The important point is that we are effectively recasting the randomness of \(\mathcal{C}\) to the randomness of \(\mathcal{X}\), which eases the analysis.
We then have the following decomposition:</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;= \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi) + \sum_{\xi \notin \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi)\\
&amp;\geq \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi)
\end{align}\]

<p>From lemma 1 and 4, we know that:</p>

\[\begin{align}
P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi) &amp;\geq 1- \exp(-2^{nR}2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}})})\\
&amp;= 1- \exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)}) 
\end{align}\]

<p>Thus,</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;\geq \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) 1-\exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)})\\
&amp;= 1- \exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)}) 
\end{align}\]

<p>If \(R &gt; I(X;\hat{X})+4\delta_{X\hat{X}}\), we can drive \(\text{Pr}(\text{success})\rightarrow 1\) for some large \(n\). Consequently, this tells us that if \(R &gt; I(X;\hat{X})+4\delta_{X\hat{X}}\) then \(\mathbb{E}[d(X,\hat{X})]\leq D(1+\epsilon)\), implying the existence of codebook \(\mathcal{C}^*\) whose distortion is at most \(D(1+\epsilon)\).
To get the prove for \(R&gt;R^{(I)}(D)\), we just need to set \(P_{\hat{X}\vert X}\) to the minimizer of the functional \(R^{(I)}(D)\), and set \(\delta_{X\hat{X}}=\frac{\tilde{\epsilon}}{4}\).</p>

<h2 id="3-calculating-the-rate-distortion-function">3. Calculating the rate-distortion function</h2>

<p>Some tips about the actual calculation of the \(R(D)\) given a input distribution and the distortion function.</p>

<p><u>Proposition 1:</u>
All the conditional probability \(P_{\hat{X}|X}\) that satisfies \(\mathbb{E}[d(X,\hat{X})]\leq D\) form a convex set.</p>

<p><u>Proposition 2:</u>
For a fixed \(P_X\), we have that \(\underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})\) is a convex optimization problem.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem. The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression. I will try to keep the material short and concise.]]></summary></entry><entry><title type="html">Noisy-channel coding theorem</title><link href="http://localhost:4000/channel_coding_proof/" rel="alternate" type="text/html" title="Noisy-channel coding theorem" /><published>2023-11-21T00:00:00+01:00</published><updated>2023-11-21T00:00:00+01:00</updated><id>http://localhost:4000/channel_coding_proof</id><content type="html" xml:base="http://localhost:4000/channel_coding_proof/"><![CDATA[<p>Here is my second attempt to understand the proof for the channel coding theorem.
My initial exposure to this concept was through Professor Amos Lapidoth’s lecture in Information Theory I.
His presentation, though concise, was packed with ingenious methods that were challenging to fully comprehend at time.
To better understand it, I shall employ the Feynman technique - explaining the concept to someone else.</p>

<h2 id="1-communication-through-a-noisy-medium">1. Communication through a noisy medium</h2>

<p>Imagine Bob’s wife, Alice, is traveling the universe for her company annual trip.
She wants to send her lovely husband some gorgious photos she’ve taken during the last couples of days.
From Alice’s space ship, these photos are first encoded as binary strings (e.g. \(00101101..\)), then converted into radio waves and finally emitted to Bob’s computer on Earth.</p>

<p>However, every interplanetary species know how deadly and quirky the space is, and signals travel through this medium is of no exception.
Due to cosmic radiation, the sent bit has a probability \(p\in [0,1]\) of being flipped; \(0\) is received as \(1\) at Bob’s computer given some chance \(p&gt;0\) for instance.</p>

<figure>
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Binary_symmetric_channel_%28en%29.svg/1920px-Binary_symmetric_channel_%28en%29.svg.png" alt="noisy communication scheme" style="width:500px; display: inline-block;" />
</figure>

<p>To circumvent such situation, the ship can send the same bit multiple times just to be sure that the receiver can recover the message faithfully.
For example, Alice can instead send \(010\) as \(000111000\) (each bit is duplicated three times). The encoder at Alice’s spaceship would then follow the mapping:</p>

\[\begin{align}
0 &amp;\rightarrow 000\\
1 &amp;\rightarrow 111
\end{align}\]

<p>On Earth, Bob can leverage a voting scheme to decode the message, say, for every block of 3 bits, decodes it as the majority bit in that block (e.g. \(011 \rightarrow 1\)).</p>

<p>To be more concrete, let’s assume \(p=0.2\).
Before, the probability of incorrectly receiving a bit \(P_e\), say bit \(0\) is received and decoded as \(1\), equals to \(0.2\).
Now, with the redundant bits introduced, the error happens when \(000\) is received as one of \(\{011, 101, 110, 111\}\) (Bob will decode it as a \(1\)), and this event occurs with probability \(P_e \approx 0.104\)!
It seems like the probability of error \(P_e\) will vanish as we keep sending the same bit for a large number of times.
Wonderful right?
However, the reduction in \(P_e\) would come with a cost!</p>

<center>
    <figure>
        <img src="" alt="prob of error decay rate" style="width:500px; display: inline-block;" />
        <figcaption>...</figcaption>
    </figure>
</center>

<p>Every time the space ship sends a bit through the radio channel, this service charges Alice some amount of money.
In case the encoder duplicates a bit \(K\) times, the rate of communication would then be \(R=\frac{1}{K}\) bits per channel use.
To send a single bit reliably, the ship emits \(K\) times the radio signal to transmit it to Earth.
From the previous, we speculate that only when \(R \rightarrow 0\) then \(P_e\rightarrow 0\).
Bad news for the family’s bank account!</p>

<p>But the couple have to worry no more, Claude Shannon dropped a phenomenal paper in 1948 which states that \(\forall \varepsilon&gt;0\), there exists an encoder-decoder pair that allows us to make \(P_e &lt; \varepsilon\) without the need to drive \(R\rightarrow 0\).
In plain words, you can make the probablity of error arbitrarily small and still having a reasonable communication rate.</p>

<h2 id="2-noisy-channel-coding-theorem">2. Noisy-channel coding theorem</h2>

<p>Formally, the basic scheme consists of a set of messages \(\mathcal{M}\), an encoder \(f: \mathcal{M} \rightarrow \mathcal{X}^n\) with \(n\) be the block length, and a decoder \(\phi: \mathcal{Y}^n \rightarrow \mathcal{M} \cup \{?\}\).
The symbol \(?\) denotes the failure case when the decoder doesn’t know the original message.
The communication rate is \(R=\frac{\log |\mathcal{M}|}{n}\) bits per channel use, assume the messages are drawn uniformly.
We can index \(\mathcal{M}\) by \(\{1,\dots, \lfloor 2^{nR} \rfloor\}\).
Finally, the noisy channel \(W(\mathcal{Y}|\mathcal{X})\) is memoryless and assumed to be fixed in this setting.</p>

<p><u>Definition 1:</u> 
The probability of error associated with message \(m\) is denoted by \(\lambda_m\).
Verbosely,</p>

\[\begin{align}
\lambda_m &amp;= \sum_{\mathbf{y} \in\mathcal{Y}^n | \phi(\mathbf{y}) \neq m} P(Y = \mathbf{y} | X = \mathbf{x}(m))\\
&amp;= \sum_{\mathbf{y} \in\mathcal{Y}^n | \phi(\mathbf{y}) \neq m} \prod_{i=1}^n W(y_i | x_i(m)) &amp;\text{(Channel is memoryless)}
\end{align}\]

<p>Next, we define \(\lambda_\max = \underset{m\in \mathcal{M}}{\max} \lambda_m\), the maximum chance of getting an error considering all messages.
The average error chance is \(P_e^{(n)} = \frac{1}{|\mathcal{M}|} \sum_{m\in \mathcal{M}} \lambda_m\).</p>

<p><u>Definition 2:</u> 
Rate \(R\) is <strong>achievable</strong> if there exists a sequence of encoder-decoder pairs \(\{f, \phi\}_n\) such that \(\lim_{n\rightarrow \infty} \lambda_\max = 0\).</p>

<p><u>Definition 3:</u> 
The capacity of a channel, denoted \(C\), is the <strong>supremum</strong> of all achievable rates.
Furthermore, we define \(C^{(I)} = \underset{Q\in P_\mathcal{X}}{\max} I(Q, W)\), where \(Q\) is a probability distribution for input symbols in \(\mathcal{X}\) and \(I(\cdot;\cdot)\) is the <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> between two distributions.</p>

<p><u>Shannon's noisy-channel coding theorem:</u> 
\(C = C^{(I)}\)</p>

<p>This theorem consists of two parts which we shall prove consequently:</p>

<ol>
  <li><strong>Converse part:</strong> If \(R &gt; C^{(I)}\), then \(R\) is not achievable.</li>
  <li><strong>Direct part:</strong> If \(R &lt; C^{(I)}\), then \(R\) is achievable.</li>
</ol>

<h3 id="21-convert-part">2.1 Convert part</h3>

<p>A contrapositive statement for the converse says that if you give me a wonderful encoder-decoder pair whose rate \(R\) is achievable (e.g. \(\lambda_\max \rightarrow 0\) as \(n\rightarrow \infty\)), then \(R \leq C^{(I)}\).
Next, I would like to introduce the Fano’s inequality which is necessary for the proof later:</p>

<p><u>Fano's inequality:</u></p>

\[\begin{align}
H(M|Y) &amp;\leq H(P_e) + P_e\log(|\mathcal{M}|-1)\\
&amp;\leq 1 + P_e\log|\mathcal{M}|
\end{align}\]

<p>Where \(P_e=P(\hat{M} \neq M)\) with \(\hat{M}=\phi(Y)\in\mathcal{M}\).
We have \(H(P_e)\leq 1\) because \(P_e\) is a Bernoulli distribution and the maximum entropy of a binary random variable is \(1\) bit.
The proof for this inequality is rather a trivial algebraic manipulation, for curious readers you can refer to <em>Elements of Information Theory by Thomas &amp; Cover</em>.</p>

<p><u>Proof of the converse part:</u></p>

\[\begin{align}
nR = \log |\mathcal{M}| = H(M) &amp;= I(M; Y^n) + H(M|Y^n)\\
&amp;\leq I(M; Y^n) + 1 + P_e^{(n)}nR &amp;\text{(Fano's inequality)}
\end{align}\]

<p>Dividing \(n\) both sides,</p>

\[R \leq \frac{1}{n} I(M; Y^n) + \frac{1}{n} + P_e^{(n)}R\]

<p>Given that \(R\) is achievable, we have \(P_e^{(n)}\rightarrow 0\). Also, \(\frac{1}{n}\rightarrow 0\) as \(n\rightarrow \infty\). The remain work is to bound \(\frac{1}{n} I(M; Y^n)\):</p>

\[\begin{align}
I(M; Y^n) &amp;\leq I(X^n; Y^n) &amp;\text{(Data Processing Ineq.)}\\
&amp;= H(Y^n) - H(Y^n|X^n) \\
&amp;= \sum_{i=1}^n H(Y_i|Y^{i-1}) - H(Y_i|Y^{i-1}, X^n) &amp;\text{(Chain rule)}\\
&amp;\leq \sum_{i=1}^n H(Y_i) - H(Y_i|X_i) &amp;\text{(Channel is memoryless)}\\
&amp;= \sum_{i=1}^n I(X_i; Y_i)\\
&amp;\leq n \underset{Q\in P_\mathcal{X}}{\max} I(Q; W) = n C^{(I)}
\end{align}\]

<p>Hence, \(\frac{1}{n} I(M; Y^n) \leq C^{(I)} \Rightarrow R \leq C^{(I)}\), completing the proof.</p>

<h3 id="22-direct-part">2.2 Direct part</h3>

<p>We would need several key ingredients for the actual proof of the direct part.</p>

<p><u>Joint Weak Typicality:</u>
Given a joint probability \(P_{XY}\) and a tolerance $\epsilon$, and two sequences \(\mathbf{x},\mathbf{y}\) that satisfy the following conditions:</p>

<ol>
  <li>\(\mathbf{x}\) is weakly typical, e.g. \(2^{-n(H(P_X)+\epsilon)} &lt; \prod_{i=0}^n P_X(x_i) &lt; 2^{-n(H(P_X)-\epsilon)}\).</li>
  <li>\(\mathbf{y}\) is weakly typical, e.g. \(2^{-n(H(P_Y)+\epsilon)} &lt; \prod_{i=0}^n P_Y(y_i) &lt; 2^{-n(H(P_Y)-\epsilon)}\).</li>
</ol>

<p>Then, the pair \((\mathbf{x, y})\) is weakly typical if \(2^{-n(H(P_{XY})+\epsilon)} &lt; \prod_{i=0}^n P_{XY}(x_i, y_i) &lt; 2^{-n(H(P_{XY})-\epsilon)}\).
We denotes such event as \((\mathbf{x,y}) \in \mathcal{A}_\epsilon^{(n)}(P_{XY})\).
A jointly typical pair of sequences has some properties that will be needed for the main proof:</p>

<ol>
  <li>If the elements of \((\mathbf{x,y})=\{(x_i,y_i)\}_{i=1}^n\) are drawn IID from \(P_{XY}\), then for any \(\delta&gt;0\) there exists \(N(\delta)\) so that \(\forall n&gt;N(\delta)\) we have \(\text{Pr}((\mathbf{x,y}) \notin \mathcal{A}_\epsilon^{(n)}(P_{XY})) \leq \delta\).</li>
  <li>If \(\mathbf{x}=\{x_i\}_{i=1}^n\) and \(\mathbf{y}=\{y_i\}_{i=1}^n\) are drawn IID from \(P_X\) and \(P_Y\) respectively, then the chance that the pair of sequences \((\mathbf{x,y})\) is jointly typical is very unlikely:</li>
</ol>

\[\textbf{Pr}((\mathbf{x,y}) \in \mathcal{A}_\epsilon^{(n)}(P_{XY})) \leq 2^{-n(I(X;Y)- 3\epsilon)}\]

<p><u>Proof of the direct part:</u></p>

<p>Let us fix the input symbols distribution \(Q \in P_X\).
We would like to prove that for any rate \(R &lt; I(Q, W)\), \(R\) is achievable.
If we can prove this previous statement, the same thing holds when \(Q\) is the capacity-achieving input distribution and hence the proof for \(R &lt; C^{(I)}\) follows naturally.</p>

<p>To decode the messages, we shall use something called jointly typical decoding.
For a given output sequence \(Y^n\) from the channel, we check for the existance of a message \(m\) whose code \(X^n(m)\) is jointly typical with \(Y^n\), e.g. whether \((X^n,Y^n) \in \mathcal{A}^{(n)} (P_{XY})\).
If there exists a single \(m\) satisfies this condition, return \(m\).
Otherwise, if there is none or multiple such \(m\)’s, we declare error.</p>

<p>Now, consider a random rate-\(R\) blocklength-\(n\) codebook \(\mathcal{C}\).
To construct this codebook, the row elements (there are \(n\) of them) in each of the \(2^{nR}\) rows in \(\mathcal{C}\) are drawn IID from \(Q\).
The probability of error for message \(m\) associating with this codebook is denoted by \(\lambda_m(\mathcal{C})\).
The expected error is then written as \(\bar{\lambda}_m = \mathbb{E}_{P_{\mathcal{C}}}[\lambda_m(\mathcal{C})]\).
Similarly, the average error for codebook \(\mathcal{C}\) is \(P_e^{(n)}(\mathcal{C})\) and the expected average error is \(P_e^{(n)}\).</p>

<p><u>Claim 1:</u> 
The expected error is independent of the message \(m\), e.g. \(\bar{\lambda}_1 = \dots = \bar{\lambda}_m\).</p>

<p><u>Proof of claim 1 (non-rigorous) :</u> This is due to symmetry!</p>

<p>Follow from claim 1, we can easily see that the expected average error \(P_e^{(n)}=\bar{\lambda}_1\).
From now on, we will focus on bounding \(\bar{\lambda}_1\) which turns out to be easy to work with!</p>

<p>Given that \(m=1\) and the fact that we are using jointly typical decoding, let us define \(\varepsilon_i = [(X^n(i), Y^n) \in \mathcal{A}_\epsilon^{(n)}]\), the event where the code for message \(i\) is jointly typical with \(Y^n\).
The error \(\bar{\lambda}_1\) can then be expressed as:</p>

\[\bar{\lambda}_1 = \text{Pr}(\varepsilon_1^c \cup \varepsilon_2 \cup \dots \cup \varepsilon_m)\]

<p>This is exactly equivalent what we have defined as an error for the jointly typical decoding.
We then have the following bounds,</p>

\[\begin{align}
\bar{\lambda}_1 &amp;\leq \text{Pr}(\varepsilon_1^c) + \sum_{i=2}^{2^{nR}} \text{Pr}(\varepsilon_i) &amp;\text{(Union bound)}\\
&amp;\leq \delta + \sum_{i=2}^{2^{nR}} 2^{-n(I(X;Y)-3\epsilon)} &amp;\text{(Properties of joint typicality)}\\
&amp;\leq \delta + 2^{-n(I(X;Y)-R-3\epsilon)}
\end{align}\]

<p>Now, if \(R &lt; I(X;Y) - 3\epsilon\), we can make \(\bar{\lambda}_1 \leq 2\delta\) for some large \(n\). Also note that \(\text{Pr}(\varepsilon_1^c) \leq \delta\) only hold for a large \(n\) too.</p>

<p>To get the actual proof, we replace \(Q\) with the capacity-achieving distribution \(\underset{Q \in P_X}{\text{argmax}}~I(Q,W)\).
The last conclusion above becomes \(R &lt; C^{(I)} - 3\epsilon\) implying \(P_e^{(n)} = \bar{\lambda}_1 \leq 2\delta\).
Next, we have a fundamental lemma about the average of numbers:</p>

<p><u>Lemma 1:</u> Consider a sequence of real numbers \(a_1,\dots, a_n\). 
If the average \(\bar{a} = \frac{1}{n} \sum_{i=1}^n a_i \leq \gamma\), then there exists an \(a_i \leq \gamma\).</p>

<p>This lemma can be proven easily by contradiction.
The implication of this is that previously we have shown that the average error \(P_e^{(n)} \leq 2\delta\) over all random codebook, then there must exist a codebook \(\mathcal{C}^*\) whose average error \(P_e^{(n)}(\mathcal{C}^*) \leq 2\delta\).
It remains to bound the maximal error \(\lambda_\max(\mathcal{C}^*) = \underset{m\in \mathcal{M}}{\max}\lambda_m(\mathcal{C}^*)\), which is actually we need to show for the achievability of the rate \(R\).</p>

<p><u>Lemma 2:</u> 
Consider a sequence of \(2n\) real numbers such that \(a_1\leq \dots \leq a_{2n}\) and its average \(\bar{a} \leq \nu\).
It holds that \(a_n \leq 2\nu\).</p>

<p><u>Proof:</u></p>

\[\begin{align}
2n\nu &amp;\geq \sum_{i=1}^{2n} a_i = \sum_{i=1}^{n} a_i + \sum_{i=n+1}^{2n} a_i\\
&amp;\geq \sum_{i=n+1}^{2n} a_i \\
&amp;\geq \sum_{i=n+1}^{2n} a_n = n a_n
\end{align}\]

<p>Using the lemma, we have a bound on the maximal error \(\lambda_\max(\mathcal{C}^*)\leq 4\delta\) by throwing away half of the codewords in \(\mathcal{C}^*\). 
However, by throwing away codewords we also lose the rate of communication, it is now \(R-\frac{1}{n}\) instead of \(R\).
Indeed, the \(\frac{1}{n}\) would vanish for a large \(n\).
Overall, this strategy of bounding the maximum is called the <em>expurgation trick</em>.</p>

<p>For a rate \(\tilde{R} &lt; C^{(I)}\) and any \(\tilde{\epsilon}&gt;0\), we can show that for \(n\) large enough there exists a rate-\(R\) (\(\tilde{R} &lt; R\)) blocklength-\(n\) codebook where \(\lambda_\max \leq \tilde{\epsilon}\) by setting \(R=\frac{(\tilde{R}+C)}{2}\), \(\delta=\frac{\tilde{\epsilon}}{4}\), \(\epsilon &lt; \frac{C^{(I)}-R}{3}\) and apply the proof above.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Here is my second attempt to understand the proof for the channel coding theorem. My initial exposure to this concept was through Professor Amos Lapidoth’s lecture in Information Theory I. His presentation, though concise, was packed with ingenious methods that were challenging to fully comprehend at time. To better understand it, I shall employ the Feynman technique - explaining the concept to someone else.]]></summary></entry></feed>