<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-04T10:29:17+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">EntropyMaxxing</title><author><name>Khánh Vũ</name></author><entry><title type="html">Jane Street Puzzle - August 2024</title><link href="http://localhost:4000/jane_street_aug24/" rel="alternate" type="text/html" title="Jane Street Puzzle - August 2024" /><published>2024-09-03T00:00:00+02:00</published><updated>2024-09-03T00:00:00+02:00</updated><id>http://localhost:4000/jane_street_aug24</id><content type="html" xml:base="http://localhost:4000/jane_street_aug24/"><![CDATA[<!-- Personal remarks -->
<p>I decided to slack off my exam studying with Jane Street’s puzzle.
I first read the problem in early August, forgot about it, and then submitted my answer on August 16—and got it right on the first try! (username: Khanh Vu).
This month’s puzzle involves straightforward state analysis with a nice probability garnish.</p>

<h2 id="the-problem">The problem</h2>

<p>Aaron and Beren play a game on an infinite complete binary tree where each edge is independently labeled as \(\mathrm{A}\) with probability \(p\) and \(\mathrm{B}\) otherwise.
The game starts with Aaron at the root, and the players take turns moving a shared token down the tree.
On each turn, the active player chooses one of the two child nodes to move the token to.
If the token traverses an edge labeled B, Beren wins; otherwise, Aaron wins.</p>

<p>The goal is to find the <a href="https://en.wikipedia.org/wiki/Infimum_and_supremum">infimum</a> of all probabilities \(p\) for which Aaron has a nonzero probability of winning. 
<!-- This infimum represents the smallest probability $$p$$ at which Aaron’s chance of winning is not zero, considering that Beren can choose a path that includes at least one edge labeled B. --></p>

<p>You can read the full problem statement here at: <a href="https://www.janestreet.com/puzzles/tree-edge-triage-index/">www.janestreet.com/puzzles/tree-edge-triage-index/</a>.</p>

<h2 id="my-approach">My approach</h2>

<p>What does it mean for Aaron to win?
The universe that Aaron and Beren reside in is controlled by a demon, I assume.
This demon loves upside down binary trees, so it generates an infinite number of binary trees out of boredom.
Among these trees, if there exists a tree that allows Aaron to keep playing indefinitely, then we say that he wins with non-zero probability.</p>

<p>The demon grows a tree in the following fashion:
A tree starts with a root, and at each time step, every leaf node grows two child nodes.
The edges connecting each parent to its children are labeled \(\mathrm{A}\) with probability \(p\), and \(\mathrm{B}\) otherwise.
As \(p\) gets closer to \(1\), it is more likely for Aaron to win.
So it makes sense that the problem asks for some sort of a lower bound for such \(p\).</p>

<center>
    <figure>
        <img src="../assets/images/jane_street/js00.png" alt="cases" style="display: inline-block;" />
        <figcaption> <em>Figure 1. When it's Aaron turn, there are four scenarios.</em> </figcaption>
    </figure>
</center>

<p>We can zoom in and examine the local picture.
Assuming that it’s Aaron turn and the token is at a node \(u\), the only state that makes him lose immediately is when the two edges are labeled \((\mathrm{B}, \mathrm{B})\) (state \(4\) in Figure \(1\)).
Otherwise, he moves the token down to the child node \(v_1\) or \(v_2\), depending on the prospect of the two options (or it is possible at all to move the token to \(v_i\) if the edge is labeled \(\mathrm{B}\)).</p>

<center>
    <figure>
        <img src="../assets/images/jane_street/js01.png" alt="cases" style="display: inline-block;" />
        <figcaption> <em>Figure 2. When it's Beren turn, only the first state allows the game to continue.</em> </figcaption>
    </figure>
</center>

<p>When the token is in Beren’s control, Aaron’s only chance of winning is if both edges are labeled \((\mathrm{A}, \mathrm{A})\).
In any other case, Beren wins by moving the token along the edge labeled \(\mathrm{B}\).</p>

<p>Formally, let \(w_u\) be the event that Aaron wins if he starts his turn at node \(u\).
Now, node \(u\) has children \(v_1\) and \(v_2\).
Let \(w_{v_1}\) be the event that Aaron wins if it’s Beren’s turn and the token is at node \(v_1\), and similarly, let \(w_{v_2}\) represent the event that Aaron wins if it’s Beren’s turn and the token is at node \(v_2\).
Then, we have the following recursive relation:</p>

\[\begin{align}
    \mathbb{P}(w_u) &amp;= p^2 \cdot \mathbb{P}(w_{v_1} \lor w_{v_2}) + p(1-p)\cdot\mathbb{P}(w_{v_1}) + (1-p)p\cdot\mathbb{P}(w_{v_2}) &amp;\text{(1)} \\
    \mathbb{P}(w_v) &amp;= p^2 \cdot \mathbb{P}(w_{u_1} \land w_{u_2}) = p^2 \cdot \mathbb{P}(w_u)^2 &amp;\text{(2)} 
\end{align}\]

<p>Eq. (2) used the fact that \(w_{u_1}\) and \(w_{u_2}\) are independent events.</p>

<p>To break things down for clarity, Eq. (1) and (2) are composed of terms capturing the states in Fig. 1 and 2 respectively.
For instance, the term \(p^2 \cdot \mathbb{P}(w_{v_1} \lor w_{v_2})\) in Eq. (1) accounts for the state where both edges connecting \(u\) to children \(v_1, v_2\) are labeled \((\mathrm{A}, \mathrm{A})\).
In this case, Aaron wins (e.g. \(w_u\) holds) if either \(w_{v_1}\) or \(w_{v_2}\) holds.</p>

<p>Now, from the sum rule of probability, we know that:</p>

\[\newcommand{\ind}{\perp\!\!\!\!\perp} 
\begin{align}
    \mathbb{P}(w_{v_1} \lor w_{v_2}) &amp;= \mathbb{P}(w_{v_1}) + \mathbb{P}(w_{v_2}) - \mathbb{P}(w_{v_1} \land w_{v_2}) \\
    &amp;= 2\mathbb{P}(w_v) - \mathbb{P}(w_v)^2 &amp;(\text{since $w_{v_1} \ind w_{v_2}$})
\end{align}\]

<p>For brevity, let \(x:= \mathbb{P}(w_u)\).
Combining the fact above and plugging (2) into (1), we arrive at the following identity:</p>

\[\begin{align}
    x &amp;= 2x^2p^3 - x^4p^6
\end{align}\]

<p>Let the left-hand side be defined as \(f(x) := x\) and the right-hand side as \(g(x) := 2x^2p^3 - x^4p^6\). 
Graphically, the problem is now to find a value \(p_0 \in (0, 1)\) such that for all \(p &gt; p_0\), the function \(f(x)\) intersects with \(g(x)\) at some \(x:=\mathbb{P}(w_u) &gt; 0\).
<a href="https://www.wolframalpha.com/input?i=x+%3D+2x%5E2p%5E3+-+x%5E4p%5E3+%3E+0">Solving for \(p_0\)</a>, we get \(\boxed{\frac{\sqrt{3}}{2^{5/6}}}\approx 0.972\).</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[I decided to slack off my exam studying with Jane Street’s puzzle. I first read the problem in early August, forgot about it, and then submitted my answer on August 16—and got it right on the first try! (username: Khanh Vu). This month’s puzzle involves straightforward state analysis with a nice probability garnish.]]></summary></entry><entry><title type="html">Counting with expectation</title><link href="http://localhost:4000/expectation_counting/" rel="alternate" type="text/html" title="Counting with expectation" /><published>2024-08-08T00:00:00+02:00</published><updated>2024-08-08T00:00:00+02:00</updated><id>http://localhost:4000/expectation_counting</id><content type="html" xml:base="http://localhost:4000/expectation_counting/"><![CDATA[<p><strong>Problem 10 (Individual test, ARML 2013)</strong>.
For a positive integer \(n\), let \(C(n)\) equal the number of pairs of consecutive \(1\)’s in the binary representation of \(n\).
For example, \(C(183)=C(10110111_2)=3\).
Compute \(C(1) + C(2) + \ldots + C(256)\).</p>

<p><em>Solution</em>.
You can go ahead with doing casework and will eventually discover some recurrent formulas like in the <a href="https://www.arml.com/ARML/arml_2019/public_contest_files/2009_2014_book/ARML_2009_2014.pdf">intended solution</a>.</p>

<p>However, this problem has a shorter solution that leverages <a href="https://en.wikipedia.org/wiki/Expected_value#Properties:~:text=0.-,Linearity%20of%20expectation,-%3A%5B34">linearity of expectation</a>.
In short, linearity of expectation means:</p>

\[\mathbb{E}[\mathrm{X}_1 + \mathrm{X}_2 + \ldots + \mathrm{X}_n] = \mathbb{E}[\mathrm{X}_1] + \mathbb{E}[\mathrm{X}_2] + \ldots + \mathbb{E}[\mathrm{X}_n]\]

<p>This holds regardless of whether these random variables are dependent or not.
Now, we can rewrite the target quantity in term of an expectation:</p>

\[\sum_{i=1}^{256} C(i) = 256 \cdot \mathbb{E}[C(\mathrm{X})],\]

<p>for \(\mathrm{X}\) being a discrete RV drawn uniformly from \(\{1, 2, \ldots, 256\}\).
Let \(\text{bin}(\mathrm{X})\) denotes the binary representation of \(\mathrm{X}\) and \(\text{bin}(\mathrm{X})_i\) denotes the \(i\)-th bit from the left.
Let \(Z_i = [\text{bin}(\mathrm{X})_i = 1 \land \text{bin}(\mathrm{X})_{i+1} = 1]\) be an indicator function that takes value 1 if both \(i\)-th and \((i+1)\)-th bits are \(1\)’s, and otherwise takes value 0.
Given this, we can write</p>

\[\begin{align}
\mathbb{E}[C(\mathrm{X})] &amp;= \mathbb{E}\left[\sum_{i=1}^7 Z_i\right] \\
&amp;= \sum_{i=1}^7 \mathbb{E}[Z_i] &amp;\text{(Linearity of expectation)}\\ 
&amp;= \sum_{i=1}^7 \mathbb{P}\left(\text{bin}(\mathrm{X})_i = 1 \land \text{bin}(\mathrm{X})_{i+1} = 1\right)\\
&amp;= 7\cdot \frac{1}{2^2} = \frac{7}{4}
\end{align}\]

<p>The answer would then be \(256 \cdot \frac{7}{4} =\boxed{448}\).
Cute trick isn’t it?</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Problem 10 (Individual test, ARML 2013). For a positive integer \(n\), let \(C(n)\) equal the number of pairs of consecutive \(1\)’s in the binary representation of \(n\). For example, \(C(183)=C(10110111_2)=3\). Compute \(C(1) + C(2) + \ldots + C(256)\).]]></summary></entry><entry><title type="html">An upper bound for the Kullback-Leibler divergence</title><link href="http://localhost:4000/kl_upper_bound/" rel="alternate" type="text/html" title="An upper bound for the Kullback-Leibler divergence" /><published>2024-07-23T00:00:00+02:00</published><updated>2024-07-23T00:00:00+02:00</updated><id>http://localhost:4000/kl_upper_bound</id><content type="html" xml:base="http://localhost:4000/kl_upper_bound/"><![CDATA[<p>Zürich is so hot now in the night that I couldn’t fall asleep.
While lying on the bed, I arrived at an interesting upper bound for the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence (relative entropy)</a> between two Bernoulli distributions.
In this case, the KL divergence is expressed as:</p>

\[D_\text{KL}(p \parallel q) := p\log\left(\frac{p}{q}\right) + (1-p)\log\left(\frac{1-p}{1-q}\right)\]

<p>Here, \(p, q\) denote the parameters of the two distributions.
I assume \(p, q \in (0, 1)\) and \(\log(\cdot)\) is the natural logarithm function.
For other cases like when \(p&gt;0\) and \(q=0\), the KL divergence diverges to infinity.
I can show that, in this setting, the following holds:</p>

\[D_\text{KL}(p \parallel q) \le \frac{(p - q)^2}{(1-q)q} =: \frac{2D_\text{TV}(p, q)^2}{\mathbb{V}[X]}\]

<p>Above, \(D_\text{TV}(\cdot, \cdot)\) is the <a href="https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures">variational distance</a> between two distributions and \(X\) is a random variable distributed according to Bernoulli(\(q\)).
My result is in fact a looser bound as compared to the <a href="https://en.wikipedia.org/wiki/Pinsker%27s_inequality">reversed Pinsker’s inequality</a>.</p>

<p>For the proof, I define a function \(f(q):=D_\text{KL}(p \parallel q)\) with \(p\) kept as a constant.</p>

<p><strong>Corollary 1.</strong>
The function \(f\) is convex on \((0, 1)\).</p>

<p><em>Proof</em>.
\(f\) is convex if and only if \(\forall \lambda \in [0, 1]\) and \(\forall q_1, q_2 \in \text{dom}(f)\):</p>

\[f(\lambda q_1 + (1-\lambda) q_2) \le \lambda f(q_1) + (1-\lambda) f(q_2)\]

<p>Indeed this is true since:</p>

<p>\(\begin{align}
    f(\lambda q_1 + (1-\lambda) q_2) &amp;= p \log\left(\frac{p}{\lambda q_1 + (1-\lambda) q_2}\right) + (1-p) \log\left(\frac{1-p}{1 - \lambda q_1 - (1-\lambda) q_2}\right) \\
    &amp;= -p \log\left(\frac{\lambda q_1 + (1-\lambda) q_2}{p}\right) - (1-p) \log\left(\frac{\lambda (1-q_1) + (1-\lambda) (1-q_2)}{1-p}\right) \\ 
    &amp;\le \lambda \left(p\log\frac{p}{q_1} + (1-p)\log\frac{1-p}{1-q_1} \right) + (1-\lambda) \left(p\log\frac{p}{q_2} + (1-p)\log\frac{1-p}{1-q_2} \right) \\
    &amp;= \lambda f(q_1) + (1-\lambda) f(q_2)
\end{align}\)
The inequality above is due to the convexity of \(-\log(\cdot)\) and Jensen’s inequality.</p>

<p><strong>Corallary 2.</strong>
\(\forall q_1, q_2 \in \text{dom}(f)\), it holds that:</p>

\[f(q_1) \ge f(q_2) + f'(q_2)(q_1 - q_2)\]

<p>where \(f'\) is the first-order derivative of \(f\) at \(q_2\).</p>

<p><em>Proof</em>.
Given that \(f\) is convex on its domain, we have:</p>

\[\begin{align}
    f(\lambda q_1 + (1-\lambda) q_2) &amp;\le \lambda f(q_1) + (1-\lambda) f(q_2) \\
    f(q_2 + \lambda(q_1 - q_2)) - f(q_2) + \lambda f(q_2) &amp;\le \lambda f(q_1) \\
    \frac{f(q_2 + \lambda(q_1 - q_2)) - f(q_2)}{\lambda (q_1 - q_2)} (q_1 - q_2) + f(q_2) &amp;\le f(q_1) &amp;\text{(Dividing $\lambda$ both sides)} \\
    \lim_{\lambda \rightarrow 0} \frac{f(q_2 + \lambda(q_1 - q_2)) - f(q_2)}{\lambda (q_1 - q_2)} (q_1 - q_2) + f(q_2) &amp;\le f(q_1) \\
    f'(q_2) (q_1 - q_2) + f(q_2) &amp;\le f(q_1)
\end{align}\]

<p>We can explicitly derive the derivative of \(f\) at a given \(q\):</p>

\[f'(q) = \frac{p - q}{(q - 1)q}\]

<p>Putting everything together, we have the desired upper bound:</p>

\[\begin{align}
    f(p) &amp;\ge f(q) + f'(q)(p - q) &amp;\text{(Corollary 2)}\\
    f'(q)(p - q) &amp;\ge f(q) &amp;\text{($f(p)=0$ by definition)}\\
    \frac{(q - p)^2}{(1 - q)q} &amp;\ge f(q):= D_\text{KL}(p \parallel q)
\end{align}\]

<p>This bound is particularly useful when it comes to deriving some concentration bounds for the KL divergence with respect to \(q\).
I will discuss this matter in another blog post.
It’s 1:44AM now and I need to go to bed for real this time.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Zürich is so hot now in the night that I couldn’t fall asleep. While lying on the bed, I arrived at an interesting upper bound for the KL divergence (relative entropy) between two Bernoulli distributions. In this case, the KL divergence is expressed as:]]></summary></entry><entry><title type="html">Weekly Math Puzzle - Week 24/2024</title><link href="http://localhost:4000/weekly_math_2424/" rel="alternate" type="text/html" title="Weekly Math Puzzle - Week 24/2024" /><published>2024-06-12T00:00:00+02:00</published><updated>2024-06-12T00:00:00+02:00</updated><id>http://localhost:4000/weekly_math_2424</id><content type="html" xml:base="http://localhost:4000/weekly_math_2424/"><![CDATA[<p>Putnam’s problems were tought so we decided to go back to the basics this week.
Here are a couple of problems from <em>Problem-solving strategies</em> by Arthur Engel.
We think it might be a good idea to stick to a textbook for a while.</p>

<h2 id="problem-1-ch-1-problem-31">Problem 1 (Ch. 1, Problem 31)</h2>

<p>Let \(a_1, a_2, \ldots, a_n\) be a permutation of the numbers \(1, 2, \ldots, n\).
If \(n\) is odd, then the product \(P=(a_1-1)(a_2-2)\ldots(a_n-n)\) is even.
Prove this.</p>

<p><em>My approach</em>.
Assume that the product \(P\) is odd.
Then, all the factors must be odd, meaning that \(a_i-i\) must be odd for all \(i=1,2,\ldots,n\).
This is the same as saying that the parity of \(a_i\) is different from the parity of \(i\), e.g. when \(i\) is odd, \(a_i\) must be even and vice versa.
However, since \(n\) is odd, we have more odd numbers than even numbers in the set \(\{1,2,\ldots,n\}\).
So it is not possible to match all the odd \(a_i\) with the even \(i\).
Hence, the product \(P\) must be even.</p>

<h2 id="problem-2-ch-1-problem-8">Problem 2 (Ch. 1, Problem 8)</h2>

<p>There is a positive integer in each square of a rectangular table.
In each move, you may double each number in a row or subtract 1 from each number of a column.
Prove that you can reach a table of zeros by a sequence of these permitted moves.</p>

<p><em>My approach</em>. 
Notice that if you can solve the case of a \(n\times 1\) table, you can solve the case of a \(n\times m\) table by solving the \(m\) columns one by one.</p>

<p>So, let’s focus on the case of a \(n\times 1\) table.
Let’s denote the numbers in the table as \(a_1, a_2, \ldots, a_n\).
Without the loss of generality, we can assume that \(a_1 \leq a_2 \leq \ldots \leq a_n\).
By subtracting 1 from this table \(a_1-1\) times, we get \(a_1=1\) and \(a_i = a_i - a_1 + 1\) for all \(i=2,3,\ldots,n\).
If there exists an index \(j\) such that \(a_j &gt; 1\) and \(a_{i&lt;j}=1\), we can double the rows \(i=1,2,\ldots,j-1\) as long as the value of \(a_i\) does not exceed \(a_j\). 
Then, we can repeatedly substract 1 until the smallest number in the table becomes 1 again.
By repeating this process, we can reach a table of ones.
And by substracting 1 from the table of ones, we can reach a table of zeros.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Putnam’s problems were tought so we decided to go back to the basics this week. Here are a couple of problems from Problem-solving strategies by Arthur Engel. We think it might be a good idea to stick to a textbook for a while.]]></summary></entry><entry><title type="html">Weekly Math Puzzle - Week 22/2024</title><link href="http://localhost:4000/weekly_math_2224/" rel="alternate" type="text/html" title="Weekly Math Puzzle - Week 22/2024" /><published>2024-06-03T00:00:00+02:00</published><updated>2024-06-03T00:00:00+02:00</updated><id>http://localhost:4000/weekly_math_2224</id><content type="html" xml:base="http://localhost:4000/weekly_math_2224/"><![CDATA[<p>This week we decided to level up the difficulty by attempting two problems from the Putnam competition.</p>

<h2 id="problem-1-2018-putnam-a5">Problem 1 (2018 Putnam A5)</h2>

<p>Let \(f:\mathbb{R}\rightarrow\mathbb{R}\) be an indefinitely differentiable function satisfying \(f(0)=0\), \(f(1)=1\), and \(f(x)\geq 0\) for all \(x\in\mathbb{R}\).
Show that there exists a positive integer $n$ and a real number \(x\) such that \(f^{(n)}(x)&lt;0\).</p>

<h3 id="my-approach">My approach</h3>

<p>If there exists \(x&lt;y\in\mathbb{R}\) such that \(f(x)&gt;f(y)\), then by the mean value theorem, there exists a point \(c\in(x,y)\) such that \(f'(c)=\frac{f(x)-f(y)}{x-y}&lt;0\).
So the only interesting case is when \(f\) is non-decreasing on \(\mathbb{R}\), i.e. \(f'(x)\geq 0\) for all \(x\in\mathbb{R}\).
In this case, \(f(x)=f'(x)=0\) for all \(x\in(-\infty,0]\).</p>

<p>From the mean value theorem, there exists some \(c_1\in(0,1)\) such that</p>

\[f'(c_1)=\frac{f(1)-f(0)}{1-0}=1\]

<p>As before, we can assume that \(f'(x)\) is non-decreasing on \(\mathbb{R}\), and that \(\forall x\in(-\infty,0], f'(x)=0\), \(f'(c_1)=1\).
Applying the same argument for higher-order derivatives, we can construct a sequence \(1&gt;c_1&gt;c_2&gt;\ldots&gt;c_n&gt;0\) such that \(f^{(n)}(c_n)=1\).</p>

<p>From this, we can make \(c_n\) abitrarily close to 0 as \(n\rightarrow\infty\), implying that \(f^{(n)}\) is discontinuous at \(x=0\), e.g. \(f^{(n)}\) becomes a step function at 0.
This contradicts the assumption that \(f\) is indefinitely differentiable (derivatives of all orders must be continuous), so it is not true that \(f^{(n)}(x)\geq 0\) for all \(n\in\mathbb{N}\) and \(x\in\mathbb{R}\).</p>

<p><strong>UDP</strong>: My solution is not correct. We only know that the sequence \((c_n)\) is convergent due to <a href="https://en.wikipedia.org/wiki/Bolzano%E2%80%93Weierstrass_theorem">Bolzano–Weierstrass theorem</a>, but we cannot guarantee that it converges to 0.</p>

<h2 id="problem-2-2008-putnam-a2">Problem 2 (2008 Putnam A2)</h2>

<p>Alan and Barbara play a game in which they take turns filling entries of an initially empty \(2008 \times 2008\) array.
Alan plays first.
At each turn, a player chooses a real number and places it in a vacant entry.
The game ends when all the entries are filled.
Alan wins if the determinant of the resulting matrix is nonzero;
Barbara wins if its is zero. Which player has a winning strategy?</p>

<h3 id="my-approach-1">My approach</h3>

<p>Please check my friend (Felix)’s solution <a href="https://www.felixgbreuer.com/weekly_math">here</a>.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[This week we decided to level up the difficulty by attempting two problems from the Putnam competition.]]></summary></entry><entry><title type="html">Weekly Math Puzzle - Week 21/2024</title><link href="http://localhost:4000/weekly_math_2124/" rel="alternate" type="text/html" title="Weekly Math Puzzle - Week 21/2024" /><published>2024-05-24T00:00:00+02:00</published><updated>2024-05-24T00:00:00+02:00</updated><id>http://localhost:4000/weekly_math_2124</id><content type="html" xml:base="http://localhost:4000/weekly_math_2124/"><![CDATA[<p>Me and a friend at ETHZ started to solve some math problems for fun, and we plan to do this weekly.
To keep track of my progress, I will try to post the solutions to the problems (if I manage to solve them) on my website.
Also, please spare me if the solutions are not the most elegant ones, I am still learning :)</p>

<p>This week we started with a problem from the AIME (American Invitational Mathematics Examination) in the year 2000.</p>

<h2 id="aime-i-2000-problem-12">AIME I 2000 Problem 12</h2>

<p><a href="https://artofproblemsolving.com/wiki/index.php/2000_AIME_I_Problems">Link to the problem</a></p>

<p>Given a function \(f\) for which</p>

\[f(x) = f(398 - x) = f(2158 - x) = f(3214 - x)\]

<p>holds for all real $x,$ what is the largest number of different values that can appear in the list \(f(0),f(1),f(2),\ldots,f(999)\)?</p>

<h3 id="my-solution">My solution</h3>

<p>Since the property holds for all real $x$, we can start to plug in around the given values to see if we can find a pattern:</p>

\[\begin{align}
f(x) &amp;= f(398 - x) = f(2158 - x) = f(3214 - x)\\
&amp;= f(398 - (2158 - x)) = f(x - 1760) \tag{1}\\
&amp;= f(398 - (3214 - x)) = f(x - 2816) \tag{2}\\
&amp;= f(2158 - (3214 - x)) = f(x - 1056) \tag{3}\\
&amp;= f(2158 - (398 - x)) = f(x + 1760) \tag{4}\\
&amp;= f(3214 - (398 - x)) = f(x + 2816) \tag{5}\\
&amp;= f(3214 - (2158 - x)) = f(x + 1056) \tag{6}
\end{align}\]

<p>From \((6)\), we can see that the function is periodic with a period of 1056.
But we can do better than that! Consider plugging \((3)\) into \((4)\), we get:</p>

\[f(x) = f(x - 1056 + 1760) = f(x + 704)\]

<p>This means that the function is also periodic with a period of 704 and so on if we plug in the other equations.
Starting with a tuple \((1056, 1760, 2816)\), we can generate all the periods of the function by subtracting the smaller number from the larger one.
For instance, we have the following chain of periods:</p>

\[(1056, 1760, 2816) \rightarrow (704, 1056, 1760) \rightarrow (352, 704, 1056) \rightarrow (352, 352, 704) \rightarrow (0, 352, 352) \rightarrow (0, 0, 352)\]

<p>So it holds that \(f(x) = f(x + 352)\).
This means that the function is periodic with a period of 352, and we can not get any further with this construction.
The answer must be at most 352 for now.</p>

<p>For \(x\in[0,398]\), the function \(f(x)\) is symmetric since \(f(x) = f(398 - x)\).
Furthermore, the function is also symmetric for \(x\in[0, 46]\) since:</p>

\[\begin{align}
f(x) = f(398 - (x + 352)) = f(46 - x)
\end{align}\]

<p>Hence, it can only be for \(x\in[\frac{46}{2}, \frac{398}{2}]\) that the function \(f(x)\) can take on different values.
So the answer is \(199-23+1=\boxed{177}\).</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Me and a friend at ETHZ started to solve some math problems for fun, and we plan to do this weekly. To keep track of my progress, I will try to post the solutions to the problems (if I manage to solve them) on my website. Also, please spare me if the solutions are not the most elegant ones, I am still learning :)]]></summary></entry><entry><title type="html">Mathematical optimization cheatsheet</title><link href="http://localhost:4000/opt_cheatsheet/" rel="alternate" type="text/html" title="Mathematical optimization cheatsheet" /><published>2024-04-26T00:00:00+02:00</published><updated>2024-04-26T00:00:00+02:00</updated><id>http://localhost:4000/opt_cheatsheet</id><content type="html" xml:base="http://localhost:4000/opt_cheatsheet/"><![CDATA[<p>In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms.</p>

<p>In the first part, I will recite the definitions of convexity, smoothness, and strong convexity, which are fundamental properties in the realm of convex optimization on differentiable functions.
In the second part, we will relax to the case of non-differentiable functions and discuss subgradients, proximal operators and other concepts that are essential in the analysis of non-smooth optimization algorithms.</p>

<h1 id="1-convex-optimization">1. Convex optimization</h1>

<p>Let us define a function $f: \text{dom}(f)\rightarrow \mathbb{R}$ with $\text{dom}(f) \subseteq \mathbb{R}^d$.
We assume that $f$ is differentiable, meaning that the gradient $\nabla f(x) \in \mathbb{R}^d$ exists $\forall x \in \text{dom}(f)$.</p>

<center>
    <figure>
        <img src="../assets/images/smooth_strong_convex.png" alt="smoothness and strong convexity illustration" style="display: inline-block;" />
        <figcaption> Figure 1. Smoothness and strong convexity play complementary roles in bounding the function values. </figcaption>
    </figure>
</center>

<h2 id="11-convexity">1.1 Convexity</h2>

<p><strong>(a) Definition.</strong>
A function $f$ is convex if its domain is a convex set and if for all $x, y \in \text{dom}(f)$ and $\theta \in [0, 1]$, we have</p>

\[f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)\]

<p>This definition is also known as the Jensen’s inequality.
Oftentimes, it is more convenient to work with one of the following equivalent definitions:</p>

<p><strong>(b) (First-order characterization)</strong>
\(f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle\)</p>

<p>Geometrically, this definition implies that the function lies above its tangent line at any point (see Figure 1).
Or, equivalently, the vector \((\nabla f(x), -1) \in \mathbb{R}^{d+1}\) defines a supporting hyperplane to the epigraph of \(f\) at \((x, f(x))\).</p>

<p><strong>(c) (Second-order characterization)</strong>
Suppose that $f$ is twice differentiable.
A function $f$ is convex if its domain is a convex set and if for all $x \in \text{dom}(f)$, we have</p>

\[\nabla^2 f(x) \succeq 0\]

<p>where $\nabla^2 f(x)$ is the Hessian matrix of $f$ at $x$.</p>

<p><strong>(d) (Monotonicity of the gradient)</strong>
A function $f$ is convex if its domain is a convex set and if for all $x, y \in \text{dom}(f)$, we have</p>

\[\langle \nabla f(y) - \nabla f(x), y-x \rangle \geq 0\]

<h2 id="12-smooth-functions">1.2 Smooth functions</h2>

<p><strong>(a) Definition.</strong>
Let $X\subseteq \text{dom}(f)$ be a convex set and let $L &gt; 0$. A function $f$ is $L$-smooth on $X$ if for all $x, y \in X$, we have</p>

\[f(y) \leq f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2}\|y-x\|^2\]

<p>One can think of this definition as Lipschitz continuity of the gradient map.
When $f$ is twice differentiable, smoothness implies that the largest eigenvalue of the Hessian is bounded by $L$, e.g. \(\|\nabla^2 f(x)\| \leq L\).
If $f$ is $L$-smooth, then $f$ is also $L’$-smooth for all $L’ \geq L$.
Alternatively, we can use the following equivalent definition:</p>

<p><strong>(b) (Lipschitz continuity of the gradient)</strong>
\(\| \nabla f(x) - \nabla f(y)\| \leq L\| x-y\|\) for all \(x, y \in X\).</p>

<p><strong>(c)</strong>
\(g(x) = \frac{L}{2} x^\top x - f(x)\) is convex.</p>

<p><strong>(d)</strong>
\(\langle \nabla f(x) - \nabla f(y), x-y \rangle \leq L\|x-y\|^2\) for all \(x, y \in X\).</p>

<p>These definitions are not actually equivalent, but they are related.
We have the following relations:</p>

\[[b] \Rightarrow [a] \Leftrightarrow [c] \Leftrightarrow [d]\]

<p>If $f$ is additionally convex, then \([a] \Rightarrow [b]\).</p>

<h2 id="13-strong-convexity">1.3 Strong convexity</h2>

<p><strong>(a) Definition.</strong>
Let $X\subseteq \text{dom}(f)$ be a convex set and let $\mu &gt; 0$. A function $f$ is $\mu$-strongly convex on $X$ if for all $x, y \in X$, we have</p>

\[f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle + \frac{\mu}{2}\|y-x\|^2\]

<p>The notion of strong convexity complements the notion of smoothness.
Strong convexity implies that the function grows at least quadratically.
Moreover, $f$ is strongly convex implies that $f$ is strictly convex and has a unique minimizer.
And similarly to smoothness, if $f$ is $\mu$-strongly convex, then $f$ is also $\mu’$-strongly convex for all $\mu’ \leq \mu$.
Alternatively, we can use the following equivalent definition:</p>

<p><strong>(b)</strong>
\(\| \nabla f(x) - \nabla f(y)\| \geq \mu\| x-y\|\) for all \(x, y \in X\).</p>

<p><strong>(c)</strong>
\(g(x) = f(x) - \frac{\mu}{2} x^\top x\) is convex.</p>

<p><strong>(d)</strong>
\(\langle \nabla f(x) - \nabla f(y), x-y \rangle \geq \mu\|x-y\|^2\) for all \(x, y \in X\).</p>

<p>The relations are as follows:</p>

\[[b] \Leftarrow [a] \Leftrightarrow [c] \Leftrightarrow [d]\]

<p>Figure 1 above illustrates the complementary roles of smoothness and strong convexity in bounding the function values.</p>

<h1 id="2-non-smooth-optimization">2. Non-smooth optimization</h1>

<p>In this section, we define the function \(f\) similar to before, but we make no assumption of differentiability.</p>

<h2 id="21-non-smooth-functions">2.1 Non-smooth functions</h2>

<p><strong>(a) Definition.</strong> 
If \(f\) is not differentiable, or if the gradients of \(f\) exists but are not Lipschitz continuous, then we say that \(f\) is non-smooth.
This definition is exactly the negation of the smoothness property.
Examples of non-smooth functions include the absolute value function \(f(x) = |x|\), the hinge loss function \(f(x) = \max(0, 1-x)\), or many activation functions in neural networks such as the ReLU function \(f(x) = \max(0, x)\).</p>

<p><strong>(b) Subgradients.</strong>
A vector \(g \in \mathbb{R}^d\) is a subgradient of \(f\) at \(x\) if for all \(y \in \text{dom}(f)\), we have</p>

\[f(y) \geq f(x) + \langle g, y-x \rangle\]

<p>We define a subdifferential \(\partial f(x)\) denoting the set of all subgradients of \(f\) at \(x\).
Geometrically, \(g\) is a subgradient of \(f\) at \(x\) if the hyperplane defined by \((g, -1)\in\mathbb{R}^{d+1}\) supports the epigraph of \(f\) at \((x, f(x))\).</p>

<p><strong>(c) Properties of subgradients.</strong></p>

<ul>
  <li>If \(f\) is differentiable at \(x\in \text{dom}(f)\), then \(\partial f(x) \subseteq \{\nabla f(x)\}\), meaning that the subdifferential either contains the gradient or is empty.</li>
  <li>If \(f\) is convex, then \(\partial f(x)\) is non-empty for all \(x\in \text{int(dom)}(f)\).</li>
  <li>If \(\text{dom}(f)\) is convex and \(\partial f(x) \neq \emptyset\) for all \(x\in \text{dom}(f)\), then \(f\) is convex.</li>
  <li><strong>(Subgradient optimality condition)</strong> If \(\mathbf{0} \in \partial f(x)\), then \(x\) is a global minimum of \(f\).</li>
</ul>

<h2 id="22-dual-norms">2.2 Dual norms</h2>

<p><strong>(a) Definition.</strong>
Given a general norm \(\|\cdot\|\), the dual norm \(\|\cdot\|_*\) is defined as</p>

\[\|z\|_* = \max_{\|x\|\leq 1} \langle z, x \rangle\]

<p><strong>(b) Examples.</strong>
In general, \(\|\cdot\|_p\) is the dual norm of \(\|\cdot\|_q\) if \(\frac{1}{p} + \frac{1}{q} = 1\).</p>

<ul>
  <li>The dual norm of the \(\ell_1\) norm is the \(\ell_\infty\) norm.</li>
  <li>The dual norm of the \(\ell_2\) norm is the \(\ell_2\) norm.</li>
</ul>

<p><strong>(c) Interpretation of dual norms.</strong>
(WIP)</p>

<h2 id="23-proximal-operators">2.3 Proximal operators</h2>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms.]]></summary></entry><entry><title type="html">Rate-distortion theory</title><link href="http://localhost:4000/rate_distortion/" rel="alternate" type="text/html" title="Rate-distortion theory" /><published>2023-12-19T00:00:00+01:00</published><updated>2023-12-19T00:00:00+01:00</updated><id>http://localhost:4000/rate_distortion</id><content type="html" xml:base="http://localhost:4000/rate_distortion/"><![CDATA[<p>In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem.
The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression.
I will try to keep the material short and concise.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li><a href="#1-definitions">Definitions</a></li>
  <li><a href="#2-the-rate-distortion-theorem">Rate-distortion theorem</a>
    <ol>
      <li><a href="#21-converse-part">Converse part</a></li>
      <li><a href="#22-direct-part">Direct part</a></li>
    </ol>
  </li>
</ol>

<h2 id="1-definitions">1. Definitions</h2>

<p>We have a sequence \(X^n=(X_1, \dots, X_n)\) with \(X_i\) being sampled IID according to a source distribution \(P_X\). 
We also assume \(X_i\) takes on values from a finite alphabet \(\mathcal{X}\).
Let us define an encoder-decoder pair with encoding function \(f_n: \mathcal{X}^n \rightarrow \{1,\dots, 2^{nR}\}\) and decoding function \(\phi_n: \{1,\dots, 2^{nR}\} \rightarrow \hat{\mathcal{X}}^n\), where \(R\) is the rate (denoting bits per source symbol) and \(\hat{\mathcal{X}}\) is the reconstruction alphabet.
In plain words, the encoder \(f_n\) maps \(X^n\) to a index of a codebook \(\mathcal{C}\) consisting of \(2^{nR}\) codewords, and the decoder \(\phi_n\) returns the corresponding codeword \(\hat{X}^n\) which is one of the rows in \(\mathcal{C}\).</p>

<p>One example of the scheme above is the representation of real numbers in computers.
We would need infinite precision, thus infinite bits, to represent a number in \(\mathbb{R}\), so we instead truncate it to \(32\)-bit floating point, resulting in a codebook consisting \(2^{32}\) possible numbers.
Such a scheme is also called quantization.
We essentially quantize numbers in \(\mathbb{R}\) using a finite set of rational numbers \(\mathbb{Q}\).</p>

<p>To measure the quality of the chosen scheme, which is a pair \((f_n, \phi_n)\), we can measure the distortion between the input and the reconstruction as \(d(X^n,\hat{X}^n)=\frac{1}{n}\sum_{i=1}^n d(X_i, \hat{X}_i)\). Here, \(d:\mathcal{X}\times \hat{\mathcal{X}} \rightarrow \mathbb{R}^+\) is the distortion measure.
Some popular choices of distortion measure includes:</p>

<ul>
  <li><em>Hamming distortion</em> that is given by \(d(x,\hat{x}) = [x \neq \hat{x}]\). Here, \([\cdot]\) denotes the indicator function.</li>
  <li><em>Squared-error distortion</em> that is given by \(d(x,\hat{x})=(x-\hat{x})^2\).</li>
</ul>

<p>For simplicity, I assume that Hamming distortion is used in this post.
Moving on, the following introduces several definitions highlighting the interplay between rate \(R\) and distortion \(D\).</p>

<p><strong>Definition 1.</strong>
The <strong>rate distortion pair \((R,D)\)</strong> is <em>achievable</em> if there exists a sequence of \((f_n,\phi_n)\) with \(\lim_{n\rightarrow \infty} \mathbb{E} d(X^n, \phi_n(f_n(X^n))) \leq D\).</p>

<p><strong>Definition 2.</strong>
The <strong>rate distortion region</strong> is the closure of all achievable \((R,D)\).</p>

<center>
    <figure>
        <img src="../assets/images/rate_distortion.png" alt="rate-distortion function" style="width: 80%; display: inline-block;" />
    </figure>
</center>

<p><strong>Definition 3.</strong>
The <strong>rate distortion function \(R(D)\)</strong> is the <em>infimum</em> over all rates \(R\) such that \((R,D)\) is achievable for a given distortion \(D\).</p>

<p><strong>Definition 4.</strong>
The <strong>information rate distortion function</strong> is defined as</p>

\[R^{(I)}(D) = \underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})\]

<p>Where \(I(\cdot;\cdot)\) is the mutual information function and the minimum is taken over all possible \(P_{\hat{X}|X}\), assuming that the source distribution \(P_X\) is fixed. 
Alternatively, we can also define \(R^{(I)}(D)\) as the minimization over the joint \(P_{\hat{X}X}\) as long as the marginal matches the given \(P_X\).</p>

<h2 id="2-the-rate-distortion-theorem">2. The rate-distortion theorem</h2>

<p>In his foundational work in information theory, Claude Shannon stated a key result in rate-distortion theory:</p>

<p><strong>Theorem 1.</strong></p>

\[R(D)=R^{(I)}(D)=\underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})\]

<p>This gives the original definition of rate distortion function \(R(D)\) an operational value as an optimization problem. Spelling things out, the theorem has two parts:</p>

<ol>
  <li><strong>Direct part:</strong> If \(R&gt;R^{(I)}(D)\) then \((R,D)\) is achievable.</li>
  <li><strong>Converse part:</strong> If \((R,D)\) is achievable then \(R\geq R^{(I)}(D)\).</li>
</ol>

<h3 id="21-converse-part">2.1. Converse part</h3>

<p>The proof for the converse is rather straighforward and depends only on some properties of \(R^{(I)}(D)\), namely \(R^{(I)}(D)\) is monotonically non-increasing, convex, and continuous. For interested readers, you can refer to the proofs of these properties in <em>Elements of Information Theory by Thomas &amp; Cover</em>.</p>

<p><strong>Claim 1.</strong>
\(nR \geq I(X^n; \hat{X}^n)\)</p>

<p><strong>Proof of claim 1.</strong></p>

\[\begin{align}
I(X^n; \hat{X}^n) &amp;\leq I(X^n; f_n(X^n)) &amp;&amp;\text{(Data processing ineq.)}\\
&amp;= H(f_n(X^n)) &amp;&amp;(H(f_n(X^n)|X^n)=0)\\
&amp;\leq nR &amp;&amp;(\text{Codebook is of size } 2^{nR})
\end{align}\]

<p><strong>Proof of the converse part.</strong></p>

\[\begin{align}
nR &amp;\geq I(X^n; \hat{X}^n) &amp;&amp;\text{(Claim 1)}\\
&amp;= H(X^n) - H(X^n|\hat{X}^n)\\
&amp;= \sum_i H(X_i) - \sum_i H(X_i|X^{i-1}, \hat{X}^n) &amp;&amp;\text{(Chain rule)}\\
&amp;\geq \sum_i H(X_i) - \sum_i H(X_i|X^{i-1}) &amp;&amp;\text{(Conditioning reduces entropy)}\\
&amp;= \sum_i I(X_i; \hat{X}_i)\\
&amp;\geq \sum_i R^{(I)} (\mathbb{E} d(X_i, \hat{X}_i)) &amp;&amp;(\text{By def. of } R^{(I)}(D))\\
&amp;= n \sum_i \frac{1}{n} R^{(I)} (\mathbb{E} d(X_i, \hat{X}_i))\\
&amp;\geq n R^{(I)} \left(\frac{1}{n} \sum_i \mathbb{E} d(X_i, \hat{X}_i) \right) &amp;&amp;(\text{Convexity of } R^{(I)}(D) \text{ and Jensen's ineq.})\\
&amp;= n R^{(I)} (\mathbb{E} d(X^n, \hat{X}^n)) &amp;&amp;\text{(By def. of distortion)}\\ 
&amp;\geq n R^{(I)} (D) &amp;&amp;(\text{Monotonicity of } R^{(I)}(D) \text{ and the claim that } \mathbb{E} d(X^n, \hat{X}^n) \leq D)\\
\end{align}\]

<p>The above completes the proof of the converse part.
Intuitively, the converse says that if you give me a wonderful scheme that achieves distortion \(\leq D\), I will show that your rate \(R\) must be at least \(R^{(I)}(D)\).</p>

<h3 id="22-direct-part">2.2. Direct part</h3>

<p>In contrast to the converse part, the proof for the direct part is tricky.
Nevertheless, it is actually quite similar to the direct part of noise-channel coding theorem.
The crux of this proof would follow the random codebook construction with some small differences.</p>

<p>I will start by laying out some ingredients for the main proof.</p>

<p><strong>Lemma 1.</strong>
Given \(N\) IID Bernoulli \(p\) variables \(X_1,\dots,X_n\), we have that</p>

\[P(\text{at least 1 success}) = P((X_1=1) \lor \cdots \lor (X_n=1)) \rightarrow 1 \text{ if } np\rightarrow \infty\]

<p><strong>Proof of lemma 1.</strong></p>

\[\begin{align}
P(\text{at least 1 success}) &amp;= 1-P(\text{failure in all trials}) \\
&amp;\geq 1-(1-p)^n \\
&amp;\geq 1-\exp(-np) \rightarrow 1 \text{ as } np\rightarrow \infty
\end{align}\]

<p><strong>Lemma 2.</strong>
Given a function \(g:\mathcal{X}\rightarrow \mathbb{R}^+\) and a sequence \(\underline{x}=(x_1,\dots,x_n)\) with \(x_i\in\mathcal{X}\), the following holds:</p>

\[\underline{x} \in \mathcal{T}_{\varepsilon}^{(n)} (P_X)\Rightarrow \frac{1}{n} \sum_i g(x_i) \leq (1+\varepsilon) \mathbb{E}g(X)\]

<p>Here, \(\mathcal{T}_{\varepsilon}^{(n)}(P_X)\) is a strongly typical set with tolerence \(\varepsilon\).</p>

<p><strong>Proof of lemma 2.</strong> The proof follows naturally from the definition of a strongly typical set.</p>

<p><strong>Lemma 3.</strong>
Given two sequences of random variables \(\{\tilde{X}_i\}, \{\tilde{Y}_i\}\) that are drawn IID from \(P_X\) and \(P_Y\) respectively, where \(P_X\) and \(P_Y\) are the marginals of some \(P_{XY}\), then</p>

\[Pr((\tilde{X}, \tilde{Y})\in \mathcal{A}_{\varepsilon}^{(n)}(P_{XY})) \leq 2^{-n(I(X;Y)-3\varepsilon)}\]

<p>Where \(\mathcal{A}_{\varepsilon}^{(n)}(P_{XY})\) is a weakly typical set with tolerence $\varepsilon$.</p>

<p><strong>Lemma 4.</strong>
Given \(0&lt;\varepsilon'&lt;\varepsilon\), if \(\underline{x}\in \mathcal{T}_{\varepsilon'}^{(n)}(P_X)\) and \(\{Y_i\} ~\) IID from \(P_Y\) then:</p>

\[Pr((\underline{x}, \underline{Y})\in \mathcal{T}_{\varepsilon}^{(n)}(P_{XY})) \geq 2^{-n(I(X;Y)+4\delta_{XY})}\]

<p>The lemma 3 has been proven in the previous blog post, and the proof of lemma 4 is omitted since it is quite complicated.</p>

<p><strong>Proof of the direct part:</strong>
Given the source distribution \(P_X\),
we fix the conditional probability \(P_{\hat{X}|X}\) such that it satisfies \(\mathbb{E}_{P_{X\hat{X}}} d(X,\hat{X}) \leq D\). 
We will show that if \(R &gt; I_{P_{X\hat{X}}}(X;\hat{X}) + \tilde{\epsilon}\) then \((R,D)\) is achievable.
If we are able to prove this, the proof in the case \(R&gt;R^{(I)}(D)\) would follow naturally.</p>

<p>Since we already have \(P_X\) and \(P_{\hat{X}\vert X}\), we also get the output symbol probability \(P_\hat{X}\) using the law of total probability:</p>

\[P_{\hat{X}}(\hat{x}) = \sum_{x} P(\hat{x}|x) P(x)\]

<p>Similar to the direct part of noisy-channel coding, we consider a random codebook \(\mathcal{C}\in \mathcal{\hat{X}}^{2^{nR}\times n}\) where the row elements follow \(P_{\hat{X}}\).
In this proof, the decoder follows the strongly typical decoding scheme;
given a source sequence \(\underline{x}\in \mathcal{X}^n\), we search for the existence of a row \(j\) in \(\mathcal{C}\) such that \((\underline{x}, \hat{x}(j))\in \mathcal{T}_{\epsilon}^{(n)}(P_X \circ P_{\hat{X}|X})\).
Also, to be more precise with the constant terms invovled, we assume that \(0&lt;\epsilon'&lt;\epsilon&lt;\tilde{\epsilon}\).
We define the event of “success” for the decoder \(\phi_n\) if there exists such \(j\) satisfying the above statement, and if so, the reconstruction for \(\underline{x}\) is then \(\hat{x}(j)\) (the row \(j\) in \(\mathcal{C}\)).
In this case, the distortion is upper bounded by:</p>

\[\begin{align}
d(\underline{x}, \hat{x}(j)) &amp;\leq \mathbb{E}[d(x,\hat{x})](1+\epsilon) &amp;\text{(Lemma 2)}\\
&amp;=D(1+\epsilon) &amp;\text{(By assumption)}
\end{align}\]

<p>In the case where there is no such \(j\) exist, the distortion is assumed to be at most \(d_\max \doteq \underset{x,\hat{x}}{\max} d(x,\hat{x})\).
For this, the expected distortion, assuming that \(\underline{x}, \mathcal{C}\) are random, is upper bounded by:</p>

\[\mathbb{E}[d(X,\hat{X})] \leq \text{Pr(success)}D(1+\epsilon) + \text{Pr(failure)} d_\max\]

<p>For a fixed codebook \(\mathcal{C}\),</p>

\[\mathbb{E}[d(X,\hat{X})\vert \mathcal{C}] \leq \text{Pr}(\text{success}\vert \mathcal{C})D(1+\epsilon) + \text{Pr}(\text{failure}\vert \mathcal{C}) d_\max\]

<p>The plan now is quite similar to the noisy-channel coding’s proof: if I can upper bound the expected distortion \(\mathbb{E}[d(X,\hat{X})]\) (averaged over \(\mathcal{C}\)) by some amount \(\gamma\), then there must exist a codebook \(\mathcal{C}^*\) that actually achieves the distortion of at most \(\gamma\).
To achieve this plan, we shall turn our attention to \(\text{Pr}(\text{success})\) which we can eventually show to happen with almost guarantee.
There are two ways we can factor \(\text{Pr}(\text{success})\) with the law of total probability:</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;= \sum_{\mathcal{C}} \text{Pr}(\mathcal{C}) \text{Pr}(\text{success} | \mathcal{C}) &amp;\text{(1)}\\
\text{Pr}(\text{success}) &amp;= \sum_{\underline{x}} \text{Pr}(\underline{x}) \text{Pr}(\text{success} | \underline{x}) &amp;\text{(2)}
\end{align}\]

<p>Notice that the right-hand side of \((2)\) depends on the choice of \(\mathcal{C}\).
The important point is that we are effectively recasting the randomness of \(\mathcal{C}\) to the randomness of \(\mathcal{X}\), which eases the analysis.
We then have the following decomposition:</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;= \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi) + \sum_{\xi \notin \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi)\\
&amp;\geq \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi)
\end{align}\]

<p>From lemma 1 and 4, we know that:</p>

\[\begin{align}
P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi) &amp;\geq 1- \exp(-2^{nR}2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}})})\\
&amp;= 1- \exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)}) 
\end{align}\]

<p>Thus,</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;\geq \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) 1-\exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)})\\
&amp;= 1- \exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)}) 
\end{align}\]

<p>If \(R &gt; I(X;\hat{X})+4\delta_{X\hat{X}}\), we can drive \(\text{Pr}(\text{success})\rightarrow 1\) for some large \(n\). Consequently, this tells us that if \(R &gt; I(X;\hat{X})+4\delta_{X\hat{X}}\) then \(\mathbb{E}[d(X,\hat{X})]\leq D(1+\epsilon)\), implying the existence of codebook \(\mathcal{C}^*\) whose distortion is at most \(D(1+\epsilon)\).
To get the prove for \(R&gt;R^{(I)}(D)\), we just need to set \(P_{\hat{X}\vert X}\) to the minimizer of the functional \(R^{(I)}(D)\), and set \(\delta_{X\hat{X}}=\frac{\tilde{\epsilon}}{4}\).</p>

<!-- ## 3. Calculating the rate-distortion function

Some tips about the actual calculation of the $$R(D)$$ given a input distribution and the distortion function.

<u>Proposition 1:</u>
All the conditional probability $$P_{\hat{X}|X}$$ that satisfies $$\mathbb{E}[d(X,\hat{X})]\leq D$$ form a convex set.

<u>Proposition 2:</u>
For a fixed $$P_X$$, we have that $$\underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})$$ is a convex optimization problem.  -->]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem. The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression. I will try to keep the material short and concise.]]></summary></entry><entry><title type="html">Noisy-channel coding theorem</title><link href="http://localhost:4000/channel_coding_proof/" rel="alternate" type="text/html" title="Noisy-channel coding theorem" /><published>2023-11-21T00:00:00+01:00</published><updated>2023-11-21T00:00:00+01:00</updated><id>http://localhost:4000/channel_coding_proof</id><content type="html" xml:base="http://localhost:4000/channel_coding_proof/"><![CDATA[<p>Here is my second attempt to understand the proof for the channel coding theorem.
My initial exposure to this concept was through Professor Amos Lapidoth’s lecture in Information Theory I.
His presentation, though concise, was packed with ingenious methods that were challenging to fully comprehend at time.
To better understand it, I shall employ the Feynman technique - explaining the concept to someone else.</p>

<h2 id="1-communication-through-a-noisy-medium">1. Communication through a noisy medium</h2>

<p>Imagine Bob’s wife, Alice, is traveling the universe for her company annual trip.
She wants to send her lovely husband some gorgious photos she’ve taken during the last couples of days.
From Alice’s space ship, these photos are first encoded as binary strings (e.g. \(00101101..\)), then converted into radio waves and finally emitted to Bob’s computer on Earth.</p>

<p>However, every interplanetary species know how deadly and quirky the space is, and signals travel through this medium is of no exception.
Due to cosmic radiation, the sent bit has a probability \(p\in [0,1]\) of being flipped; \(0\) is received as \(1\) at Bob’s computer given some chance \(p&gt;0\) for instance.</p>

<figure>
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Binary_symmetric_channel_%28en%29.svg/1920px-Binary_symmetric_channel_%28en%29.svg.png" alt="noisy communication scheme" style="width:500px; display: inline-block;" />
</figure>

<p>To circumvent such situation, the ship can send the same bit multiple times just to be sure that the receiver can recover the message faithfully.
For example, Alice can instead send \(010\) as \(000111000\) (each bit is duplicated three times). The encoder at Alice’s spaceship would then follow the mapping:</p>

\[\begin{align}
0 &amp;\rightarrow 000\\
1 &amp;\rightarrow 111
\end{align}\]

<p>On Earth, Bob can leverage a voting scheme to decode the message, say, for every block of 3 bits, decodes it as the majority bit in that block (e.g. \(011 \rightarrow 1\)).</p>

<p>To be more concrete, let’s assume \(p=0.2\).
Before, the probability of incorrectly receiving a bit \(P_e\), say bit \(0\) is received and decoded as \(1\), equals to \(0.2\).
Now, with the redundant bits introduced, the error happens when \(000\) is received as one of \(\{011, 101, 110, 111\}\) (Bob will decode it as a \(1\)), and this event occurs with probability \(P_e \approx 0.104\)!
It seems like the probability of error \(P_e\) will vanish as we keep sending the same bit for a large number of times.
Wonderful right?
However, the reduction in \(P_e\) would come with a cost!</p>

<center>
    <figure>
        <img src="../assets/images/error_prob.png" alt="prob of error decay rate" style="display: inline-block;" />
        <!-- <figcaption>...</figcaption> -->
    </figure>
</center>

<p>Every time the space ship sends a bit through the radio channel, this service charges Alice some amount of money.
In case the encoder duplicates a bit \(K\) times, the rate of communication would then be \(R=\frac{1}{K}\) bits per channel use.
To send a single bit reliably, the ship emits \(K\) times the radio signal to transmit it to Earth.
From the previous, we speculate that only when \(R \rightarrow 0\) then \(P_e\rightarrow 0\).
Bad news for the family’s bank account!</p>

<p>But the couple have to worry no more, Claude Shannon dropped a phenomenal paper in 1948 which states that \(\forall \varepsilon&gt;0\), there exists an encoder-decoder pair that allows us to make \(P_e &lt; \varepsilon\) without the need to drive \(R\rightarrow 0\).
In plain words, you can make the probablity of error arbitrarily small and still having a reasonable communication rate.</p>

<h2 id="2-noisy-channel-coding-theorem">2. Noisy-channel coding theorem</h2>

<p>Formally, the basic scheme consists of a set of messages \(\mathcal{M}\), an encoder \(f: \mathcal{M} \rightarrow \mathcal{X}^n\) with \(n\) be the block length, and a decoder \(\phi: \mathcal{Y}^n \rightarrow \mathcal{M} \cup \{?\}\).
The symbol \(?\) denotes the failure case when the decoder doesn’t know the original message.
The communication rate is \(R=\frac{\log |\mathcal{M}|}{n}\) bits per channel use, assume the messages are drawn uniformly.
We can index \(\mathcal{M}\) by \(\{1,\dots, \lfloor 2^{nR} \rfloor\}\).
Finally, the noisy channel \(W(\mathcal{Y}|\mathcal{X})\) is memoryless and assumed to be fixed in this setting.</p>

<p><strong>Definition 1:</strong>
The probability of error associated with message \(m\) is denoted by \(\lambda_m\).
Verbosely,</p>

\[\begin{align}
\lambda_m &amp;= \sum_{\mathbf{y} \in\mathcal{Y}^n | \phi(\mathbf{y}) \neq m} P(Y = \mathbf{y} | X = \mathbf{x}(m))\\
&amp;= \sum_{\mathbf{y} \in\mathcal{Y}^n | \phi(\mathbf{y}) \neq m} \prod_{i=1}^n W(y_i | x_i(m)) &amp;\text{(Channel is memoryless)}
\end{align}\]

<p>Next, we define \(\lambda_\max = \underset{m\in \mathcal{M}}{\max} \lambda_m\), the maximum chance of getting an error considering all messages.
The average error chance is \(P_e^{(n)} = \frac{1}{|\mathcal{M}|} \sum_{m\in \mathcal{M}} \lambda_m\).</p>

<p><strong>Definition 2.</strong>
Rate \(R\) is <strong>achievable</strong> if there exists a sequence of encoder-decoder pairs \(\{f, \phi\}_n\) such that \(\lim_{n\rightarrow \infty} \lambda_\max = 0\).</p>

<p><strong>Definition 3:</strong>
The capacity of a channel, denoted \(C\), is the <strong>supremum</strong> of all achievable rates.
Furthermore, we define \(C^{(I)} = \underset{Q\in P_\mathcal{X}}{\max} I(Q, W)\), where \(Q\) is a probability distribution for input symbols in \(\mathcal{X}\) and \(I(\cdot;\cdot)\) is the <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> between two distributions.</p>

<!-- **Shannon's noisy-channel coding theorem:** -->
<!-- $$C = C^{(I)}$$ -->

<p style="background-color: lightblue; padding: 10px;">
    <strong>Shannon’s noisy-channel coding theorem:</strong>
    \[
        C = C^{(I)}
    \]
</p>

<p>This theorem consists of two parts which we shall prove consequently:</p>

<ol>
  <li><strong>Converse part:</strong> If \(R &gt; C^{(I)}\), then \(R\) is not achievable.</li>
  <li><strong>Direct part:</strong> If \(R &lt; C^{(I)}\), then \(R\) is achievable.</li>
</ol>

<h3 id="21-converse-part">2.1 Converse part</h3>

<p>A contrapositive statement for the converse says that if you give me a wonderful encoder-decoder pair whose rate \(R\) is achievable (e.g. \(\lambda_\max \rightarrow 0\) as \(n\rightarrow \infty\)), then \(R \leq C^{(I)}\).
Next, I would like to introduce the Fano’s inequality which is necessary for the proof later:</p>

<p><strong>Fano’s inequality:</strong></p>

\[\begin{align}
H(M|Y) &amp;\leq H(P_e) + P_e\log(|\mathcal{M}|-1)\\
&amp;\leq 1 + P_e\log|\mathcal{M}|
\end{align}\]

<p>Where \(P_e=P(\hat{M} \neq M)\) with \(\hat{M}=\phi(Y)\in\mathcal{M}\).
We have \(H(P_e)\leq 1\) because \(P_e\) is a Bernoulli distribution and the maximum entropy of a binary random variable is \(1\) bit.
The proof for this inequality is rather a trivial algebraic manipulation, for curious readers you can refer to <em>Elements of Information Theory by Thomas &amp; Cover</em>.</p>

<p><strong>Proof of the converse part:</strong></p>

\[\begin{align}
nR = \log |\mathcal{M}| = H(M) &amp;= I(M; Y^n) + H(M|Y^n)\\
&amp;\leq I(M; Y^n) + 1 + P_e^{(n)}nR &amp;\text{(Fano's inequality)}
\end{align}\]

<p>Dividing \(n\) both sides,</p>

\[R \leq \frac{1}{n} I(M; Y^n) + \frac{1}{n} + P_e^{(n)}R\]

<p>Given that \(R\) is achievable, we have \(P_e^{(n)}\rightarrow 0\). Also, \(\frac{1}{n}\rightarrow 0\) as \(n\rightarrow \infty\). The remain work is to bound \(\frac{1}{n} I(M; Y^n)\):</p>

\[\begin{align}
I(M; Y^n) &amp;\leq I(X^n; Y^n) &amp;\text{(Data Processing Ineq.)}\\
&amp;= H(Y^n) - H(Y^n|X^n) \\
&amp;= \sum_{i=1}^n H(Y_i|Y^{i-1}) - H(Y_i|Y^{i-1}, X^n) &amp;\text{(Chain rule)}\\
&amp;\leq \sum_{i=1}^n H(Y_i) - H(Y_i|X_i) &amp;\text{(Channel is memoryless)}\\
&amp;= \sum_{i=1}^n I(X_i; Y_i)\\
&amp;\leq n \underset{Q\in P_\mathcal{X}}{\max} I(Q; W) = n C^{(I)}
\end{align}\]

<p>Hence, \(\frac{1}{n} I(M; Y^n) \leq C^{(I)} \Rightarrow R \leq C^{(I)}\), completing the proof.</p>

<h3 id="22-direct-part">2.2 Direct part</h3>

<p>We would need several key ingredients for the actual proof of the direct part.</p>

<p><strong>Joint Weak Typicality.</strong>
Given a joint probability \(P_{XY}\) and a tolerance $\epsilon$, and two sequences \(\mathbf{x},\mathbf{y}\) that satisfy the following conditions:</p>

<ol>
  <li>\(\mathbf{x}\) is weakly typical, e.g. \(2^{-n(H(P_X)+\epsilon)} &lt; \prod_{i=0}^n P_X(x_i) &lt; 2^{-n(H(P_X)-\epsilon)}\).</li>
  <li>\(\mathbf{y}\) is weakly typical, e.g. \(2^{-n(H(P_Y)+\epsilon)} &lt; \prod_{i=0}^n P_Y(y_i) &lt; 2^{-n(H(P_Y)-\epsilon)}\).</li>
</ol>

<p>Then, the pair \((\mathbf{x, y})\) is weakly typical if \(2^{-n(H(P_{XY})+\epsilon)} &lt; \prod_{i=0}^n P_{XY}(x_i, y_i) &lt; 2^{-n(H(P_{XY})-\epsilon)}\).
We denotes such event as \((\mathbf{x,y}) \in \mathcal{A}_\epsilon^{(n)}(P_{XY})\).
A jointly typical pair of sequences has some properties that will be needed for the main proof:</p>

<ol>
  <li>If the elements of \((\mathbf{x,y})=\{(x_i,y_i)\}_{i=1}^n\) are drawn IID from \(P_{XY}\), then for any \(\delta&gt;0\) there exists \(N(\delta)\) so that \(\forall n&gt;N(\delta)\) we have \(\text{Pr}((\mathbf{x,y}) \notin \mathcal{A}_\epsilon^{(n)}(P_{XY})) \leq \delta\).</li>
  <li>If \(\mathbf{x}=\{x_i\}_{i=1}^n\) and \(\mathbf{y}=\{y_i\}_{i=1}^n\) are drawn IID from \(P_X\) and \(P_Y\) respectively, then the chance that the pair of sequences \((\mathbf{x,y})\) is jointly typical is very unlikely:</li>
</ol>

\[\textbf{Pr}((\mathbf{x,y}) \in \mathcal{A}_\epsilon^{(n)}(P_{XY})) \leq 2^{-n(I(X;Y)- 3\epsilon)}\]

<p><strong>Proof of the direct part:</strong></p>

<p>Let us fix the input symbols distribution \(Q \in P_X\).
We would like to prove that for any rate \(R &lt; I(Q, W)\), \(R\) is achievable.
If we can prove this previous statement, the same thing holds when \(Q\) is the capacity-achieving input distribution and hence the proof for \(R &lt; C^{(I)}\) follows naturally.</p>

<p>To decode the messages, we shall use something called jointly typical decoding.
For a given output sequence \(Y^n\) from the channel, we check for the existance of a message \(m\) whose code \(X^n(m)\) is jointly typical with \(Y^n\), e.g. whether \((X^n,Y^n) \in \mathcal{A}^{(n)} (P_{XY})\).
If there exists a single \(m\) satisfies this condition, return \(m\).
Otherwise, if there is none or multiple such \(m\)’s, we declare error.</p>

<p>Now, consider a random rate-\(R\) blocklength-\(n\) codebook \(\mathcal{C}\).
To construct this codebook, the row elements (there are \(n\) of them) in each of the \(2^{nR}\) rows in \(\mathcal{C}\) are drawn IID from \(Q\).
The probability of error for message \(m\) associating with this codebook is denoted by \(\lambda_m(\mathcal{C})\).
The expected error is then written as \(\bar{\lambda}_m = \mathbb{E}_{P_{\mathcal{C}}}[\lambda_m(\mathcal{C})]\).
Similarly, the average error for codebook \(\mathcal{C}\) is \(P_e^{(n)}(\mathcal{C})\) and the expected average error is \(P_e^{(n)}\).</p>

<p><strong>Claim 1:</strong>
The expected error is independent of the message \(m\), e.g. \(\bar{\lambda}_1 = \dots = \bar{\lambda}_m\).</p>

<p><strong>Proof of claim 1 (non-rigorous):</strong>
This is due to symmetry!</p>

<p>Follow from claim 1, we can easily see that the expected average error \(P_e^{(n)}=\bar{\lambda}_1\).
From now on, we will focus on bounding \(\bar{\lambda}_1\) which turns out to be easy to work with!</p>

<p>Given that \(m=1\) and the fact that we are using jointly typical decoding, let us define \(\varepsilon_i = [(X^n(i), Y^n) \in \mathcal{A}_\epsilon^{(n)}]\), the event where the code for message \(i\) is jointly typical with \(Y^n\).
The error \(\bar{\lambda}_1\) can then be expressed as:</p>

\[\bar{\lambda}_1 = \text{Pr}(\varepsilon_1^c \cup \varepsilon_2 \cup \dots \cup \varepsilon_m)\]

<p>This is exactly equivalent what we have defined as an error for the jointly typical decoding.
We then have the following bounds,</p>

\[\begin{align}
\bar{\lambda}_1 &amp;\leq \text{Pr}(\varepsilon_1^c) + \sum_{i=2}^{2^{nR}} \text{Pr}(\varepsilon_i) &amp;\text{(Union bound)}\\
&amp;\leq \delta + \sum_{i=2}^{2^{nR}} 2^{-n(I(X;Y)-3\epsilon)} &amp;\text{(Properties of joint typicality)}\\
&amp;\leq \delta + 2^{-n(I(X;Y)-R-3\epsilon)}
\end{align}\]

<p>Now, if \(R &lt; I(X;Y) - 3\epsilon\), we can make \(\bar{\lambda}_1 \leq 2\delta\) for some large \(n\). Also note that \(\text{Pr}(\varepsilon_1^c) \leq \delta\) only hold for a large \(n\) too.</p>

<p>To get the actual proof, we replace \(Q\) with the capacity-achieving distribution \(\underset{Q \in P_X}{\text{argmax}}~I(Q,W)\).
The last conclusion above becomes \(R &lt; C^{(I)} - 3\epsilon\) implying \(P_e^{(n)} = \bar{\lambda}_1 \leq 2\delta\).
Next, we have a fundamental lemma about the average of numbers:</p>

<p><strong>Lemma 1:</strong> Consider a sequence of real numbers \(a_1,\dots, a_n\). 
If the average \(\bar{a} = \frac{1}{n} \sum_{i=1}^n a_i \leq \gamma\), then there exists an \(a_i \leq \gamma\).</p>

<p>This lemma can be proven easily by contradiction.
The implication of this is that previously we have shown that the average error \(P_e^{(n)} \leq 2\delta\) over all random codebook, then there must exist a codebook \(\mathcal{C}^*\) whose average error \(P_e^{(n)}(\mathcal{C}^*) \leq 2\delta\).
It remains to bound the maximal error \(\lambda_\max(\mathcal{C}^*) = \underset{m\in \mathcal{M}}{\max}\lambda_m(\mathcal{C}^*)\), which is actually we need to show for the achievability of the rate \(R\).</p>

<p><strong>Lemma 2:</strong>
Consider a sequence of \(2n\) real numbers such that \(a_1\leq \dots \leq a_{2n}\) and its average \(\bar{a} \leq \nu\).
It holds that \(a_n \leq 2\nu\).</p>

<p><strong>Proof:</strong></p>

\[\begin{align}
2n\nu &amp;\geq \sum_{i=1}^{2n} a_i = \sum_{i=1}^{n} a_i + \sum_{i=n+1}^{2n} a_i\\
&amp;\geq \sum_{i=n+1}^{2n} a_i \\
&amp;\geq \sum_{i=n+1}^{2n} a_n = n a_n
\end{align}\]

<p>Using the lemma, we have a bound on the maximal error \(\lambda_\max(\mathcal{C}^*)\leq 4\delta\) by throwing away half of the codewords in \(\mathcal{C}^*\). 
However, by throwing away codewords we also lose the rate of communication, it is now \(R-\frac{1}{n}\) instead of \(R\).
Indeed, the \(\frac{1}{n}\) would vanish for a large \(n\).
Overall, this strategy of bounding the maximum is called the <em>expurgation trick</em>.</p>

<p>For a rate \(\tilde{R} &lt; C^{(I)}\) and any \(\tilde{\epsilon}&gt;0\), we can show that for \(n\) large enough there exists a rate-\(R\) (\(\tilde{R} &lt; R\)) blocklength-\(n\) codebook where \(\lambda_\max \leq \tilde{\epsilon}\) by setting \(R=\frac{(\tilde{R}+C)}{2}\), \(\delta=\frac{\tilde{\epsilon}}{4}\), \(\epsilon &lt; \frac{C^{(I)}-R}{3}\) and apply the proof above.</p>]]></content><author><name>Khánh Vũ</name></author><summary type="html"><![CDATA[Here is my second attempt to understand the proof for the channel coding theorem. My initial exposure to this concept was through Professor Amos Lapidoth’s lecture in Information Theory I. His presentation, though concise, was packed with ingenious methods that were challenging to fully comprehend at time. To better understand it, I shall employ the Feynman technique - explaining the concept to someone else.]]></summary></entry></feed>