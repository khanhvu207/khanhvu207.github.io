<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Rate-distortion theory - EntropyMaxxing</title>
<meta name="description" content="In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem. The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression. I will try to keep the material short and concise.">


  <meta name="author" content="Khánh Vũ">
  
  <meta property="article:author" content="Khánh Vũ">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="EntropyMaxxing">
<meta property="og:title" content="Rate-distortion theory">
<meta property="og:url" content="http://localhost:4000/rate_distortion/">


  <meta property="og:description" content="In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem. The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression. I will try to keep the material short and concise.">







  <meta property="article:published_time" content="2023-12-19T00:00:00+01:00">





  

  


<link rel="canonical" href="http://localhost:4000/rate_distortion/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Khánh Vũ",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="EntropyMaxxing Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- for mathjax support -->
<!-- 
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js", "AMSmath.js", "AMSsymbols.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
 -->

<!-- for mathjax support -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js", "AMSmath.js", "AMSsymbols.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] },
      TeX: {
        Macros: {
        }
      }
    });
  </script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js"></script>


<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--posts">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          EntropyMaxxing
          
        </a>
        <!-- <div class="cute-gif"> -->
          <!-- <img src="../assets/images/penguin.gif" style="width: 25%; display: inline-block;" alt="cute gif"> -->
        <!-- </div> -->
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/avatar.png" alt="Khánh Vũ" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Khánh Vũ</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>i’m just a chill guy :)</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">Rate-distortion theory</h1>
    
    <p>In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of the main theorem.
The rate-distortion theorem plays a crucial role in modern signal processing, especially in understanding the limit of data compression.
I will try to keep the material short and concise.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li><a href="#1-definitions">Definitions</a></li>
  <li><a href="#2-the-rate-distortion-theorem">Rate-distortion theorem</a>
    <ol>
      <li><a href="#21-converse-part">Converse part</a></li>
      <li><a href="#22-direct-part">Direct part</a></li>
    </ol>
  </li>
</ol>

<h2 id="1-definitions">1. Definitions</h2>

<p>We have a sequence \(X^n=(X_1, \dots, X_n)\) with \(X_i\) being sampled IID according to a source distribution \(P_X\). 
We also assume \(X_i\) takes on values from a finite alphabet \(\mathcal{X}\).
Let us define an encoder-decoder pair with encoding function \(f_n: \mathcal{X}^n \rightarrow \{1,\dots, 2^{nR}\}\) and decoding function \(\phi_n: \{1,\dots, 2^{nR}\} \rightarrow \hat{\mathcal{X}}^n\), where \(R\) is the rate (denoting bits per source symbol) and \(\hat{\mathcal{X}}\) is the reconstruction alphabet.
In plain words, the encoder \(f_n\) maps \(X^n\) to a index of a codebook \(\mathcal{C}\) consisting of \(2^{nR}\) codewords, and the decoder \(\phi_n\) returns the corresponding codeword \(\hat{X}^n\) which is one of the rows in \(\mathcal{C}\).</p>

<p>One example of the scheme above is the representation of real numbers in computers.
We would need infinite precision, thus infinite bits, to represent a number in \(\mathbb{R}\), so we instead truncate it to \(32\)-bit floating point, resulting in a codebook consisting \(2^{32}\) possible numbers.
Such a scheme is also called quantization.
We essentially quantize numbers in \(\mathbb{R}\) using a finite set of rational numbers \(\mathbb{Q}\).</p>

<p>To measure the quality of the chosen scheme, which is a pair \((f_n, \phi_n)\), we can measure the distortion between the input and the reconstruction as \(d(X^n,\hat{X}^n)=\frac{1}{n}\sum_{i=1}^n d(X_i, \hat{X}_i)\). Here, \(d:\mathcal{X}\times \hat{\mathcal{X}} \rightarrow \mathbb{R}^+\) is the distortion measure.
Some popular choices of distortion measure includes:</p>

<ul>
  <li><em>Hamming distortion</em> that is given by \(d(x,\hat{x}) = [x \neq \hat{x}]\). Here, \([\cdot]\) denotes the indicator function.</li>
  <li><em>Squared-error distortion</em> that is given by \(d(x,\hat{x})=(x-\hat{x})^2\).</li>
</ul>

<p>For simplicity, I assume that Hamming distortion is used in this post.
Moving on, the following introduces several definitions highlighting the interplay between rate \(R\) and distortion \(D\).</p>

<p><strong>Definition 1.</strong>
The <strong>rate distortion pair \((R,D)\)</strong> is <em>achievable</em> if there exists a sequence of \((f_n,\phi_n)\) with \(\lim_{n\rightarrow \infty} \mathbb{E} d(X^n, \phi_n(f_n(X^n))) \leq D\).</p>

<p><strong>Definition 2.</strong>
The <strong>rate distortion region</strong> is the closure of all achievable \((R,D)\).</p>

<center>
    <figure>
        <img src="../assets/images/rate_distortion.png" alt="rate-distortion function" style="width: 80%; display: inline-block;" />
    </figure>
</center>

<p><strong>Definition 3.</strong>
The <strong>rate distortion function \(R(D)\)</strong> is the <em>infimum</em> over all rates \(R\) such that \((R,D)\) is achievable for a given distortion \(D\).</p>

<p><strong>Definition 4.</strong>
The <strong>information rate distortion function</strong> is defined as</p>

\[R^{(I)}(D) = \underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})\]

<p>Where \(I(\cdot;\cdot)\) is the mutual information function and the minimum is taken over all possible \(P_{\hat{X}|X}\), assuming that the source distribution \(P_X\) is fixed. 
Alternatively, we can also define \(R^{(I)}(D)\) as the minimization over the joint \(P_{\hat{X}X}\) as long as the marginal matches the given \(P_X\).</p>

<h2 id="2-the-rate-distortion-theorem">2. The rate-distortion theorem</h2>

<p>In his foundational work in information theory, Claude Shannon stated a key result in rate-distortion theory:</p>

<p><strong>Theorem 1.</strong></p>

\[R(D)=R^{(I)}(D)=\underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})\]

<p>This gives the original definition of rate distortion function \(R(D)\) an operational value as an optimization problem. Spelling things out, the theorem has two parts:</p>

<ol>
  <li><strong>Direct part:</strong> If \(R&gt;R^{(I)}(D)\) then \((R,D)\) is achievable.</li>
  <li><strong>Converse part:</strong> If \((R,D)\) is achievable then \(R\geq R^{(I)}(D)\).</li>
</ol>

<h3 id="21-converse-part">2.1. Converse part</h3>

<p>The proof for the converse is rather straighforward and depends only on some properties of \(R^{(I)}(D)\), namely \(R^{(I)}(D)\) is monotonically non-increasing, convex, and continuous. For interested readers, you can refer to the proofs of these properties in <em>Elements of Information Theory by Thomas &amp; Cover</em>.</p>

<p><strong>Claim 1.</strong>
\(nR \geq I(X^n; \hat{X}^n)\)</p>

<p><strong>Proof of claim 1.</strong></p>

\[\begin{align}
I(X^n; \hat{X}^n) &amp;\leq I(X^n; f_n(X^n)) &amp;&amp;\text{(Data processing ineq.)}\\
&amp;= H(f_n(X^n)) &amp;&amp;(H(f_n(X^n)|X^n)=0)\\
&amp;\leq nR &amp;&amp;(\text{Codebook is of size } 2^{nR})
\end{align}\]

<p><strong>Proof of the converse part.</strong></p>

\[\begin{align}
nR &amp;\geq I(X^n; \hat{X}^n) &amp;&amp;\text{(Claim 1)}\\
&amp;= H(X^n) - H(X^n|\hat{X}^n)\\
&amp;= \sum_i H(X_i) - \sum_i H(X_i|X^{i-1}, \hat{X}^n) &amp;&amp;\text{(Chain rule)}\\
&amp;\geq \sum_i H(X_i) - \sum_i H(X_i|X^{i-1}) &amp;&amp;\text{(Conditioning reduces entropy)}\\
&amp;= \sum_i I(X_i; \hat{X}_i)\\
&amp;\geq \sum_i R^{(I)} (\mathbb{E} d(X_i, \hat{X}_i)) &amp;&amp;(\text{By def. of } R^{(I)}(D))\\
&amp;= n \sum_i \frac{1}{n} R^{(I)} (\mathbb{E} d(X_i, \hat{X}_i))\\
&amp;\geq n R^{(I)} \left(\frac{1}{n} \sum_i \mathbb{E} d(X_i, \hat{X}_i) \right) &amp;&amp;(\text{Convexity of } R^{(I)}(D) \text{ and Jensen's ineq.})\\
&amp;= n R^{(I)} (\mathbb{E} d(X^n, \hat{X}^n)) &amp;&amp;\text{(By def. of distortion)}\\ 
&amp;\geq n R^{(I)} (D) &amp;&amp;(\text{Monotonicity of } R^{(I)}(D) \text{ and the claim that } \mathbb{E} d(X^n, \hat{X}^n) \leq D)\\
\end{align}\]

<p>The above completes the proof of the converse part.
Intuitively, the converse says that if you give me a wonderful scheme that achieves distortion \(\leq D\), I will show that your rate \(R\) must be at least \(R^{(I)}(D)\).</p>

<h3 id="22-direct-part">2.2. Direct part</h3>

<p>In contrast to the converse part, the proof for the direct part is tricky.
Nevertheless, it is actually quite similar to the direct part of noise-channel coding theorem.
The crux of this proof would follow the random codebook construction with some small differences.</p>

<p>I will start by laying out some ingredients for the main proof.</p>

<p><strong>Lemma 1.</strong>
Given \(N\) IID Bernoulli \(p\) variables \(X_1,\dots,X_n\), we have that</p>

\[P(\text{at least 1 success}) = P((X_1=1) \lor \cdots \lor (X_n=1)) \rightarrow 1 \text{ if } np\rightarrow \infty\]

<p><strong>Proof of lemma 1.</strong></p>

\[\begin{align}
P(\text{at least 1 success}) &amp;= 1-P(\text{failure in all trials}) \\
&amp;\geq 1-(1-p)^n \\
&amp;\geq 1-\exp(-np) \rightarrow 1 \text{ as } np\rightarrow \infty
\end{align}\]

<p><strong>Lemma 2.</strong>
Given a function \(g:\mathcal{X}\rightarrow \mathbb{R}^+\) and a sequence \(\underline{x}=(x_1,\dots,x_n)\) with \(x_i\in\mathcal{X}\), the following holds:</p>

\[\underline{x} \in \mathcal{T}_{\varepsilon}^{(n)} (P_X)\Rightarrow \frac{1}{n} \sum_i g(x_i) \leq (1+\varepsilon) \mathbb{E}g(X)\]

<p>Here, \(\mathcal{T}_{\varepsilon}^{(n)}(P_X)\) is a strongly typical set with tolerence \(\varepsilon\).</p>

<p><strong>Proof of lemma 2.</strong> The proof follows naturally from the definition of a strongly typical set.</p>

<p><strong>Lemma 3.</strong>
Given two sequences of random variables \(\{\tilde{X}_i\}, \{\tilde{Y}_i\}\) that are drawn IID from \(P_X\) and \(P_Y\) respectively, where \(P_X\) and \(P_Y\) are the marginals of some \(P_{XY}\), then</p>

\[Pr((\tilde{X}, \tilde{Y})\in \mathcal{A}_{\varepsilon}^{(n)}(P_{XY})) \leq 2^{-n(I(X;Y)-3\varepsilon)}\]

<p>Where \(\mathcal{A}_{\varepsilon}^{(n)}(P_{XY})\) is a weakly typical set with tolerence $\varepsilon$.</p>

<p><strong>Lemma 4.</strong>
Given \(0&lt;\varepsilon'&lt;\varepsilon\), if \(\underline{x}\in \mathcal{T}_{\varepsilon'}^{(n)}(P_X)\) and \(\{Y_i\} ~\) IID from \(P_Y\) then:</p>

\[Pr((\underline{x}, \underline{Y})\in \mathcal{T}_{\varepsilon}^{(n)}(P_{XY})) \geq 2^{-n(I(X;Y)+4\delta_{XY})}\]

<p>The lemma 3 has been proven in the previous blog post, and the proof of lemma 4 is omitted since it is quite complicated.</p>

<p><strong>Proof of the direct part:</strong>
Given the source distribution \(P_X\),
we fix the conditional probability \(P_{\hat{X}|X}\) such that it satisfies \(\mathbb{E}_{P_{X\hat{X}}} d(X,\hat{X}) \leq D\). 
We will show that if \(R &gt; I_{P_{X\hat{X}}}(X;\hat{X}) + \tilde{\epsilon}\) then \((R,D)\) is achievable.
If we are able to prove this, the proof in the case \(R&gt;R^{(I)}(D)\) would follow naturally.</p>

<p>Since we already have \(P_X\) and \(P_{\hat{X}\vert X}\), we also get the output symbol probability \(P_\hat{X}\) using the law of total probability:</p>

\[P_{\hat{X}}(\hat{x}) = \sum_{x} P(\hat{x}|x) P(x)\]

<p>Similar to the direct part of noisy-channel coding, we consider a random codebook \(\mathcal{C}\in \mathcal{\hat{X}}^{2^{nR}\times n}\) where the row elements follow \(P_{\hat{X}}\).
In this proof, the decoder follows the strongly typical decoding scheme;
given a source sequence \(\underline{x}\in \mathcal{X}^n\), we search for the existence of a row \(j\) in \(\mathcal{C}\) such that \((\underline{x}, \hat{x}(j))\in \mathcal{T}_{\epsilon}^{(n)}(P_X \circ P_{\hat{X}|X})\).
Also, to be more precise with the constant terms invovled, we assume that \(0&lt;\epsilon'&lt;\epsilon&lt;\tilde{\epsilon}\).
We define the event of “success” for the decoder \(\phi_n\) if there exists such \(j\) satisfying the above statement, and if so, the reconstruction for \(\underline{x}\) is then \(\hat{x}(j)\) (the row \(j\) in \(\mathcal{C}\)).
In this case, the distortion is upper bounded by:</p>

\[\begin{align}
d(\underline{x}, \hat{x}(j)) &amp;\leq \mathbb{E}[d(x,\hat{x})](1+\epsilon) &amp;\text{(Lemma 2)}\\
&amp;=D(1+\epsilon) &amp;\text{(By assumption)}
\end{align}\]

<p>In the case where there is no such \(j\) exist, the distortion is assumed to be at most \(d_\max \doteq \underset{x,\hat{x}}{\max} d(x,\hat{x})\).
For this, the expected distortion, assuming that \(\underline{x}, \mathcal{C}\) are random, is upper bounded by:</p>

\[\mathbb{E}[d(X,\hat{X})] \leq \text{Pr(success)}D(1+\epsilon) + \text{Pr(failure)} d_\max\]

<p>For a fixed codebook \(\mathcal{C}\),</p>

\[\mathbb{E}[d(X,\hat{X})\vert \mathcal{C}] \leq \text{Pr}(\text{success}\vert \mathcal{C})D(1+\epsilon) + \text{Pr}(\text{failure}\vert \mathcal{C}) d_\max\]

<p>The plan now is quite similar to the noisy-channel coding’s proof: if I can upper bound the expected distortion \(\mathbb{E}[d(X,\hat{X})]\) (averaged over \(\mathcal{C}\)) by some amount \(\gamma\), then there must exist a codebook \(\mathcal{C}^*\) that actually achieves the distortion of at most \(\gamma\).
To achieve this plan, we shall turn our attention to \(\text{Pr}(\text{success})\) which we can eventually show to happen with almost guarantee.
There are two ways we can factor \(\text{Pr}(\text{success})\) with the law of total probability:</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;= \sum_{\mathcal{C}} \text{Pr}(\mathcal{C}) \text{Pr}(\text{success} | \mathcal{C}) &amp;\text{(1)}\\
\text{Pr}(\text{success}) &amp;= \sum_{\underline{x}} \text{Pr}(\underline{x}) \text{Pr}(\text{success} | \underline{x}) &amp;\text{(2)}
\end{align}\]

<p>Notice that the right-hand side of \((2)\) depends on the choice of \(\mathcal{C}\).
The important point is that we are effectively recasting the randomness of \(\mathcal{C}\) to the randomness of \(\mathcal{X}\), which eases the analysis.
We then have the following decomposition:</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;= \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi) + \sum_{\xi \notin \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi)\\
&amp;\geq \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi)
\end{align}\]

<p>From lemma 1 and 4, we know that:</p>

\[\begin{align}
P(\underline{X}=\xi) P(\text{success} | \underline{X}=\xi) &amp;\geq 1- \exp(-2^{nR}2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}})})\\
&amp;= 1- \exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)}) 
\end{align}\]

<p>Thus,</p>

\[\begin{align}
\text{Pr}(\text{success}) &amp;\geq \sum_{\xi \in \mathcal{T}_{\epsilon'}^{(n)}(P_X)} P(\underline{X}=\xi) 1-\exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)})\\
&amp;= 1- \exp(-2^{-n(I(X;\hat{X})+4\delta_{X\hat{X}}-R)}) 
\end{align}\]

<p>If \(R &gt; I(X;\hat{X})+4\delta_{X\hat{X}}\), we can drive \(\text{Pr}(\text{success})\rightarrow 1\) for some large \(n\). Consequently, this tells us that if \(R &gt; I(X;\hat{X})+4\delta_{X\hat{X}}\) then \(\mathbb{E}[d(X,\hat{X})]\leq D(1+\epsilon)\), implying the existence of codebook \(\mathcal{C}^*\) whose distortion is at most \(D(1+\epsilon)\).
To get the prove for \(R&gt;R^{(I)}(D)\), we just need to set \(P_{\hat{X}\vert X}\) to the minimizer of the functional \(R^{(I)}(D)\), and set \(\delta_{X\hat{X}}=\frac{\tilde{\epsilon}}{4}\).</p>

<!-- ## 3. Calculating the rate-distortion function

Some tips about the actual calculation of the $$R(D)$$ given a input distribution and the distortion function.

<u>Proposition 1:</u>
All the conditional probability $$P_{\hat{X}|X}$$ that satisfies $$\mathbb{E}[d(X,\hat{X})]\leq D$$ form a convex set.

<u>Proposition 2:</u>
For a fixed $$P_X$$, we have that $$\underset{P_{\hat{X}|X}:\sum_{x,\hat{x}} P(x)P(\hat{x}|x)d(x, \hat{x})\leq D}{\min} I(X;\hat{X})$$ is a convex optimization problem.  -->


<!-- <ul class="taxonomy__index">
  
  
    <li>
      <a href="#2025">
        <strong>2025</strong> <span class="taxonomy__count">4</span>
      </a>
    </li>
  
    <li>
      <a href="#2024">
        <strong>2024</strong> <span class="taxonomy__count">6</span>
      </a>
    </li>
  
    <li>
      <a href="#2023">
        <strong>2023</strong> <span class="taxonomy__count">2</span>
      </a>
    </li>
  
</ul> -->

<!-- 


  <section id="2025" class="taxonomy__section">
    <h2 class="archive__subtitle">2025</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/probability_theory/" rel="permalink">Some personal notes on probability theory
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A self-contained summary of some key results in probability theory.
I’ll mostly summarize the results from Durrett’s Probability: Theory and Examples, Bandit...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/fisher_information/" rel="permalink">What is Fisher information?
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">It bugs me that I often have hard time to recall what Fisher information is.
So let’s write it down here.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/confidence_sets_exact/" rel="permalink">Confidence sets for Gaussian linear models
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In statistical inference, we often want to construct confidence sets for the parameter of interest; for example, you may have heard of the 95% confidence int...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ucb/" rel="permalink">The upper confidence bound algorithm
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Bandits are a class of reinforcement learning (RL) problems where a learner has to choose between different actions, each of which has an unknown reward.
Th...</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2024" class="taxonomy__section">
    <h2 class="archive__subtitle">2024</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/why_i_do_math/" rel="permalink">Why I think mathematics is beautiful
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A friend of mine asked me why I think mathematics is beautiful.
I believe the exact answer does not matter, but it is important to have an answer.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/modern_hashing/" rel="permalink">Modern hashing made simple
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
This is a short personal note on “Modern Hashing Made Simple” by Bender et al. (2024).
I’m going to present this paper for the Advanced Graph Algorithms and...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/jane_street_aug24/" rel="permalink">Jane Street Puzzle - August 2024
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
I decided to slack off my exam studying with Jane Street’s puzzle.
I first read the problem in early August, forgot about it, and then submitted my answer o...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/expectation_counting/" rel="permalink">Counting with expectation
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Problem 10 (Individual test, ARML 2013).
For a positive integer \(n\), let \(C(n)\) equal the number of pairs of consecutive \(1\)’s in the binary representa...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/kl_upper_bound/" rel="permalink">An upper bound for the Kullback-Leibler divergence
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Zürich is so hot now in the night that I couldn’t fall asleep.
While lying on the bed, I arrived at an interesting upper bound for the KL divergence (relativ...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/opt_cheatsheet/" rel="permalink">Mathematical optimization cheatsheet
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In this cheatsheet, I will discuss many concepts that are essential in the analysis of optimization algorithms.

</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2023" class="taxonomy__section">
    <h2 class="archive__subtitle">2023</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/rate_distortion/" rel="permalink">Rate-distortion theory
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In this post, I will summarize Shannon’s rate-distortion theory, including some self-contained definitions and proofs to the converse and direction part of t...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/channel_coding_proof/" rel="permalink">Noisy-channel coding theorem
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Here is my second attempt to understand the proof for the channel coding theorem.
My initial exposure to this concept was through Professor Amos Lapidoth’s l...</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>
 -->

  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/khanhvu207" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/khanhvu207" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Khánh Vũ. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
